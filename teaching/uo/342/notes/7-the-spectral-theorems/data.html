<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.webp" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 7: The Spectral Theorems</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">In the first part of the course, we studied bases of eigenvectors, and in the second part, orthonormal bases. These two notions are somewhat in conflict: while we can always construct an orthonormal basis for a vector space with the Gram-Schmidt process, applying that to a basis of eigenvectors for a diagonalizable matrix generally won&#x2019;t preserve the fact that they&#x2019;re eigenvectors. What we&#x2019;ll explore in this last part of the course is the situation in which we can get the best of both worlds &mdash; an orthonormal basis of eigenvectors. Without any more preliminaries, let&#x2019;s state and prove the theorem that characterizes exactly when this is possible.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">The Real Spectral Theorem</div><p class="body-text">A matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with real entries has an orthonormal basis of eigenvectors with real eigenvalues if and only if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is symmetric (i.e. <span class="tex-holder inline-math" data-source-tex="A^T = A">$A^T = A$).</span></p></div><p class="body-text">We&#x2019;ll see the proof of this shortly; to keep it from being overwhelmingly long, we&#x2019;ll split it into a few chunks that we can tackle one at a time.</p><div class="notes-lem notes-environment"><div class="notes-lem-title notes-title">Lemma: real symmetric matrices have real eigenvalues</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> symmetric matrix with real entries. Then all of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are real.</p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{C}^n">$\vec{v} \in \mathbb{C}^n$</span> be an eigenvector with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda \in \mathbb{C}">$\lambda \in \mathbb{C}$</span> &mdash; we have to assume both are complex since we haven&#x2019;t yet shown that they have to be real. Multiplying <span class="tex-holder inline-math" data-source-tex="A">$A$</span> on the right by <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and the left by <span class="tex-holder inline-math" data-source-tex="\overline{\vec{v}}^T">$\overline{\vec{v}}^T$,</span> we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{\vec{v}^T} A \vec{v} &= \overline{\vec{v}^T} \lambda \vec{v}\\[NEWLINE][TAB]&= \lambda \left( \vec{v} \bullet \overline{\vec{v}} \right).[NEWLINE]\end{align*}">$$\begin{align*}\overline{\vec{v}^T} A \vec{v} &= \overline{\vec{v}^T} \lambda \vec{v}\\[4px]&= \lambda \left( \vec{v} \bullet \overline{\vec{v}} \right).\end{align*}$$</span></p><p class="body-text">Now <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \overline{\vec{v}}">$\vec{v} \bullet \overline{\vec{v}}$</span> is a real number, since</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{\vec{v} \bullet \overline{\vec{v}}} &= \overline{\vec{v}} \bullet \vec{v}\\[NEWLINE][TAB]&= \vec{v} \bullet \overline{\vec{v}},[NEWLINE]\end{align*}">$$\begin{align*}\overline{\vec{v} \bullet \overline{\vec{v}}} &= \overline{\vec{v}} \bullet \vec{v}\\[4px]&= \vec{v} \bullet \overline{\vec{v}},\end{align*}$$</span></p><p class="body-text">and the same is true for <span class="tex-holder inline-math" data-source-tex="\overline{\vec{v}^T} A \vec{v}">$\overline{\vec{v}^T} A \vec{v}$:</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{\overline{\vec{v}^T} A \vec{v}} &= \vec{v}^T \overline{A} \overline{\vec{v}}\\[NEWLINE][TAB]&= \vec{v}^T A \overline{\vec{v}}\\[NEWLINE][TAB]&= \vec{v}^T A^T \overline{\vec{v}}\\[NEWLINE][TAB]&= \left( \overline{\vec{v}}^T A \vec{v} \right)^T\\[NEWLINE][TAB]&= \overline{\vec{v}}^T A \vec{v}.[NEWLINE]\end{align*}">$$\begin{align*}\overline{\overline{\vec{v}^T} A \vec{v}} &= \vec{v}^T \overline{A} \overline{\vec{v}}\\[4px]&= \vec{v}^T A \overline{\vec{v}}\\[4px]&= \vec{v}^T A^T \overline{\vec{v}}\\[4px]&= \left( \overline{\vec{v}}^T A \vec{v} \right)^T\\[4px]&= \overline{\vec{v}}^T A \vec{v}.\end{align*}$$</span></p><p class="body-text">All that is to say that <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> itself must also be real, since</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\lambda = \frac{\overline{\vec{v}^T} A \vec{v}}{\vec{v} \bullet \overline{\vec{v}}}.[NEWLINE]$$">$$\begin{align*}\lambda = \frac{\overline{\vec{v}^T} A \vec{v}}{\vec{v} \bullet \overline{\vec{v}}}.\end{align*}$$</span></p></div><p class="body-text">This lemma also implies that all the eigenvectors of a real symmetric matrix are real, since row reducing <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$</span> for a real-valued matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and a real eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> will only produce real solutions.</p><div class="notes-lem notes-environment"><div class="notes-lem-title notes-title">Lemma: real symmetric matrices have orthogonal eigenspaces</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> symmetric matrix with real entries and let <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> be eigenvectors corresponding to distinct eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_j">$\lambda_j$.</span> Then <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = 0">$\vec{v} \bullet \vec{w} = 0$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">We can verify this with a direct computation:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB](\lambda_i - \lambda_j) \left( \vec{v} \bullet \vec{w} \right) &= \lambda_i \left( \vec{v} \bullet \vec{w} \right) - \lambda_j \left( \vec{v} \bullet \vec{w} \right)\\[NEWLINE][TAB]&= \left( \lambda_i \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( \lambda_j \vec{w} \right)\\[NEWLINE][TAB]&= \left( A \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( A \vec{w} \right)\\[NEWLINE][TAB]&= \left( A \vec{v} \right)^T \vec{w} - \vec{v}^T \left( A \vec{w} \right)\\[NEWLINE][TAB]&= \vec{v}^T A^T \vec{w} - \vec{v}^T A \vec{w}\\[NEWLINE][TAB]&= \vec{v}^T A \vec{w} - \vec{v}^T A \vec{w}\\[NEWLINE][TAB]&= 0.[NEWLINE]\end{align*}">$$\begin{align*}(\lambda_i - \lambda_j) \left( \vec{v} \bullet \vec{w} \right) &= \lambda_i \left( \vec{v} \bullet \vec{w} \right) - \lambda_j \left( \vec{v} \bullet \vec{w} \right)\\[4px]&= \left( \lambda_i \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( \lambda_j \vec{w} \right)\\[4px]&= \left( A \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( A \vec{w} \right)\\[4px]&= \left( A \vec{v} \right)^T \vec{w} - \vec{v}^T \left( A \vec{w} \right)\\[4px]&= \vec{v}^T A^T \vec{w} - \vec{v}^T A \vec{w}\\[4px]&= \vec{v}^T A \vec{w} - \vec{v}^T A \vec{w}\\[4px]&= 0.\end{align*}$$</span></p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="\lambda_i \neq \lambda_j">$\lambda_i \neq \lambda_j$,</span> <span class="tex-holder inline-math" data-source-tex="\lambda_i - \lambda_j \neq 0">$\lambda_i - \lambda_j \neq 0$,</span> and so <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = 0">$\vec{v} \bullet \vec{w} = 0$.</span></p></div><p class="body-text">Our final lemma is considerably more technical, but it&#x2019;ll let us put all the pieces together.</p><div class="notes-lem notes-environment"><div class="notes-lem-title notes-title">Lemma: symmetric matrices have symmetric restrictions</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> symmetric matrix, let <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> be an eigenvector of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$,</span> and let <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span}\left\{ \vec{v} \right\}">$X = \operatorname{span}\left\{ \vec{v} \right\}$</span> Then if <span class="tex-holder inline-math" data-source-tex="\vec{w} \in X^\perp">$\vec{w} \in X^\perp$,</span> <span class="tex-holder inline-math" data-source-tex="A\vec{w} \in X^\perp">$A\vec{w} \in X^\perp$</span> too, and if <span class="tex-holder inline-math" data-source-tex="A|_{X^\perp} : X^\perp \to X^\perp">$A|_{X^\perp} : X^\perp \to X^\perp$</span> is the linear map given by restricting <span class="tex-holder inline-math" data-source-tex="A">$A$</span> to <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$,</span> then there is a basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> in which the matrix for <span class="tex-holder inline-math" data-source-tex="A|_{X^\perp}">$A|_{X^\perp}$</span> is symmetric.</p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">There are a ton of symbols here, but the moral of this lemma is that <span class="tex-holder inline-math" data-source-tex="A">$A$</span> can be split cleanly into its action on two subspaces: <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span}\left\{ \vec{v} \right\}">$X = \operatorname{span}\left\{ \vec{v} \right\}$</span> and <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$.</span> We already know that <span class="tex-holder inline-math" data-source-tex="A : X \to X">$A : X \to X$,</span> since <span class="tex-holder inline-math" data-source-tex="A \vec{v} = \lambda \vec{v} \in X">$A \vec{v} = \lambda \vec{v} \in X$,</span> and now we&#x2019;ll also show that <span class="tex-holder inline-math" data-source-tex="A : X^\perp \to X^\perp">$A : X^\perp \to X^\perp$</span> as the lemma states.</p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{w} \in X^\perp">$\vec{w} \in X^\perp$.</span> Then <span class="tex-holder inline-math" data-source-tex="\vec{w} \bullet \vec{v} = 0">$\vec{w} \bullet \vec{v} = 0$,</span> and so</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left( A\vec{w} \right) \bullet \vec{v} &= \left( A\vec{w} \right)^T \bullet \vec{v}\\[NEWLINE][TAB]&= \vec{w}^T A^T \vec{v}\\[NEWLINE][TAB]&= \vec{w}^T A \vec{v}\\[NEWLINE][TAB]&= \vec{w}^T \lambda \vec{v}\\[NEWLINE][TAB]&= \lambda \left( \vec{w} \bullet \vec{v} \right)\\[NEWLINE][TAB]&= 0,[NEWLINE]\end{align*}">$$\begin{align*}\left( A\vec{w} \right) \bullet \vec{v} &= \left( A\vec{w} \right)^T \bullet \vec{v}\\[4px]&= \vec{w}^T A^T \vec{v}\\[4px]&= \vec{w}^T A \vec{v}\\[4px]&= \vec{w}^T \lambda \vec{v}\\[4px]&= \lambda \left( \vec{w} \bullet \vec{v} \right)\\[4px]&= 0,\end{align*}$$</span></p><p class="body-text">meaning <span class="tex-holder inline-math" data-source-tex="A\vec{w} \in X^\perp">$A\vec{w} \in X^\perp$.</span></p><p class="body-text">Now let <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{w_1}, ..., \vec{w_{n - 1}} \right\}">$\left\{ \vec{w_1}, ..., \vec{w_{n - 1}} \right\}$</span> be an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> (via the Gram-Schmidt process). We can freely assume <span class="tex-holder inline-math" data-source-tex="\left| \left| \vec{v} \right| \right| = 1">$\left| \left| \vec{v} \right| \right| = 1$</span> since it&#x2019;s an eigenvector, and so what we&#x2019;ve shown is that the matrix of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with respect to the orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v}, \vec{w_1}, ..., \vec{w_{n - 1}} \right\}">$\left\{ \vec{v}, \vec{w_1}, ..., \vec{w_{n - 1}} \right\}$</span> for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right]\end{align*}$$</span></p><p class="body-text">for some matrix <span class="tex-holder inline-math" data-source-tex="C">$C$</span> &mdash; the second statement of this lemma is that <span class="tex-holder inline-math" data-source-tex="C">$C$</span> is also symmetric. If</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{cccc} \mid& \mid& & \mid \\ \vec{v}& \vec{w_1}& \cdots& \vec{w_{n - 1}} \\ \mid& \mid& & \mid \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{cccc} \mid& \mid& & \mid \\ \vec{v}& \vec{w_1}& \cdots& \vec{w_{n - 1}} \\ \mid& \mid& & \mid \end{array}\right],\end{align*}$$</span></p><p class="body-text">then what we&#x2019;ve shown is that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1}AB = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}B^{-1}AB = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right],\end{align*}$$</span></p><p class="body-text">and so transposing both sides results in</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^T A \left( B^{-1} \right)^T = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C^T& \\ 0 & && \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B^T A \left( B^{-1} \right)^T = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C^T& \\ 0 & && \end{array}\right].\end{align*}$$</span></p><p class="body-text">But since <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is a matrix whose columns form an orthonormal basis, it&#x2019;s unitary, and so <span class="tex-holder inline-math" data-source-tex="B^T = B^{-1}">$B^T = B^{-1}$.</span> Therefore,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1} A B = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C^T& \\ 0 & && \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}B^{-1} A B = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C^T& \\ 0 & && \end{array}\right],\end{align*}$$</span></p><p class="body-text">and so <span class="tex-holder inline-math" data-source-tex="C = C^T">$C = C^T$,</span> proving the result.</p></div><p class="body-text">At long last, we&#x2019;re ready to return to the Spectral theorem itself.</p><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof of the Real Spectral Theorem</div><p class="body-text">To begin, let&#x2019;s show that if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> of eigenvectors, then it must be symmetric. This is just a direct computation: we know <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$,</span> but <span class="tex-holder inline-math" data-source-tex="B">$B$</span> must be a unitary matrix, and so <span class="tex-holder inline-math" data-source-tex="B^{-1} = B^T">$B^{-1} = B^T$.</span> Then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^T &= \left( BDB^T \right)^T\\[NEWLINE][TAB]&= \left( B^T \right)^T D^T B^T\\[NEWLINE][TAB]&= BDB^T\\[NEWLINE][TAB]&= A,[NEWLINE]\end{align*}">$$\begin{align*}A^T &= \left( BDB^T \right)^T\\[4px]&= \left( B^T \right)^T D^T B^T\\[4px]&= BDB^T\\[4px]&= A,\end{align*}$$</span></p><p class="body-text">so <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is symmetric.</p><p class="body-text">The other direction is where all the complexity lies. If <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is a symmetric matrix with real entries, then by the first lemma, all of its eigenvalues and eigenvectors are real. By the second, any two eigenvectors from different eigenspaces are orthogonal, and we can arrange for eigenvectors within a single eigenspace to be orthogonal too: if <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> is the eigenspace corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$,</span> then we can just run the Gram-Schmidt process on <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> to produce an orthogonal basis.</p><p class="body-text">We&#x2019;re almost done! We&#x2019;ve shown that all the eigenvalues and eigenvectors are real, and that the eigenvectors are also orthogonal. The only substantive thing left to show is that there are enough eigenvectors &mdash; that the algebraic multiplicity of every eigenvalue is the same as the geometric multiplicity. This is where the final lemma comes in. Every matrix is guaranteed to have one eigenvector at the very least (the worst-case scenario is that there&#x2019;s only a single eigenvalue with algebraic multiplicity <span class="tex-holder inline-math" data-source-tex="n">$n$</span> and geometric multiplicity <span class="tex-holder inline-math" data-source-tex="1">$1$).</span> Let&#x2019;s call that guaranteed eigenvector for <span class="tex-holder inline-math" data-source-tex="A">$A$</span> <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$.</span> The third lemma tells us that if we take an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="\left( \operatorname{span}\left\{ \vec{v_1} \right\} \right)^\perp">$\left( \operatorname{span}\left\{ \vec{v_1} \right\} \right)^\perp$,</span> then <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is still a symmetric matrix when expressed in that basis. That means it&#x2019;s guaranteed to have another real eigenvector <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> that&#x2019;s orthogonal to <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$,</span> and so we can repeat this process <span class="tex-holder inline-math" data-source-tex="n">$n$</span> times until all that&#x2019;s left is a <span class="tex-holder inline-math" data-source-tex="1 \times 1">$1 \times 1$</span> matrix, which is always diagonalizable (since it&#x2019;s diagonal). In total, we have <span class="tex-holder inline-math" data-source-tex="n">$n$</span> distinct, orthogonal eigenvectors with real eigenvectors, and to cap off the proof, we can rescale every eigenvector to have magnitude <span class="tex-holder inline-math" data-source-tex="1">$1$,</span> making them orthonormal.</p></div><p class="body-text">That was a ton of work! It&#x2019;s easily the most complicated theorem we&#x2019;ll prove in the course. To wrap up this section, we&#x2019;ll state the more general version for complex matrices for completeness, although we won&#x2019;t have many uses for it in this class.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">The Complex Spectral Theorem</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix, and denote by <span class="tex-holder inline-math" data-source-tex="A^*">$A^*$</span> the <strong>conjugate transpose</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$;</span> i.e. <span class="tex-holder inline-math" data-source-tex="A^* = \overline{A}^T">$A^* = \overline{A}^T$.</span> Then <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has an orthonormal basis of eigenvectors if and only if <span class="tex-holder inline-math" data-source-tex="A^* A = A A^*">$A^* A = A A^*$</span> (which is called being a <strong>normal</strong> operator).</p></div><p class="body-text">We&#x2019;re approaching the end of the course! So far, we&#x2019;ve discussed eigenvectors and eigenvalues and inner products, and this section has concerned where the two intersect &mdash; specifically, that symmetric matrices are the ones that are diagonalizable with an orthonormal basis of eigenvalues. In the remaining two sections, we&#x2019;ll develop theory for the less-ideal cases: first, a way to very nearly diagonalize square nondiagonalizable matrices, and then a diagonalization-like process that applies to any matrix, square or not, and has wide-ranging applications.</p><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>