<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 2: Diagonalization</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">Suppose we have an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> whose eigenvectors <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$</span> form a basis and have corresponding eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$.</span> As we&#x2019;ve seen in the last section, it&#x2019;s most convenient to deal with <span class="tex-holder inline-math" data-source-tex="A">$A$</span> when we&#x2019;re using this basis. If <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is the matrix with columns <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$,</span> then <span class="tex-holder inline-math" data-source-tex="B^{-1}AB">$B^{-1}AB$</span> acts (from right to left) by converting a vector in the basis <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$</span> to the standard basis, applies <span class="tex-holder inline-math" data-source-tex="A">$A$</span> on it, and then converts the output back to <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$.</span> Specifically,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1}AB\vec{e_i} &= B^{-1}A\vec{v_i}\\[NEWLINE][TAB]&= B^{-1}\lambda_i \vec{v_i}\\[NEWLINE][TAB]&= \lambda_i\vec{e_i},[NEWLINE]\end{align*}">$$\begin{align*}B^{-1}AB\vec{e_i} &= B^{-1}A\vec{v_i}\\[4px]&= B^{-1}\lambda_i \vec{v_i}\\[4px]&= \lambda_i\vec{e_i},\end{align*}$$</span></p><p class="body-text">so the total matrix is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1}AB = \left[\begin{array}{cccc} \lambda_1& 0& \cdots& 0 \\ 0& \lambda_2& \cdots& 0 \\ \vdots& \vdots& \ddots& \vdots \\ 0& 0& \cdots& \lambda_n \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B^{-1}AB = \left[\begin{array}{cccc} \lambda_1& 0& \cdots& 0 \\ 0& \lambda_2& \cdots& 0 \\ \vdots& \vdots& \ddots& \vdots \\ 0& 0& \cdots& \lambda_n \end{array}\right].\end{align*}$$</span></p><p class="body-text">That matrix is what&#x2019;s called <strong>diagonal</strong>: only its diagonal entries are nonzero. Calling it <span class="tex-holder inline-math" data-source-tex="D">$D$,</span> we&#x2019;ve expressed <span class="tex-holder inline-math" data-source-tex="A">$A$</span> as <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$.</span> This puts the ideas of the previous section into a more symbolic form: the reason why this makes <span class="tex-holder inline-math" data-source-tex="A^{100}">$A^{100}$</span> easier to compute is that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^{100} &= \left( BDB^{-1} \right)^{100}\\[NEWLINE][TAB]&= BDB^{-1} BDB^{-1} \cdots BDB^{-1}\\[NEWLINE][TAB]&= BD^{100}B^{-1},[NEWLINE]\end{align*}">$$\begin{align*}A^{100} &= \left( BDB^{-1} \right)^{100}\\[4px]&= BDB^{-1} BDB^{-1} \cdots BDB^{-1}\\[4px]&= BD^{100}B^{-1},\end{align*}$$</span></p><p class="body-text">and since <span class="tex-holder inline-math" data-source-tex="D">$D$</span> is diagonal, <span class="tex-holder inline-math" data-source-tex="D^{100}">$D^{100}$</span> is just <span class="tex-holder inline-math" data-source-tex="D">$D$</span> with each entry raised to the 100th power. The process of expressing <span class="tex-holder inline-math" data-source-tex="A">$A$</span> in the form <span class="tex-holder inline-math" data-source-tex="BDB^{-1}">$BDB^{-1}$</span> is called <strong>diagonalizing</strong> <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> and it&#x2019;s possible exactly when <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is an invertible matrix &mdash; that is, when the eigenvectors of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are linearly independent. We know that&#x2019;s the case when the eigenvalues are all distinct, but when some eigenvalues are repeated, we&#x2019;ll need to do some more work.</p></section><h2 class="section-text"> Repeated Eigenvalues and Eigenspaces</h2><section><p class="body-text">Having <span class="tex-holder inline-math" data-source-tex="n">$n$</span> distinct eigenvalues might be a sufficient condition for a matrix to be diagonalizable, but we know it can&#x2019;t be necessary: the <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> identity matrix has the single eigenvalue of <span class="tex-holder inline-math" data-source-tex="1">$1$</span> repeated twice, and it&#x2019;s definitely diagonalizable, since it&#x2019;s already diagonal. On the other hand, there are matrices where the diagonalization process breaks down because of repeated eigenvalues. Let&#x2019;s take a look at one as a case study to see where things go wrong.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: repeated eigenvalues</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} -1& -3& 3 \\ -3& -2& 4 \\ -3& -4& 6 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} -1& -3& 3 \\ -3& -2& 4 \\ -3& -4& 6 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Find the eigenvectors and eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p><p class="body-text">We&#x2019;ll begin as usual by finding the roots of the characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$.</span> We have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= \det (A - \lambda I)\\[NEWLINE][TAB]&= (-1 - \lambda)((-2 - \lambda)(6 - \lambda) + 16) + 3(-3(6 - \lambda) + 12) + 3(12 + 3(-2 - \lambda))\\[NEWLINE][TAB]&= -\lambda^3 + 3\lambda^2 - 4.[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= \det (A - \lambda I)\\[4px]&= (-1 - \lambda)((-2 - \lambda)(6 - \lambda) + 16) + 3(-3(6 - \lambda) + 12) + 3(12 + 3(-2 - \lambda))\\[4px]&= -\lambda^3 + 3\lambda^2 - 4.\end{align*}$$</span></p><p class="body-text">Cubics are hard to factor, and unless there&#x2019;s an easy root of <span class="tex-holder inline-math" data-source-tex="\lambda = 0">$\lambda = 0$,</span> we may as well just ask a computer. This one factors as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= -(\lambda + 1)(\lambda - 2)^2,[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= -(\lambda + 1)(\lambda - 2)^2,\end{align*}$$</span></p><p class="body-text">so either <span class="tex-holder inline-math" data-source-tex="\lambda = -1">$\lambda = -1$</span> or <span class="tex-holder inline-math" data-source-tex="\lambda = 2">$\lambda = 2$.</span> For the first possibility, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ -3& -4& 7 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& -3& 3 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= -\frac{1}{3}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& 0& 3 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ 1& 0& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -\frac{1}{3}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ -3& -4& 7 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& -3& 3 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= -\frac{1}{3}\\[4px]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& 0& 3 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ 1& 0& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -\frac{1}{3}\end{align*}$$</span></p><p class="body-text">and so a solution is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Let&#x2019;s take a look at the other eigenvalue:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ -3& -4& 4 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ 0& -1& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 1& -1 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -1 \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ -\!\!= \vec{r_2}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ -3& -4& 4 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ 0& -1& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 1& 1& -1 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -1 \end{array}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ -\!\!= \vec{r_2}\end{align*}$$</span></p><p class="body-text">Our solution here is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} 0 \\ 1 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_2} = \left[\begin{array}{c} 0 \\ 1 \\ 1 \end{array}\right].\end{align*}$$</span></p></div><p class="body-text">There are only two linearly independent eigenvectors for <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> and so it&#x2019;s not diagonalizable. So what went wrong? The repeated eigenvalue of <span class="tex-holder inline-math" data-source-tex="\lambda = 2">$\lambda = 2$</span> feels like it might be the culprit: since it appeared as a root of the characteristic polynomial with multiplicity 2, we&#x2019;d expect there to be two linearly independent eigenvectors corresponding to it. Let&#x2019;s dig into that a little more and see if we can find exactly where our expectation deviates from reality.</p><p class="body-text">Suppose <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_k}">$\vec{v_1}, ..., \vec{v_k}$</span> are all the eigenvectors of an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with the eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$.</span> Then we can form a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> by extending <span class="tex-holder inline-math" data-source-tex="\{ \vec{v_1}, ..., \vec{v_k} \}">$\{ \vec{v_1}, ..., \vec{v_k} \}$</span> to</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\{ \vec{v_1}, ..., \vec{v_k}, \vec{v_{k+1}}, ..., \vec{v_n} \}.[NEWLINE]$$">$$\begin{align*}\{ \vec{v_1}, ..., \vec{v_k}, \vec{v_{k+1}}, ..., \vec{v_n} \}.\end{align*}$$</span></p><p class="body-text">If <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is the matrix with these <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> as columns (i.e. the change-of-basis matrix from this basis to the standard one), then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1}.[NEWLINE]\end{align*}">$$\begin{align*}A = B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1}.\end{align*}$$</span></p><p class="body-text">This is a matrix written in <em>block</em> form: in the top left is a <span class="tex-holder inline-math" data-source-tex="k \times k">$k \times k$</span> block with <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> down the diagonal, in the bottom right is an <span class="tex-holder inline-math" data-source-tex="(n - k) \times (n - k)">$(n - k) \times (n - k)$</span> block called <span class="tex-holder inline-math" data-source-tex="C_2">$C_2$</span> with arbitrary entries, and similarly for the other two blocks. What&#x2019;s important is that the bottom-left block is all zeros, since <span class="tex-holder inline-math" data-source-tex="A\vec{v_i} = \lambda_1\vec{v_i}">$A\vec{v_i} = \lambda_1\vec{v_i}$</span> for all <span class="tex-holder inline-math" data-source-tex="i">$i$</span> with <span class="tex-holder inline-math" data-source-tex="1 \leq i \leq k">$1 \leq i \leq k$.</span> If we take the characteristic polynomial of both sides, we find</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda I \right)\\[NEWLINE][TAB]&= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda (BIB^{-1}) \right)\\[NEWLINE][TAB]&= \det \left( B \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) B^{-1} \right)\\[NEWLINE][TAB]&= \det B \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) \det B^{-1}\\[NEWLINE][TAB]&= \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right).\\[NEWLINE][TAB]&= (\lambda_1 - \lambda)^k \chi_{C_2}(\lambda).[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda I \right)\\[4px]&= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda (BIB^{-1}) \right)\\[4px]&= \det \left( B \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) B^{-1} \right)\\[4px]&= \det B \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) \det B^{-1}\\[4px]&= \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right).\\[4px]&= (\lambda_1 - \lambda)^k \chi_{C_2}(\lambda).\end{align*}$$</span></p><p class="body-text">So, what has all this work told us? If a matrix has at least <span class="tex-holder inline-math" data-source-tex="k">$k$</span> linearly independent eigenvectors with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$,</span> then <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> appears as a root of the characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span> with multiplicity <em>at least</em> <span class="tex-holder inline-math" data-source-tex="k">$k$,</span> but possibly more. Let&#x2019;s give these two multiplicities names to properly distinguish them.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: algebraic and geometric multiplicity</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. The <strong>algebraic multiplicity</strong> of an eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is the power of the factor <span class="tex-holder inline-math" data-source-tex="(\lambda_i - \lambda)">$(\lambda_i - \lambda)$</span> in <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$.</span> The <strong>eigenspace</strong> <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> is the subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> of eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$,</span> and the <strong>geometric multiplicity</strong> of <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> is <span class="tex-holder inline-math" data-source-tex="\dim E_i">$\dim E_i$.</span></p><p class="body-text">The sum of the algebraic multiplicities of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is always equal to <span class="tex-holder inline-math" data-source-tex="n">$n$</span> (since <span class="tex-holder inline-math" data-source-tex="\deg \chi_A(\lambda) = n">$\deg \chi_A(\lambda) = n$),</span> but the sum of the geometric multiplicities is equal to <span class="tex-holder inline-math" data-source-tex="n">$n$</span> only if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is diagonalizable (and vice versa). Moreover, the geometric multiplicity of a <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> is at least 1 and no more than its algebraic multiplicity.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: eigenspaces</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix and let <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> be an eigenvalue of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> Show that the eigenspace <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> is actually a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span></p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: algebraic and geometric multiplicity</div><p class="body-text">Consider the matrices</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A_1 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 0 \\ 0& 0& 5 \end{array}\right], \quad A_2 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 1 \\ 0& 0& 5 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A_1 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 0 \\ 0& 0& 5 \end{array}\right], \quad A_2 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 1 \\ 0& 0& 5 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Both have characteristic polynomials of <span class="tex-holder inline-math" data-source-tex="(3 - \lambda)(5 - \lambda)^2">$(3 - \lambda)(5 - \lambda)^2$,</span> so their eigenvalues are <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 3">$\lambda_1 = 3$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_2 = 5">$\lambda_2 = 5$,</span> with algebraic multiplicities 1 and 2, respectively. The previous definition tells us that the geometric multiplicity of <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> should be 1 for both <span class="tex-holder inline-math" data-source-tex="A_1">$A_1$</span> and <span class="tex-holder inline-math" data-source-tex="A_2">$A_2$,</span> but that the geometric multiplicity of <span class="tex-holder inline-math" data-source-tex="\lambda_2">$\lambda_2$</span> will either be 1 or 2. Solving for the eigenvectors of <span class="tex-holder inline-math" data-source-tex="A_1">$A_1$,</span> we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|c} A_1 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 0 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{c|c} A_1 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 0 & 0 \\ 0& 0& 0 & 0 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|c} A_1 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 0 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[4px]\left[\begin{array}{c|c} A_1 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 0 & 0 \\ 0& 0& 0 & 0 \end{array}\right],\end{align*}$$</span></p><p class="body-text">so the eigenspaces <span class="tex-holder inline-math" data-source-tex="E_1">$E_1$</span> and <span class="tex-holder inline-math" data-source-tex="E_2">$E_2$</span> are</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[NEWLINE][TAB]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right] \right\},[NEWLINE]\end{align*}">$$\begin{align*}E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[4px]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right] \right\},\end{align*}$$</span></p><p class="body-text">meaning the geometric multiplicities of both <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_2">$\lambda_2$</span> are equal to their algebraic multiplicities. That means <span class="tex-holder inline-math" data-source-tex="A_1">$A_1$</span> is diagonalizable, which is pretty unsurprising since it&#x2019;s already diagonal. In contrast, <span class="tex-holder inline-math" data-source-tex="A_2">$A_2$</span> fails to be diagonalizable:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|c} A_2 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 1 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{c|c} A_2 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|c} A_2 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 1 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[4px]\left[\begin{array}{c|c} A_2 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right],\end{align*}$$</span></p><p class="body-text">so the eigenspaces are</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[NEWLINE][TAB]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right] \right\},[NEWLINE]\end{align*}">$$\begin{align*}E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[4px]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right] \right\},\end{align*}$$</span></p><p class="body-text">and so both eigenvalues have geometric multiplicities of 1. Later on, we&#x2019;ll develop a sense in which <span class="tex-holder inline-math" data-source-tex="A_2">$A_2$</span> is &#x201C;almost&#x201D; diagonal, or at least the closest we can get, but for now, it&#x2019;s a black-and-white distinction.</p></div></section><h2 class="section-text"> Complex Eigenvalues</h2><section><p class="body-text">There&#x2019;s one more facet of the characteristic polynomial to discuss: rather famously, not every polynomial in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}[x]">$\mathbb{R}[x]$</span> (i.e. with real coefficients) has roots in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}">$\mathbb{R}$.</span> For example, the characteristic polynomial of the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{cc} 0& 1 \\ -1& 0 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{cc} 0& 1 \\ -1& 0 \end{array}\right]\end{align*}$$</span></p><p class="body-text">is <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda) = \lambda^2 + 1">$\chi_A(\lambda) = \lambda^2 + 1$,</span> whose roots are <span class="tex-holder inline-math" data-source-tex="\pm i">$\pm i$</span> &mdash; complex numbers. To fully understand diagonalization in every possible case, we&#x2019;ll sometimes need to deal with complex eigenvalues. As a quick refresher, defining <span class="tex-holder inline-math" data-source-tex="i = \sqrt{-1}">$i = \sqrt{-1}$</span> produces a number system <span class="tex-holder inline-math" data-source-tex="\mathbb{C}">$\mathbb{C}$</span> (the complex numbers), that&#x2019;s <strong>algebraically closed</strong>, meaning any polynomial in <span class="tex-holder inline-math" data-source-tex="\mathbb{C}[x]">$\mathbb{C}[x]$</span> has roots in <span class="tex-holder inline-math" data-source-tex="\mathbb{C}">$\mathbb{C}$.</span> Moreover, if a polynomial in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}[x]">$\mathbb{R}[x]$</span> has complex roots, they come in conjugate pairs: if <span class="tex-holder inline-math" data-source-tex="a + bi">$a + bi$</span> is a root, then so is <span class="tex-holder inline-math" data-source-tex="a - bi">$a - bi$.</span> In fact, this even extends to the eigenvectors.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: complex eigenvectors</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix with real entries that has a pair of complex conjugate eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1 = a + bi">$\lambda_1 = a + bi$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_2 = a - bi">$\lambda_2 = a - bi$.</span> If <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{C}^n">$\vec{v} \in \mathbb{C}^n$</span> is an eigenvector corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$,</span> then <span class="tex-holder inline-math" data-source-tex="\overline{\vec{v}}">$\overline{\vec{v}}$</span> (the vector formed by conjugating every entry of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$)</span> is an eigenvector corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_2">$\lambda_2$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="A\vec{v} = \lambda_1\vec{v}">$A\vec{v} = \lambda_1\vec{v}$,</span> conjugating both sides results in <span class="tex-holder inline-math" data-source-tex="\overline{A\vec{v}} = \overline{\lambda_1\vec{v}}">$\overline{A\vec{v}} = \overline{\lambda_1\vec{v}}$.</span> Now conjugation splits over addition and multiplication of complex numbers &mdash; it&#x2019;s easy but not particularly interesting to check that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{(a + bi) + (c + di)} &= (a - bi) + (c - di),\\[NEWLINE][TAB]\overline{(a + bi)(c + di)} &= (a - bi)(c - di).[NEWLINE]\end{align*}">$$\begin{align*}\overline{(a + bi) + (c + di)} &= (a - bi) + (c - di),\\[4px]\overline{(a + bi)(c + di)} &= (a - bi)(c - di).\end{align*}$$</span></p><p class="body-text">The result is now just a (slightly tedious) computation:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{A\vec{v}} &= \overline{\left[\begin{array}{ccc} a_{11}& \cdots& a_{1n} \\ \vdots& \ddots& \vdots \\ a_{n1}& \cdots& a_{nn} \end{array}\right] \left[\begin{array}{c} v_1 \\ \vdots \\ v_n \end{array}\right]}\\[NEWLINE][TAB]&= \overline{\left[\begin{array}{c} a_{11} v_1 + \cdots + a_{1n}v_n \\ \vdots \\ a_{n1} v_1 + \cdots + a_{nn}v_n \end{array}\right]}\\[NEWLINE][TAB]&= \left[\begin{array}{c} \overline{a_{11} v_1 + \cdots + a_{1n}v_n} \\ \vdots \\ \overline{a_{n1} v_1 + \cdots + a_{nn}v_n} \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{c} \overline{a_{11}} \cdot \overline{v_1} + \cdots + \overline{a_{1n}} \cdot \overline{v_n} \\ \vdots \\ \overline{a_{n1}} \cdot \overline{v_1} + \cdots + \overline{a_{nn}} \cdot \overline{v_n} \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{c} a_{11} \cdot \overline{v_1} + \cdots + a_{1n} \cdot \overline{v_n} \\ \vdots \\ a_{n1} \cdot \overline{v_1} + \cdots + a_{nn} \cdot \overline{v_n} \end{array}\right]\\[NEWLINE][TAB]&= A\overline{\vec{v}},[NEWLINE]\end{align*}">$$\begin{align*}\overline{A\vec{v}} &= \overline{\left[\begin{array}{ccc} a_{11}& \cdots& a_{1n} \\ \vdots& \ddots& \vdots \\ a_{n1}& \cdots& a_{nn} \end{array}\right] \left[\begin{array}{c} v_1 \\ \vdots \\ v_n \end{array}\right]}\\[4px]&= \overline{\left[\begin{array}{c} a_{11} v_1 + \cdots + a_{1n}v_n \\ \vdots \\ a_{n1} v_1 + \cdots + a_{nn}v_n \end{array}\right]}\\[4px]&= \left[\begin{array}{c} \overline{a_{11} v_1 + \cdots + a_{1n}v_n} \\ \vdots \\ \overline{a_{n1} v_1 + \cdots + a_{nn}v_n} \end{array}\right]\\[4px]&= \left[\begin{array}{c} \overline{a_{11}} \cdot \overline{v_1} + \cdots + \overline{a_{1n}} \cdot \overline{v_n} \\ \vdots \\ \overline{a_{n1}} \cdot \overline{v_1} + \cdots + \overline{a_{nn}} \cdot \overline{v_n} \end{array}\right]\\[4px]&= \left[\begin{array}{c} a_{11} \cdot \overline{v_1} + \cdots + a_{1n} \cdot \overline{v_n} \\ \vdots \\ a_{n1} \cdot \overline{v_1} + \cdots + a_{nn} \cdot \overline{v_n} \end{array}\right]\\[4px]&= A\overline{\vec{v}},\end{align*}$$</span></p><p class="body-text">where the second-to-last step is because every entry of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is real. On the other hand,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{A\vec{v}} &= \overline{\lambda_1\vec{v}}\\[NEWLINE][TAB]&= \overline{\lambda_1} \cdot \overline{\vec{v}}\\[NEWLINE][TAB]&= \lambda_2 \overline{\vec{v}}.[NEWLINE]\end{align*}">$$\begin{align*}\overline{A\vec{v}} &= \overline{\lambda_1\vec{v}}\\[4px]&= \overline{\lambda_1} \cdot \overline{\vec{v}}\\[4px]&= \lambda_2 \overline{\vec{v}}.\end{align*}$$</span></p><p class="body-text">In total, <span class="tex-holder inline-math" data-source-tex="A\overline{\vec{v}} = \lambda_2 \overline{\vec{v}}">$A\overline{\vec{v}} = \lambda_2 \overline{\vec{v}}$,</span> as required.</p></div><p class="body-text">Although it&#x2019;s a little frustrating that we&#x2019;ll have to work with complex numbers even when our matrices have all real entries, the good news is that linear algebra works just as well over <span class="tex-holder inline-math" data-source-tex="\mathbb{C}">$\mathbb{C}$</span> as over <span class="tex-holder inline-math" data-source-tex="\mathbb{R}">$\mathbb{R}$,</span> so there won&#x2019;t be too much trouble. Let&#x2019;s start by finding some complex eigenvalues and eigenvectors and then see what insights we can draw.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: complex eigenvalues</div><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} 3& 0& -3 \\ -2& 5& 2 \\ -4& -5& 4 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} 3& 0& -3 \\ -2& 5& 2 \\ -4& -5& 4 \end{array}\right].\end{align*}$$</span></p><p class="body-text">The process of finding <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span> is the same as always:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= (3 - \lambda)((5 - \lambda)(4 - \lambda) + 10) - 3(10 + 4(5 - \lambda))\\[NEWLINE][TAB]&= -\lambda^3 + 12\lambda^2 - 45\lambda\\[NEWLINE][TAB]&= -\lambda(\lambda^2 - 12\lambda + 45).[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= (3 - \lambda)((5 - \lambda)(4 - \lambda) + 10) - 3(10 + 4(5 - \lambda))\\[4px]&= -\lambda^3 + 12\lambda^2 - 45\lambda\\[4px]&= -\lambda(\lambda^2 - 12\lambda + 45).\end{align*}$$</span></p><p class="body-text">So <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 0">$\lambda_1 = 0$</span> is an eigenvalue. To find the other two, we can use the quadratic formula:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\lambda &= \frac{12 \pm \sqrt{144 - 180}}{2}\\[NEWLINE][TAB]&= \frac{12 \pm \sqrt{-36}}{2}\\[NEWLINE][TAB]&= 6 \pm 3i.[NEWLINE]\end{align*}">$$\begin{align*}\lambda &= \frac{12 \pm \sqrt{144 - 180}}{2}\\[4px]&= \frac{12 \pm \sqrt{-36}}{2}\\[4px]&= 6 \pm 3i.\end{align*}$$</span></p><p class="body-text">As expected, the complex roots are occurring in conjugate pairs. The eigenvector corresponding to <span class="tex-holder inline-math" data-source-tex="0">$0$</span> follows the usual steps:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 3& 0& -3 & 0 \\ -2& 5& 2 & 0 \\ -4& -5& 4 & 0 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& -1 & 0 \\ -2& 5& 2 & 0 \\ -4& -5& 4 & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= \frac{1}{3}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& -1 & 0 \\ 0& 5& 0 & 0 \\ 0& -5& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 2\vec{r_1} \\ \vec{r_3} \ +\!\!= 4\vec{r_1} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& -1 & 0 \\ 0& 5& 0 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ +\!\!= \vec{r_2}\\[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 1 \\ 0 \\ 1 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 3& 0& -3 & 0 \\ -2& 5& 2 & 0 \\ -4& -5& 4 & 0 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} 1& 0& -1 & 0 \\ -2& 5& 2 & 0 \\ -4& -5& 4 & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= \frac{1}{3}\\[4px]\left[\begin{array}{ccc|c} 1& 0& -1 & 0 \\ 0& 5& 0 & 0 \\ 0& -5& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ +\!\!= 2\vec{r_1} \\ \vec{r_3} \ +\!\!= 4\vec{r_1} \end{array}\\[4px]\left[\begin{array}{ccc|c} 1& 0& -1 & 0 \\ 0& 5& 0 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ +\!\!= \vec{r_2}\\[4px]\vec{v_1} = \left[\begin{array}{c} 1 \\ 0 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Since the remaining two eigenvalues are a conjugate pair, we only have to find an eigenvector for one of them, and then the other will be its conjugate. With <span class="tex-holder inline-math" data-source-tex="\lambda_2 = 6 + 3i">$\lambda_2 = 6 + 3i$,</span> we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3 - 3i& 0& -3 & 0 \\ -2& -1 - 3i& 2 & 0 \\ -4& -5& -2 - 3i & 0 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1 + i& 0& 1 & 0 \\ -2& -1 - 3i& 2 & 0 \\ -4& -5& -2 - 3i & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= -\frac{1}{3}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ -2& -1 - 3i& 2 & 0 \\ -4& -5& -2 - 3i & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= 1 - i\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& -1 - 3i& 3 - i & 0 \\ 0& -5& -5i & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_2} \ +\!\!= \vec{r_1} \\ \vec{r_3} \ +\!\!= 2\vec{r_1} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& -1 - 3i& 3 - i & 0 \\ 0& 1& i & 0 \end{array}\right] & \quad \vec{r_3} \ \times\!\!= -\frac{1}{5}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& 1& i & 0 \\ 0& -1 - 3i& 3 - i & 0 \end{array}\right] & \quad \operatorname{swap} \vec{r_2}, \vec{r_3}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& 1& i & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ +\!\!= (1 + 3i)\vec{r_2}\\[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} -1 + i \\ -2i \\ 2 \end{array}\right] = \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] + \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right]i.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} -3 - 3i& 0& -3 & 0 \\ -2& -1 - 3i& 2 & 0 \\ -4& -5& -2 - 3i & 0 \end{array}\right] &\\[4px]\left[\begin{array}{ccc|c} 1 + i& 0& 1 & 0 \\ -2& -1 - 3i& 2 & 0 \\ -4& -5& -2 - 3i & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= -\frac{1}{3}\\[4px]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ -2& -1 - 3i& 2 & 0 \\ -4& -5& -2 - 3i & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= 1 - i\\[4px]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& -1 - 3i& 3 - i & 0 \\ 0& -5& -5i & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_2} \ +\!\!= \vec{r_1} \\ \vec{r_3} \ +\!\!= 2\vec{r_1} \end{array}\\[4px]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& -1 - 3i& 3 - i & 0 \\ 0& 1& i & 0 \end{array}\right] & \quad \vec{r_3} \ \times\!\!= -\frac{1}{5}\\[4px]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& 1& i & 0 \\ 0& -1 - 3i& 3 - i & 0 \end{array}\right] & \quad \operatorname{swap} \vec{r_2}, \vec{r_3}\\[4px]\left[\begin{array}{ccc|c} 2& 0& 1 - i & 0 \\ 0& 1& i & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ +\!\!= (1 + 3i)\vec{r_2}\\[4px]\vec{v_2} = \left[\begin{array}{c} -1 + i \\ -2i \\ 2 \end{array}\right] = \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] + \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right]i.\end{align*}$$</span></p><p class="body-text">The final eigenvector <span class="tex-holder inline-math" data-source-tex="\vec{v_3}">$\vec{v_3}$</span> corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_3">$\lambda_3$</span> is then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_3} = \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] - \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right]i.[NEWLINE]$$">$$\begin{align*}\vec{v_3} = \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] - \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right]i.\end{align*}$$</span></p><p class="body-text">The eigenvalues <em>and</em> eigenvectors being complex might seem like there&#x2019;s no hope of diagonalizing the matrix at all, but we can actually get shockingly close. Let&#x2019;s look at the action of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> not on <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_3}">$\vec{v_3}$,</span> but their real and imaginary parts separately:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] &= \left[\begin{array}{c} -9 \\ 6 \\ 12 \end{array}\right] = 6 \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] - 3 \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right]\\[NEWLINE][TAB]A \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right] &= \left[\begin{array}{c} 3 \\ -12 \\ 6 \end{array}\right] = 3 \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] + 6 \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] &= \left[\begin{array}{c} -9 \\ 6 \\ 12 \end{array}\right] = 6 \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] - 3 \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right]\\[4px]A \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right] &= \left[\begin{array}{c} 3 \\ -12 \\ 6 \end{array}\right] = 3 \left[\begin{array}{c} -1 \\ 0 \\ 2 \end{array}\right] + 6 \left[\begin{array}{c} 1 \\ -2 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">We&#x2019;ve come very close to diagonalizing <span class="tex-holder inline-math" data-source-tex="A">$A$:</span> while the outputs of these two vectors aren&#x2019;t scalar multiples of themselves like eigenvectors, they&#x2019;re linear combinations of one another &mdash; so instead of a perfectly diagonal matrix, we&#x2019;ll have a <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> block in the lower right.</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} 1& -1& 1 \\ 0& 0& -2 \\ 1& 2& 0 \end{array}\right] \left[\begin{array}{ccc} 0& 0& 0 \\ 0& 6& 3 \\ 0& -3& 6 \end{array}\right] \left[\begin{array}{ccc} 1& -1& 1 \\ 0& 0& -2 \\ 1& 2& 0 \end{array}\right]^{-1}.[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} 1& -1& 1 \\ 0& 0& -2 \\ 1& 2& 0 \end{array}\right] \left[\begin{array}{ccc} 0& 0& 0 \\ 0& 6& 3 \\ 0& -3& 6 \end{array}\right] \left[\begin{array}{ccc} 1& -1& 1 \\ 0& 0& -2 \\ 1& 2& 0 \end{array}\right]^{-1}.\end{align*}$$</span></p><p class="body-text">How are we supposed to interpret this geometrically? When all the eigenvalues and eigenvectors of a diagonalizable matrix were real, the eigenvectors were the axes along which the matrix exclusively scaled, but now there are no real eigenvectors in the <span class="tex-holder inline-math" data-source-tex="yz">$yz$-plane.</span> Instead, let&#x2019;s focus in on the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} 6& 3 \\ -3& 6 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} 6& 3 \\ -3& 6 \end{array}\right].\end{align*}$$</span></p><p class="body-text">The entries might look familiar: they follow the same pattern as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} \cos(\theta)& -\sin(\theta) \\ \sin(\theta)& \cos(\theta) \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} \cos(\theta)& -\sin(\theta) \\ \sin(\theta)& \cos(\theta) \end{array}\right],\end{align*}$$</span></p><p class="body-text">which is the matrix that acts on <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> by rotating counterclockwise by <span class="tex-holder inline-math" data-source-tex="\theta">$\theta$.</span> By factoring out a number from our matrix so that the rows have magnitude <span class="tex-holder inline-math" data-source-tex="1">$1$,</span> we can express our matrix as a constant times a rotation matrix:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} 6& 3 \\ -3& 6 \end{array}\right] &= \sqrt{45} \left[\begin{array}{cc} \frac{6}{\sqrt{45}}& \frac{3}{\sqrt{45}} \\ -\frac{3}{\sqrt{45}}& \frac{6}{\sqrt{45}} \end{array}\right]\\[NEWLINE][TAB]&\approx \sqrt{45} \left[\begin{array}{cc} \cos(-0.4636)& \sin(-0.4636) \\ -\sin(-0.4636)& \cos(-0.4636) \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} 6& 3 \\ -3& 6 \end{array}\right] &= \sqrt{45} \left[\begin{array}{cc} \frac{6}{\sqrt{45}}& \frac{3}{\sqrt{45}} \\ -\frac{3}{\sqrt{45}}& \frac{6}{\sqrt{45}} \end{array}\right]\\[4px]&\approx \sqrt{45} \left[\begin{array}{cc} \cos(-0.4636)& \sin(-0.4636) \\ -\sin(-0.4636)& \cos(-0.4636) \end{array}\right].\end{align*}$$</span></p><p class="body-text">So in total, this matrix scales its inputs by <span class="tex-holder inline-math" data-source-tex="\sqrt{45} \approx 6.71">$\sqrt{45} \approx 6.71$</span> and rotates them by about <span class="tex-holder inline-math" data-source-tex="0.4636">$0.4636$</span> radians (about <span class="tex-holder inline-math" data-source-tex="22.6^\circ">$22.6^\circ$)</span> clockwise.</p><div class="desmos-border"><div id="rotation-matrix" class="desmos-container"></div></div><p class="body-text">Here, the blue vector is the output of multiplying the purple one by this matrix.</p></div><p class="body-text">This is more than a motivating example: it holds in general.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: block diagonalization for complex eigenvalues</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> diagonalizable matrix with real entries, eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$,</span> and corresponding eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> (possibly repeated). Define <span class="tex-holder inline-math" data-source-tex="\vec{w_1}, ..., \vec{w_n}">$\vec{w_1}, ..., \vec{w_n}$</span> as follows: if <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> (and therefore <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$)</span> is real, then <span class="tex-holder inline-math" data-source-tex="\vec{w_i} = \vec{v_i}">$\vec{w_i} = \vec{v_i}$.</span> Otherwise, <span class="tex-holder inline-math" data-source-tex="\lambda_i = a + bi">$\lambda_i = a + bi$,</span> and another eigenvalue, say <span class="tex-holder inline-math" data-source-tex="\lambda_{i + 1}">$\lambda_{i + 1}$,</span> is <span class="tex-holder inline-math" data-source-tex="a - bi">$a - bi$.</span> Moreover, the eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{v_{i + 1}}">$\vec{v_{i + 1}}$</span> are conjugates. In that case, we define <span class="tex-holder inline-math" data-source-tex="\vec{w_i} = \operatorname{Re} \vec{v_i}">$\vec{w_i} = \operatorname{Re} \vec{v_i}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w_{i + 1}} = \operatorname{Im} \vec{v_i}">$\vec{w_{i + 1}} = \operatorname{Im} \vec{v_i}$</span> (i.e. the real and imaginary parts). Then <span class="tex-holder inline-math" data-source-tex="\{\vec{w_1}, ..., \vec{w_n}\}">$\{\vec{w_1}, ..., \vec{w_n}\}$</span> is a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> and if <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is the matrix with <span class="tex-holder inline-math" data-source-tex="\vec{w_i}">$\vec{w_i}$</span> as its columns and <span class="tex-holder inline-math" data-source-tex="D">$D$</span> is the nearly-diagonal matrix with entries of <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> for real <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> and <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> blocks</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} a& b \\ -b& a \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} a& b \\ -b& a \end{array}\right]\end{align*}$$</span></p><p class="body-text">for any pair of complex eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_i = a + bi">$\lambda_i = a + bi$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_{i + 1} = a - bi">$\lambda_{i + 1} = a - bi$,</span> then <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$.</span></p></div><p class="body-text">This theorem is so dense with math that it&#x2019;s almost more trouble than it&#x2019;s worth, but when we look past the symbols to the meaning, it&#x2019;s valuable to know. A diagonalizable matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with real entries and all real eigenvalues diagonalizes <strong>over <span class="tex-holder inline-math" data-source-tex="\mathbb{R}">$\mathbb{R}$</strong>,</span> in the sense that we can write it as <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$</span> for real matrices <span class="tex-holder inline-math" data-source-tex="B">$B$</span> and <span class="tex-holder inline-math" data-source-tex="D">$D$.</span> If <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is diagonalizable and real entries but not all real eigenvalues, though, we have a choice to make: we can diagonalize it over <span class="tex-holder inline-math" data-source-tex="\mathbb{C}">$\mathbb{C}$,</span> but it&#x2019;s often more useful to get as close as we can without using complex numbers in the expansion. To that end, the previous theorem tells us that if we replace adjacent conjugate columns in <span class="tex-holder inline-math" data-source-tex="B">$B$</span> by the real and imaginary parts of one of them, and also replace <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> blocks of <span class="tex-holder inline-math" data-source-tex="D">$D$</span> with the following move:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\begin{array}{cc} a + bi & 0 \\ 0 & a - bi \end{array} \ \longmapsto\ \begin{array}{cc} a & b \\ -b & a \end{array}[NEWLINE]\end{align*}">$$\begin{align*}\begin{array}{cc} a + bi & 0 \\ 0 & a - bi \end{array} \ \longmapsto\ \begin{array}{cc} a & b \\ -b & a \end{array}\end{align*}$$</span></p><p class="body-text">then the resulting expression <span class="tex-holder inline-math" data-source-tex="BDB^{-1}">$BDB^{-1}$</span> is both all real and still equals <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">To show this result, let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be a diagonalizable <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix with all real entries and suppose <span class="tex-holder inline-math" data-source-tex="a + bi">$a + bi$</span> is an eigenvalue with algebraic multiplicity <span class="tex-holder inline-math" data-source-tex="k">$k$.</span> Then <span class="tex-holder inline-math" data-source-tex="a - bi">$a - bi$</span> is also an eigenvalue with algebraic multiplicity <span class="tex-holder inline-math" data-source-tex="k">$k$,</span> and since <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is diagonalizable, the geometric multiplicities of both are also <span class="tex-holder inline-math" data-source-tex="k">$k$.</span> Let&#x2019;s call their eigenspaces <span class="tex-holder inline-math" data-source-tex="E_1">$E_1$</span> and <span class="tex-holder inline-math" data-source-tex="E_2">$E_2$,</span> respectively; if <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_k}\}">$\{\vec{v_1}, ..., \vec{v_k}\}$</span> is a basis for <span class="tex-holder inline-math" data-source-tex="E_1">$E_1$,</span> then <span class="tex-holder inline-math" data-source-tex="\{\overline{\vec{v_1}}, ..., \overline{\vec{v_k}}\}">$\{\overline{\vec{v_1}}, ..., \overline{\vec{v_k}}\}$</span> is a basis for <span class="tex-holder inline-math" data-source-tex="E_2">$E_2$.</span> Now since <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is diagonalizable, all <span class="tex-holder inline-math" data-source-tex="2k">$2k$</span> of these vectors are linearly independent, and so is the collection of linearly transformed vectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\frac{1}{2}\left( \vec{v_1} + \overline{\vec{v_1}} \right), \frac{1}{2}\left( \vec{v_1} - \overline{\vec{v_1}} \right), ..., \frac{1}{2}\left( \vec{v_n} + \overline{\vec{v_n}} \right), \frac{1}{2}\left( \vec{v_n} - \overline{\vec{v_n}} \right).[NEWLINE]$$">$$\begin{align*}\frac{1}{2}\left( \vec{v_1} + \overline{\vec{v_1}} \right), \frac{1}{2}\left( \vec{v_1} - \overline{\vec{v_1}} \right), ..., \frac{1}{2}\left( \vec{v_n} + \overline{\vec{v_n}} \right), \frac{1}{2}\left( \vec{v_n} - \overline{\vec{v_n}} \right).\end{align*}$$</span></p><p class="body-text">Now for any complex number <span class="tex-holder inline-math" data-source-tex="z">$z$,</span> <span class="tex-holder inline-math" data-source-tex="\operatorname{Re} z = \frac{1}{2}\left( z + \overline{z} \right)">$\operatorname{Re} z = \frac{1}{2}\left( z + \overline{z} \right)$</span> and <span class="tex-holder inline-math" data-source-tex="\operatorname{Im} z = -\frac{i}{2}\left( z - \overline{z} \right)">$\operatorname{Im} z = -\frac{i}{2}\left( z - \overline{z} \right)$:</span> if <span class="tex-holder inline-math" data-source-tex="z = c + di">$z = c + di$,</span> then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\frac{1}{2}\left( z + \overline{z} \right) &= \frac{1}{2}\left( c + di + c - di \right) = \frac{1}{2}(2c) = c\\[NEWLINE][TAB]-\frac{i}{2}\left( z - \overline{z} \right) &= \frac{1}{2}\left( c + di - c + di \right) = -\frac{i}{2}(2di) = d.[NEWLINE]\end{align*}">$$\begin{align*}\frac{1}{2}\left( z + \overline{z} \right) &= \frac{1}{2}\left( c + di + c - di \right) = \frac{1}{2}(2c) = c\\[4px]-\frac{i}{2}\left( z - \overline{z} \right) &= \frac{1}{2}\left( c + di - c + di \right) = -\frac{i}{2}(2di) = d.\end{align*}$$</span></p><p class="body-text">So these new <span class="tex-holder inline-math" data-source-tex="2k">$2k$</span> vectors are exactly the new columns of <span class="tex-holder inline-math" data-source-tex="B">$B$</span> from the theorem, and all that remains is to check that <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has the correct effect on them. For any <span class="tex-holder inline-math" data-source-tex="\vec{v_j}">$\vec{v_j}$,</span> we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A\left( \operatorname{Re} \vec{v_j} \right) &= A\left( \frac{1}{2}\left( \vec{v_j} + \overline{\vec{v_j}} \right) \right)\\[NEWLINE][TAB]&= \frac{1}{2} \left( (a + bi)\vec{v_j} + (a - bi)\overline{\vec{v_j}} \right)\\[NEWLINE][TAB]&= \frac{a}{2} \vec{v_j} + \frac{a}{2} \overline{\vec{v_j}} + \frac{bi}{2} \vec{v_j} - \frac{bi}{2} \overline{\vec{v_j}}\\[NEWLINE][TAB]&= \frac{a}{2}\left( \vec{v_j} + \overline{\vec{v_j}} \right) + \frac{bi}{2}\left( \vec{v_j} - \overline{\vec{v_j}} \right)\\[NEWLINE][TAB]&= a \operatorname{Re}\vec{v_j} - b\operatorname{Im}\vec{v_j}.[NEWLINE]\end{align*}">$$\begin{align*}A\left( \operatorname{Re} \vec{v_j} \right) &= A\left( \frac{1}{2}\left( \vec{v_j} + \overline{\vec{v_j}} \right) \right)\\[4px]&= \frac{1}{2} \left( (a + bi)\vec{v_j} + (a - bi)\overline{\vec{v_j}} \right)\\[4px]&= \frac{a}{2} \vec{v_j} + \frac{a}{2} \overline{\vec{v_j}} + \frac{bi}{2} \vec{v_j} - \frac{bi}{2} \overline{\vec{v_j}}\\[4px]&= \frac{a}{2}\left( \vec{v_j} + \overline{\vec{v_j}} \right) + \frac{bi}{2}\left( \vec{v_j} - \overline{\vec{v_j}} \right)\\[4px]&= a \operatorname{Re}\vec{v_j} - b\operatorname{Im}\vec{v_j}.\end{align*}$$</span></p><p class="body-text">Similarly,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]A\left( \operatorname{Im} \vec{v_j} \right) = b \operatorname{Re}\vec{v_j} + a \operatorname{Im}\vec{v_j},[NEWLINE]$$">$$\begin{align*}A\left( \operatorname{Im} \vec{v_j} \right) = b \operatorname{Re}\vec{v_j} + a \operatorname{Im}\vec{v_j},\end{align*}$$</span></p><p class="body-text">showing the result.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: block diagonalization</div><p class="body-text">Find the eigenvectors and eigenvalues of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} 2& 1& 2 \\ 1& 0& -6 \\ -1& 1& 5 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} 2& 1& 2 \\ 1& 0& -6 \\ -1& 1& 5 \end{array}\right]\end{align*}$$</span></p><p class="body-text">and use them to write <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$</span> for real matrices <span class="tex-holder inline-math" data-source-tex="B">$B$</span> and <span class="tex-holder inline-math" data-source-tex="D">$D$,</span> where <span class="tex-holder inline-math" data-source-tex="D">$D$</span> is block diagonal with at most <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> blocks.</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>