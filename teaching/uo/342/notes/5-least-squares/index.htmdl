### nav-buttons

After the behemoth that was the last section, let's take the opportunity to apply linear algebra to some more concrete problems. When we solve a matrix equation $A\vec{x} = \vec{b}$ currently, there are three different possible outcomes: either there's exactly one solution for $\vec{x}$, there are infinitely many (i.e. the solution contains one or more free parameters), or there are no solutions. Up to this point, we haven't been able to handle that last case beyond saying that we're unable to solve the system, but now that we have the language of the previous section, we're much better prepared to discuss it in more depth. In particular, what's the closest we can get? For any value of $\vec{x}$, $A\vec{x}$ is a linear combination of the columns of $A$ with weights given by the entries of $\vec{x}$, so $A\vec{x} = \vec{b}$ is solvable exactly when $\vec{b} \in \image A$, which is just the span of the columns of $A$. If that's not the case, then the closest vector that *is* in the image is given by a projection. Specifically, the closest output is $\vec{b}'$, where

$$
	\vec{b}' = \proj_{\image A} \vec{b}.
$$

So how can we solve $A\vec{x}' = \vec{b}'$ for a solution $\vec{x}'$? We can't use the convenient projection formula from the last section without an orthonormal basis for the image of $A$, but we do know that the vectors $\vec{b}'$ and $\vec{b} - \vec{b}'$ are orthogonal because the latter necessarily lives in the orthogonal complement of $\image A$. That means that the dot product of $\vec{b} - \vec{b}'$ with any column of $A$ is zero, and so

$$
	\left( \vec{b} - \vec{b}' \right)^T A = \vec{0}^T,
$$

since every component of that vector is one of those dot products. That's a very awkward way to write the product, though, and we're better off transposing both sides, remembering that transposing a matrix product reverses the order of the factors:

$$
	A^T \left( \vec{b} - \vec{b}' \right) &= \vec{0}

	A^T \vec{b}' &= A^T \vec{b}

	A^T A \vec{x}' &= A^T \vec{b}.
$$

And now this is a standard matrix equation we can solve for $\vec{x}'$! The moral here is that if $A\vec{x} = \vec{b}$ is unsolvable, multiplying both sides by $A^T$ produces a solvable equation whose solution is the best approximation to $\vec{b}$. Since "best" means minimizing $\left| \left| \vec{b}' - \vec{b} \right| \right|$, or equivalently minimizing

$$
	\left| \left| \vec{b}' - \vec{b} \right| \right|^2 = \left( \vec{b}' - \vec{b} \right)_1^2 + \cdots + \left( \vec{b}' - \vec{b} \right)_n^2,
$$

these approximation problems are called **least-squares problems**.

To wrap up the discussion, let's see when a least-squares solution is unique. Since we're solving $A^T A \vec{x}' = A^T \vec{b}$, that's exactly when $A^T A$ is invertible, regardless of $\vec{b}$. That's not a particularly convenient condition, so let's explore it a little further. Even though $A^T A$ is square, $A$ itself might not be. However, since $A^T A$ is invertible, $\ker A^T A = \left\{ \vec{0} \right\}$, and so $\ker A = \left\{ \vec{0} \right\}$ too --- since any vector in the kernel of $A$ is also in the kernel of $A^T A$. That means $A$ has linearly independent columns, and the reverse is true too: if $\ker A = \left\{ \vec{0} \right\}$, then any vector $\vec{x}$ satisfying $A^T A\vec{x} = \vec{0}$ also satisfies

$$
	\vec{x} \bullet \left( A^T A \vec{x} \right) &= 0

	\vec{x}^T A^T A \vec{x} &= 0

	\left( A \vec{x} \right)^T \left( A \vec{x} \right) &= 0

	\left( A \vec{x} \right) \bullet \left( A \vec{x} \right) &= 0

	A\vec{x} &= \vec{0}

	\vec{x} &= \vec{0}.
$$

In so many symbols, $\ker \left( A^T A \right) = \left\{ \vec{0} \right\}$, so $A^T A$ is invertible. It's been a bit of symbol-pushing, but we've solved and characterized least-squares problems in general.

### thm "least-squares approximations"

	Let $A$ be an $m \times n$ matrix and let $\vec{b} \in #R#^m$. A vector $\vec{x}' \in #R#^n$ that minimizes $\left| \left| A\vec{x}' - \vec{b} \right| \right|$ is the projection of $\vec{b}$ onto the image of $A$, or equivalently, a solution to $A^T A \vec{x}' = A^T \vec{b}$. Moreover, this least-squares solution $\vec{x}'$ is unique exactly when the columns of $A$ are linearly independent.

###

### ex "least-squares approximations"

	Find the least-squares solution to

	$$
		[[ 1, 0 ; 1, 1 ; 2, 1 ]]\vec{x} = [[ 2 ; 1 ; 0 ]].
	$$

	One advantage of the least-squares approach is that if the matrix equation does actually have a solution, we'll find it anyway, so we don't need to check that before we start the process. Multiplying both sides by the transpose of the coefficient matrix, we have

	$$
		[[ 1, 1, 2 ; 0, 1, 1 ]][[ 1, 0 ; 1, 1 ; 2, 1 ]]\vec{x} &= [[ 1, 1, 2 ; 0, 1, 1 ]][[ 2 ; 1 ; 0 ]]

		[[ 6, 3 ; 3, 2 ]] \vec{x} &= [[ 3 ; 1 ]]

		[[ 6, 3 | 3 ; 3, 2 | 1 ]] &

		[[ 2, 1 | 1 ; 3, 2 | 1 ]] & \quad \vec{r_1} \te \frac{1}{3}

		[[ 2, 1 | 1 ; 1, 1 | 0 ]] & \quad \vec{r_2} \me \vec{r_1}

		[[ 1, 0 | 1 ; 1, 1 | 0 ]] & \quad \vec{r_1} \me \vec{r_2}

		[[ 1, 0 | 1 ; 0, 1 | -1 ]] & \quad \vec{r_2} \me \vec{r_1}.
	$$

	The least-squares solution is therefore

	$$
		\vec{x}' = [[ 1 ; -1 ]].
	$$

	Since the columns of $A$ are linearly independent, this solution is unique, and to see how far off we are from the actual value of $\vec{b}$, we can compute

	$$
		\left| \left| [[ 1, 0 ; 1, 1 ; 2, 1 ]][[ 1 ; -1 ]] - [[ 2 ; 1 ; 0 ]] \right| \right| &= \left| \left| [[ 1 ; 0 ; 1 ]] - [[ 2 ; 1 ; 0 ]] \right| \right|

		&= \left| \left| [[ -1 ; -1 ; 1 ]] \right| \right|

		&= \sqrt{3}.
	$$

###

### exc "least-squares approximations"

	Find the least-squares solution to

	$$
		[[ 1, 2, -1 ; 2, 1, 1 ; -1, 0, -1 ]]\vec{x} = [[ 7 ; 7 ; 7 ]].
	$$

	How far away is the closest output?

###

### nav-buttons