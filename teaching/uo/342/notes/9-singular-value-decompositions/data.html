<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 9: Singular Value Decompositions</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">For most of the course up to this point, we&#x2019;ve focused on square matrices. There&#x2019;s good reason for that &mdash; square matrices are the ones that have eigenvectors and eigenvalues, and the only ones that are possibly diagonalizable. But the framing of diagonalization as a convenient consequence of eigenvectors does a disservice to how useful a concept it is in isolation. In the last section, we saw one way to produce a diagonal-like decomposition that&#x2019;s still extremely useful in its own right, even when the matrix itself wasn&#x2019;t diagonalizable. That required it to be square, though, since we still found ourselves falling back to notions of eigenvectors and eigenvalues eventually. Many &#x201C;naturally occurring&#x201D; matrices &mdash; those that come up in real-world applications &mdash; are very much not square, and though they might not have eigenvectors to speak of, they&#x2019;re still worth our time and attention. Let&#x2019;s consider an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and see what we can do.</p><p class="body-text">While <span class="tex-holder inline-math" data-source-tex="A">$A$</span> might not be square, we&#x2019;ve already seen a matrix in section 7 that&#x2019;s guaranteed to be: <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$.</span> It&#x2019;s also diagonalizable with an orthonormal basis of eigenvectors, and in an extremely loose sense, if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> had eigenvalues, then the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T">$A^T$</span> would be the same, and so the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$</span> are the squares of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p><p class="body-text">Let&#x2019;s see if we can turn that intuitive but technically-bankrupt notion into something actually meaningful. Since <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$</span> is an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> real symmetric matrix, it has eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> and an orthonormal basis of eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$.</span> If we place these <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> as columns in a matrix <span class="tex-holder inline-math" data-source-tex="V">$V$</span> (that&#x2019;s an unusual letter to use for a matrix, but it&#x2019;s standard notation), then <span class="tex-holder inline-math" data-source-tex="V">$V$</span> is unitary, so <span class="tex-holder inline-math" data-source-tex="V^{-1} = V^T">$V^{-1} = V^T$.</span> Now for each <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left| \left| A\vec{v} \right| \right|^2 &= A\vec{v_i} \bullet A\vec{v_i}\\[NEWLINE][TAB]&= \vec{v_i}^T A^T A \vec{v_i}\\[NEWLINE][TAB]&= \vec{v_i}^T \lambda_i \vec{v_i}\\[NEWLINE][TAB]&= \lambda_i.[NEWLINE]\end{align*}">$$\begin{align*}\left| \left| A\vec{v} \right| \right|^2 &= A\vec{v_i} \bullet A\vec{v_i}\\[4px]&= \vec{v_i}^T A^T A \vec{v_i}\\[4px]&= \vec{v_i}^T \lambda_i \vec{v_i}\\[4px]&= \lambda_i.\end{align*}$$</span></p><p class="body-text">That tells us quite a bit: first, that all the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$</span> are positive, and second, that those quantities that were analogous to the square roots of the eigenvalues are given by the values <span class="tex-holder inline-math" data-source-tex="\left| \left| A\vec{v_i} \right| \right|">$\left| \left| A\vec{v_i} \right| \right|$</span> for each <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$.</span> Let&#x2019;s give those values a name, and then we&#x2019;ll finish the decomposition.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: singular value</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix. The <strong>singular values</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are the values <span class="tex-holder inline-math" data-source-tex="\sigma_1, ..., \sigma_n">$\sigma_1, ..., \sigma_n$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\sigma_i = \sqrt{\lambda_i},[NEWLINE]$$">$$\begin{align*}\sigma_i = \sqrt{\lambda_i},\end{align*}$$</span></p><p class="body-text">and <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> are the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$.</span> Importantly, we always write the singular values in decreasing order.</p></div><p class="body-text">Let&#x2019;s return to decomposing our matrix <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> The outputs <span class="tex-holder inline-math" data-source-tex="A\vec{v_i}">$A\vec{v_i}$</span> and <span class="tex-holder inline-math" data-source-tex="A\vec{v_j}">$A\vec{v_j}$</span> are orthogonal, since</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left( A\vec{v_i} \right) \bullet \left( A\vec{v_j} \right) &= \vec{v_i}^T A^T A \vec{v_j}\\[NEWLINE][TAB]&= \vec{v_i}^T \lambda_j \vec{v_j}\\[NEWLINE][TAB]&= 0,[NEWLINE]\end{align*}">$$\begin{align*}\left( A\vec{v_i} \right) \bullet \left( A\vec{v_j} \right) &= \vec{v_i}^T A^T A \vec{v_j}\\[4px]&= \vec{v_i}^T \lambda_j \vec{v_j}\\[4px]&= 0,\end{align*}$$</span></p><p class="body-text">and so the vectors <span class="tex-holder inline-math" data-source-tex="A\vec{v_1}, ..., A\vec{v_k}">$A\vec{v_1}, ..., A\vec{v_k}$</span> form an orthogonal basis for <span class="tex-holder inline-math" data-source-tex="\operatorname{image} A">$\operatorname{image} A$,</span> where <span class="tex-holder inline-math" data-source-tex="\sigma_1 \geq \cdots \geq \sigma_k > 0">$\sigma_1 \geq \cdots \geq \sigma_k > 0$</span> are the nonzero singular values of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> (since the remaining vectors are mapped to zero). By extending these to an orthogonal basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$</span> (if they don&#x2019;t already form one) and normalizing them, we can produce an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{u_1}, ..., \vec{u_m} \right\}">$\left\{ \vec{u_1}, ..., \vec{u_m} \right\}$</span> for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$,</span> and in total we&#x2019;ve decomposed <span class="tex-holder inline-math" data-source-tex="A">$A$</span> into a product very reminiscent of a diagonalization:</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: singular value decomposition</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix, and let <span class="tex-holder inline-math" data-source-tex="\sigma_1 \geq \cdots \geq \sigma_k > 0">$\sigma_1 \geq \cdots \geq \sigma_k > 0$</span> be its nonzero singular values. Then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]A = U\Sigma V^T,[NEWLINE]$$">$$\begin{align*}A = U\Sigma V^T,\end{align*}$$</span></p><p class="body-text">where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]V = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{v_1}& \cdots& \vec{v_n} \\ \mid& & \mid \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}V = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{v_1}& \cdots& \vec{v_n} \\ \mid& & \mid \end{array}\right]\end{align*}$$</span></p><p class="body-text">is a unitary <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix whose columns are the eigenvectors of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\Sigma = \left[\begin{array}{c|c} D & 0 \\ \hline 0 & 0 \end{array}\right] = \left[\begin{array}{ccc|ccc} \sigma_1& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& \sigma_k & 0& \cdots& 0 \\ \hline 0& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& 0 & 0& \cdots& 0 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\Sigma = \left[\begin{array}{c|c} D & 0 \\ \hline 0 & 0 \end{array}\right] = \left[\begin{array}{ccc|ccc} \sigma_1& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& \sigma_k & 0& \cdots& 0 \\ \hline 0& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& 0 & 0& \cdots& 0 \end{array}\right]\end{align*}$$</span></p><p class="body-text">is an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix containing the nonzero singular values on its diagonal and zeros elsewhere, and</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]U = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{u_1}& \cdots& \vec{u_m} \\ \mid& & \mid \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}U = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{u_1}& \cdots& \vec{u_m} \\ \mid& & \mid \end{array}\right]\end{align*}$$</span></p><p class="body-text">is an <span class="tex-holder inline-math" data-source-tex="m \times m">$m \times m$</span> unitary matrix whose columns form an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$,</span> where <span class="tex-holder inline-math" data-source-tex="\vec{u_i}">$\vec{u_i}$</span> is <span class="tex-holder inline-math" data-source-tex="A\vec{v_i}">$A\vec{v_i}$</span> (after normalizing).</p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: singular value decomposition</div><p class="body-text">Find a singular value decomposition for <span class="tex-holder inline-math" data-source-tex="A = \left[\begin{array}{ccc} 1& 1& 2 \\ 1& -1& 1 \end{array}\right]">$A = \left[\begin{array}{ccc} 1& 1& 2 \\ 1& -1& 1 \end{array}\right]$.</span></p><p class="body-text">We&#x2019;ll start by finding the eigenvectors and eigenvalues for <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$.</span> We have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^T A = \left[\begin{array}{ccc} 2& 0& 3 \\ 0& 2& 1 \\ 3& 1& 5 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}A^T A = \left[\begin{array}{ccc} 2& 0& 3 \\ 0& 2& 1 \\ 3& 1& 5 \end{array}\right],\end{align*}$$</span></p><p class="body-text">which has eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 7">$\lambda_1 = 7$,</span> <span class="tex-holder inline-math" data-source-tex="\lambda_2 = 2">$\lambda_2 = 2$,</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_3 = 0">$\lambda_3 = 0$</span> (arranged in decreasing order), with corresponding eigenvectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 3 \\ 1 \\ 5 \end{array}\right] \qquad \vec{v_2} = \left[\begin{array}{c} -1 \\ 3 \\ 0 \end{array}\right] \qquad \vec{v_3} = \left[\begin{array}{c} -3 \\ 1 \\ 2 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 3 \\ 1 \\ 5 \end{array}\right] \qquad \vec{v_2} = \left[\begin{array}{c} -1 \\ 3 \\ 0 \end{array}\right] \qquad \vec{v_3} = \left[\begin{array}{c} -3 \\ 1 \\ 2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">The singular values of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are the square roots of the eigenvalues, so we have <span class="tex-holder inline-math" data-source-tex="\sigma_1 = \sqrt{7}">$\sigma_1 = \sqrt{7}$</span> and <span class="tex-holder inline-math" data-source-tex="\sigma_2 = \sqrt{2}">$\sigma_2 = \sqrt{2}$.</span> We won&#x2019;t need <span class="tex-holder inline-math" data-source-tex="\sigma_3 = 0">$\sigma_3 = 0$,</span> since only the nonzero singular values appear in the decomposition. Our final step is to evaluate <span class="tex-holder inline-math" data-source-tex="A">$A$</span> on the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> corresponding to nonzero singular values and use them to construct an orthonormal basis if they don&#x2019;t already form one. We have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{u_1} &= A\vec{v_1} = \left[\begin{array}{c} 14 \\ 7 \end{array}\right]\\[NEWLINE][TAB]\vec{u_2} &= A\vec{v_2} = \left[\begin{array}{c} 2 \\ -4 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\vec{u_1} &= A\vec{v_1} = \left[\begin{array}{c} 14 \\ 7 \end{array}\right]\\[4px]\vec{u_2} &= A\vec{v_2} = \left[\begin{array}{c} 2 \\ -4 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Normalizing our vectors and compiling them into three matrices, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]U &= \left[\begin{array}{cc} 2 / \sqrt{5}& 1 / \sqrt{5} \\ 1 / \sqrt{5}& -2 / \sqrt{5}  \end{array}\right]\\[NEWLINE][TAB]\Sigma &= \left[\begin{array}{ccc} \sqrt{7}& 0& 0 \\ 0& \sqrt{2}& 0 \end{array}\right]\\[NEWLINE][TAB]V &= \left[\begin{array}{ccc} 3 / \sqrt{35}& -1 / \sqrt{10}& -3 / \sqrt{14} \\ 1 / \sqrt{35}& 3 / \sqrt{10}& 1 / \sqrt{14} \\ 5 / \sqrt{35}& 0& 2 / \sqrt{14} \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}U &= \left[\begin{array}{cc} 2 / \sqrt{5}& 1 / \sqrt{5} \\ 1 / \sqrt{5}& -2 / \sqrt{5}  \end{array}\right]\\[4px]\Sigma &= \left[\begin{array}{ccc} \sqrt{7}& 0& 0 \\ 0& \sqrt{2}& 0 \end{array}\right]\\[4px]V &= \left[\begin{array}{ccc} 3 / \sqrt{35}& -1 / \sqrt{10}& -3 / \sqrt{14} \\ 1 / \sqrt{35}& 3 / \sqrt{10}& 1 / \sqrt{14} \\ 5 / \sqrt{35}& 0& 2 / \sqrt{14} \end{array}\right].\end{align*}$$</span></p><p class="body-text">Geometrically, the expression <span class="tex-holder inline-math" data-source-tex="A = U \Sigma V^T">$A = U \Sigma V^T$</span> expresses the action of <span class="tex-holder inline-math" data-source-tex="A : \mathbb{R}^3 \to \mathbb{R}^2">$A : \mathbb{R}^3 \to \mathbb{R}^2$</span> in four steps: a rotation / reflection in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$,</span> a projection to <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> a scaling of the axes in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> and finally a rotation / reflection in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$.</span> Since every matrix has an SVD, that means every linear map is expressible as these four steps, possibly swapping the projection for an inclusion to a higher-dimensional space, or just an identity matrix if the domain and codomain are equal.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: singular value decomposition</div><p class="body-text">Find a singular value decomposition for <span class="tex-holder inline-math" data-source-tex="A = \left[\begin{array}{cc} 1& -2 \\ 2& 3 \\ -1& 1 \end{array}\right]">$A = \left[\begin{array}{cc} 1& -2 \\ 2& 3 \\ -1& 1 \end{array}\right]$.</span></p></div></section><h2 class="section-text"> An Application to Image Compression</h2><section><p class="body-text">Most of the value in linear algebra comes from treating matrices as linear maps, but we can also use them to simply represent rectangular data. Let&#x2019;s begin with a 400 pixel wide, 300 pixel tall image <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and see what we can learn from its SVD.</p><div class="notes-images"><img class="notes-image" src="/teaching/uo/342/notes/9-singular-value-decompositions/graphics/original.webp"></img></div><p class="body-text">First of all, this doesn&#x2019;t directly translate to a matrix, since each pixel has red, green, and blue components, rather than a single number. Let&#x2019;s account for that by splitting it up into three images &mdash; rather than red, green, and blue components, though, let&#x2019;s use hue, saturation, and value, which are much more perceptually distinct.</p><div class="notes-images"><img class="notes-image" src="/teaching/uo/342/notes/9-singular-value-decompositions/graphics/h.webp"></img><img class="notes-image" src="/teaching/uo/342/notes/9-singular-value-decompositions/graphics/s.webp"></img><img class="notes-image" src="/teaching/uo/342/notes/9-singular-value-decompositions/graphics/v.webp"></img></div><p class="body-text">Each of these is an <span class="tex-holder inline-math" data-source-tex="300 \times 400">$300 \times 400$</span> matrix with values in <span class="tex-holder inline-math" data-source-tex="[0, 1]">$[0, 1]$,</span> but I&#x2019;ve rendered their meanings here as opposed to their values &mdash; the first image uses its data to show hue while leaving saturation and value constant, and so on.</p><p class="body-text">Let&#x2019;s call these matrices <span class="tex-holder inline-math" data-source-tex="H">$H$,</span> <span class="tex-holder inline-math" data-source-tex="S">$S$,</span> and <span class="tex-holder inline-math" data-source-tex="V">$V$,</span> and let&#x2019;s focus on just <span class="tex-holder inline-math" data-source-tex="V">$V$</span> for now. Like all matrices, it has an SVD <span class="tex-holder inline-math" data-source-tex="V = U \Sigma W^T">$V = U \Sigma W^T$</span> (we&#x2019;ll use <span class="tex-holder inline-math" data-source-tex="W">$W$</span> since the name <span class="tex-holder inline-math" data-source-tex="V">$V$</span> is already taken), where <span class="tex-holder inline-math" data-source-tex="U">$U$</span> is a <span class="tex-holder inline-math" data-source-tex="300 \times 300">$300 \times 300$</span> unitary matrix, <span class="tex-holder inline-math" data-source-tex="W">$W$</span> is an <span class="tex-holder inline-math" data-source-tex="400 \times 400">$400 \times 400$</span> unitary one, and <span class="tex-holder inline-math" data-source-tex="\Sigma">$\Sigma$</span> is a <span class="tex-holder inline-math" data-source-tex="300 \times 400">$300 \times 400$</span> matrix whose only nonzero values lie along its diagonal.</p><p class="body-text">We need to take a brief tangent to think about another perspective on matrix multiplication. Given an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and an <span class="tex-holder inline-math" data-source-tex="n \times k">$n \times k$</span> matrix <span class="tex-holder inline-math" data-source-tex="B">$B$,</span> the entry in row <span class="tex-holder inline-math" data-source-tex="i">$i$</span> and column <span class="tex-holder inline-math" data-source-tex="j">$j$</span> of <span class="tex-holder inline-math" data-source-tex="AB">$AB$</span> is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\sum_{l = 1}^n a_{il}b_{lj} = a_{i1}b_{1j} + \cdots + a_{in}b_{nj},[NEWLINE]$$">$$\begin{align*}\sum_{l = 1}^n a_{il}b_{lj} = a_{i1}b_{1j} + \cdots + a_{in}b_{nj},\end{align*}$$</span></p><p class="body-text">as we&#x2019;re very well used to by now. Each of these sums contains <span class="tex-holder inline-math" data-source-tex="n">$n$</span> terms, so the entire matrix is a sum of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of them:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]AB = \left[\begin{array}{cccc} a_{11}b_{11}& a_{11}b_{12}& \cdots& a_{11}b_{1k} \\ a_{21}b_{11}& a_{21}b_{12}& \cdots& a_{21}b_{1k} \\ \vdots& \vdots& \ddots& \vdots \\ a_{m1}b_{11}& a_{m1}b_{12}& \cdots& a_{m1}b_{1k} \end{array}\right] + \cdots + \left[\begin{array}{cccc} a_{1n}b_{n1}& a_{1n}b_{n2}& \cdots& a_{1n}b_{nk} \\ a_{2n}b_{n1}& a_{2n}b_{n2}& \cdots& a_{2n}b_{nk} \\ \vdots& \vdots& \ddots& \vdots \\ a_{mn}b_{n1}& a_{mn}b_{n2}& \cdots& a_{mn}b_{nk} \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}AB = \left[\begin{array}{cccc} a_{11}b_{11}& a_{11}b_{12}& \cdots& a_{11}b_{1k} \\ a_{21}b_{11}& a_{21}b_{12}& \cdots& a_{21}b_{1k} \\ \vdots& \vdots& \ddots& \vdots \\ a_{m1}b_{11}& a_{m1}b_{12}& \cdots& a_{m1}b_{1k} \end{array}\right] + \cdots + \left[\begin{array}{cccc} a_{1n}b_{n1}& a_{1n}b_{n2}& \cdots& a_{1n}b_{nk} \\ a_{2n}b_{n1}& a_{2n}b_{n2}& \cdots& a_{2n}b_{nk} \\ \vdots& \vdots& \ddots& \vdots \\ a_{mn}b_{n1}& a_{mn}b_{n2}& \cdots& a_{mn}b_{nk} \end{array}\right].\end{align*}$$</span></p><p class="body-text">This looks like a truly horrible way to compute a matrix product at first, but notice that the <span class="tex-holder inline-math" data-source-tex="l">$l$th</span> matrix in the series involves only the entries in the column <span class="tex-holder inline-math" data-source-tex="l">$l$</span> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and the column <span class="tex-holder inline-math" data-source-tex="l">$l$</span> of <span class="tex-holder inline-math" data-source-tex="B">$B$.</span> In fact, each term is a product of those two, just in the opposite order from what we&#x2019;re used to:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]AB = \left[\begin{array}{c} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{array}\right] \left[\begin{array}{cccc} b_{11}& b_{12}& \cdots& b_{1k} \end{array}\right] + \cdots + \left[\begin{array}{c} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} \end{array}\right] \left[\begin{array}{cccc} b_{n1}& b_{n2}& \cdots& b_{nk} \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}AB = \left[\begin{array}{c} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{array}\right] \left[\begin{array}{cccc} b_{11}& b_{12}& \cdots& b_{1k} \end{array}\right] + \cdots + \left[\begin{array}{c} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} \end{array}\right] \left[\begin{array}{cccc} b_{n1}& b_{n2}& \cdots& b_{nk} \end{array}\right].\end{align*}$$</span></p><p class="body-text">In total, we&#x2019;ve shown the following (fairly strange) fact: the product of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is equal to the matrix formed by multiplying every column of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> by the corresponding row of <span class="tex-holder inline-math" data-source-tex="B">$B$</span> and adding up all the results.</p><p class="body-text">Now let&#x2019;s return to the SVD. In <span class="tex-holder inline-math" data-source-tex="V = U \Sigma W^T">$V = U \Sigma W^T$,</span> if we denote the columns of <span class="tex-holder inline-math" data-source-tex="U">$U$</span> by <span class="tex-holder inline-math" data-source-tex="\vec{u_i}">$\vec{u_i}$,</span> the columns of <span class="tex-holder inline-math" data-source-tex="W">$W$</span> by <span class="tex-holder inline-math" data-source-tex="\vec{w_i}">$\vec{w_i}$,</span> and the singular values by <span class="tex-holder inline-math" data-source-tex="\sigma_i">$\sigma_i$,</span> then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]V = \sigma_1 \vec{u_1}\vec{w_1}^T + \cdots + \sigma_{300} \vec{u_{300}}\vec{w_{300}}^T,[NEWLINE]$$">$$\begin{align*}V = \sigma_1 \vec{u_1}\vec{w_1}^T + \cdots + \sigma_{300} \vec{u_{300}}\vec{w_{300}}^T,\end{align*}$$</span></p><p class="body-text">and since all of the <span class="tex-holder inline-math" data-source-tex="\vec{u_i}">$\vec{u_i}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w_i}">$\vec{w_i}$</span> have length <span class="tex-holder inline-math" data-source-tex="1">$1$,</span> the singular values (which we&#x2019;ve arranged in descending order) are the only part of the data that can effectively change the magnitude. Among all matrices of the form <span class="tex-holder inline-math" data-source-tex="c\vec{u}\vec{w}^T">$c\vec{u}\vec{w}^T$,</span> the one closest to <span class="tex-holder inline-math" data-source-tex="V">$V$</span> is then <span class="tex-holder inline-math" data-source-tex="\sigma_1 \vec{u_1}\vec{w_1}^T">$\sigma_1 \vec{u_1}\vec{w_1}^T$.</span> Among all matrices that are the sum of two of these terms, the closest is <span class="tex-holder inline-math" data-source-tex="\sigma_1 \vec{u_1}\vec{w_1}^T + \sigma_2 \vec{u_2}\vec{w_2}^T">$\sigma_1 \vec{u_1}\vec{w_1}^T + \sigma_2 \vec{u_2}\vec{w_2}^T$,</span> and so on. This points to a type of image compression we can easily extract: keep a few of the most important terms and throw out lots of the lower ones. That corresponds to deleting some of the right-most columns of <span class="tex-holder inline-math" data-source-tex="U">$U$</span> and <span class="tex-holder inline-math" data-source-tex="W">$W$</span> &mdash; then we can store something near the data of all of <span class="tex-holder inline-math" data-source-tex="V">$V$</span> by storing only a few columns of <span class="tex-holder inline-math" data-source-tex="U">$U$</span> and <span class="tex-holder inline-math" data-source-tex="W">$W$</span> and a few elements of <span class="tex-holder inline-math" data-source-tex="\Sigma">$\Sigma$.</span> The example below shows an approximation of <span class="tex-holder inline-math" data-source-tex="V">$V$</span> with between 1 and 200 terms.</p><div class='sliders'><div class="slider-container"><input id="depth-slider" type="range" step="0.000001" tabindex="1"><label for="depth-slider"><p class="body-text slider-subtext"></p></label></div></div><div class="desmos-border canvas-container"><canvas id="v-canvas" class="output-canvas"></canvas></div><p class="body-text">That&#x2019;s not bad! It&#x2019;s worth noting that unless we use fewer than half of the number of columns in the original image, we&#x2019;re not actually saving space, since using all the columns means all of <span class="tex-holder inline-math" data-source-tex="U">$U$</span> and <span class="tex-holder inline-math" data-source-tex="V">$V$,</span> which is roughly twice the space of the original image.</p><p class="body-text">It might seem like high levels of compression are functionally useless since the artifacts are so strong, but that&#x2019;s largely just for the value component <span class="tex-holder inline-math" data-source-tex="V">$V$</span> &mdash; in fact, that&#x2019;s largely why we did this example with it. By using different levels of compression with all three data channels, we can achieve substantial compression at minimum visual noise. Looking at the original channels, it&#x2019;s no coincidence that the hue and saturation channels had less detail in the first place &mdash; the image processor in the camera I took the photo with already removed detail from them to save space.</p><p class="body-text">To my knowledge, SVD compression isn&#x2019;t actually used by any mainstream formats, but the ideas behind it are similar to how those work. The ability to send more information over time to progressively load an image without having to produce multiple resolutions is particularly interesting.</p></section><h2 class="section-text"> Eigenfaces</h2><section><p class="body-text">For one final demo, here&#x2019;s another SVD application. I&#x2019;ve taken every graduate student with a directory photo at UO, run a small script to autocrop them all to the face and center the eyes, thrown out a few outliers that weren&#x2019;t taken head-on or were so close to the camera that they couldn&#x2019;t be lined up with the others, and finally scaled them all to <span class="tex-holder inline-math" data-source-tex="100 \times 100">$100 \times 100$.</span> Then I placed them as columns in a <span class="tex-holder inline-math" data-source-tex="30000 \times 44">$30000 \times 44$</span> matrix and ran an SVD on it, and the result is the faces below. By changing the face and depth, we can see how well the first <span class="tex-holder inline-math" data-source-tex="n">$n$</span> eigenvectors approximate any given student, and the first one (called the <strong>principal eigenvector</strong>, or in this case the principal eigenface) is the closest face to every one in the list.</p><div class='sliders'><div class="slider-container"><input id="index-slider" type="range" step="0.000001" tabindex="1"><label for="index-slider"><p class="body-text slider-subtext"></p></label></div><div class="slider-container"><input id="depth-2-slider" type="range" step="0.000001" tabindex="1"><label for="depth-2-slider"><p class="body-text slider-subtext"></p></label></div></div><div class="desmos-border canvas-container"><canvas id="eigenface-canvas" class="output-canvas"></canvas></div><p class="body-text">That&#x2019;s it for this course! We&#x2019;ve had the chance to cover many of linear algebra&#x2019;s most useful and used topics, and it&#x2019;s likely to form the mathematical underpinning of wherever you head next, whether that&#x2019;s more math or another STEM field. Thanks for being a part of the course &mdash; I hope to see you in future ones!</p><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>