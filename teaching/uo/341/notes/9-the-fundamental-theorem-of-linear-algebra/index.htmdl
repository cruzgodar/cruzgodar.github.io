### nav-buttons



We'll begin our final section of core material by tackling a very important question: how do we efficiently translate from one basis to another? While the answer will be simple, it will unfortunately be a little counterintuitive.

Let's take a finite-dimensional vector space $V$ and bases $\mathcal{B} = \{\vec{v_1}, ..., \vec{v_n}\}$ and $\mathcal{C} = \{\vec{w_1}, ..., \vec{w_n}\}$ (both bases have to have the same number of elements, so using $n$ as the final subscript for both is justified). If $\vec{v} \in V$ has a coordinate vector

$$
	[\vec{v}]_\mathcal{B} = [[ c_1 ; \vdots ; c_n ]],
$$

then $\vec{v} = c_1\vec{v_1} + \cdots + c_n\vec{v_n}$ by definition. But we can look at it a slightly different way: the coordinate vector of $\vec{v_i}$ in the basis $\mathcal{B}$ is just $\vec{e_i}$, since the expression of a basis vector in the basis that it's a part of is just one of itself and no other vectors. In other words,

$$
	[\vec{v}]_{\mathcal{B}} &= [[ c_1 ; \vdots ; c_n ]]
	
	&= c_1\vec{e_1} + \cdots + c_n\vec{e_n}
	
	&= c_1 [\vec{v_1}]_\mathcal{B} + \cdots + c_n [\vec{v_n}]_\mathcal{B}.
$$

That kind of statement should look familiar --- it's saying that the function $B : V \to \mathbb{R}^n$ given by $B(\vec{v}) = [\vec{v}]_\mathcal{B}$ is actually a linear transformation! Since linear transformations are defined by their actions on just a basis, we can find $[\vec{v}]_\mathcal{C}$ by finding $[\vec{v_i}]_\mathcal{C}$ for each basis vector $\vec{v_i}$, and in fact, the conversion from $\mathcal{B}$ to $\mathcal{C}$ will be a linear transformation, so we'll be able to express it as a matrix.

So --- how do we convert the basis vectors? By far the simplest way is to only handle converting to and from a standard basis whenever possible, since that's almost always very easy to express vectors in. To convert the **coordinate vector** $\vec{e_1}$ in a basis $\mathcal{B}$ to a coordinate vector in the standard basis $\mathcal{E}$, we send $\vec{e_1}$ to $[\vec{v_1}]_\mathcal{E}$, since the first describes a vector comprised of one copy of $\vec{v_1}$ and no others, and the second describes $\vec{v_1}$ as its entries. Applying this logic to the others and putting them into a matrix, we have

$$
	B = [[ \mid, , \mid ; [\vec{v_1}]_\mathcal{E}, \cdots, [\vec{v_n}]_\mathcal{E} ; \mid, , \mid ]]
$$

is a matrix that sends coordinate vectors in the basis $\mathcal{B} = \{\vec{v_1}, ..., \vec{v_n}\}$ to coordinate vectors in the standard basis. And now we need to stop and say that again, because this is unfortunately *very* unintuitive. Since $B\vec{e_i} = [\vec{v_i}]_\mathcal{E}$, it looks for all the world like $B$ translates *from* the standard basis *to* $\mathcal{B}$, but we need to remember what we're dealing with. In that equation, $\vec{e_i}$ is not an *element* of the standard basis (e.g. a polynomial, matrix, or linear transformation), but a *coordinate vector* in the basis $\mathcal{B}$, representing a single copy of $\vec{v_i}$. On the other side of the equation, $[\vec{v_i}]_\mathcal{E}$ is the expression of $\vec{v_i}$ in the standard basis, written as a coordinate vector. We're in desperate need of examples, so let's get right to them.

### ex a change-of-basis matrix

	Let $V$ be the subspace of $#R#[x]$ consisting of polynomials with degree less than $4$ and let $\mathcal{B} = \{ 1 + x, x + x^2, x - 2x^3, x^3 \}$ be a basis for it. Express $1 + x + x^2 + x^3$ in $\mathcal{B}$.

	To get started, let $\mathcal{E} = \{1, x, x^2, x^3\}$ be the standard basis for $V$. We can assemble the change-of-basis matrix $B$ by taking each of the vectors in $\mathcal{B}$ and expressing them as a coordinate vector in $\mathcal{E}$. Specifically, we have

	$$
		[1 + x]_\mathcal{E} = [[ 1 ; 1 ; 0 ; 0 ]] \qquad [x + x^2]_\mathcal{E} = [[ 0 ; 1 ; 1 ; 0 ]] \qquad [x - 2x^3]_\mathcal{E} = [[ 0 ; 1 ; 0 ; -2 ]] \qquad [x^3]_\mathcal{E} = [[ 0 ; 0 ; 0 ; 1 ]].
	$$

	Assembling these as columns in a matrix results in

	$$
		B = [[ 1, 0, 0, 0 ; 1, 1, 1, 0 ; 0, 1, 0, 0 ; 0, 0, -2, 1 ]].
	$$

	As we've seen, this matrix converts coordinate vectors in $\mathcal{B}$ to coordinate vectors in $\mathcal{E}$ --- for example, if $\vec{v}$ is a vector with $[\vec{v}]_\mathcal{B} = [[ 1 ; 2 ; 3 ; 4 ]]$, then

	$$
		[\vec{v}]_\mathcal{E} &= B[\vec{v}]_\mathcal{B}

		&= [[ 1, 0, 0, 0 ; 1, 1, 1, 0 ; 0, 1, 0, 0 ; 0, 0, -2, 1 ]][[ 1 ; 2 ; 3 ; 4 ]]

		&= [[ 1 ; 6 ; 2 ; -2 ]],
	$$

	so $\vec{v} = 1 + 6x + 2x^2 - 2x^3$. But this isn't really that helpful --- it's just another way of saying to take a linear combination of the basis vectors in $\mathcal{B}$. What we really want, and what we're being asked for, is to go the other way: to take the polynomial $1 + x + x^2 + x^3$, which is easily expressible in the standard basis, and express it in $\mathcal{B}$. Since

	$$
		[\vec{v}]_\mathcal{E} &= B[\vec{v}]_\mathcal{B}
	$$

	for any vector $\vec{v}$, we can multiply both sides by $B^{-1}$ on the left to find the result we want:

	$$
		B^{-1}[\vec{v}]_\mathcal{E} &= [\vec{v}]_\mathcal{B}.
	$$

	The matrix $B$ has to be invertible, since it's square and consists of linearly independent columns (since they come from vectors comprising a basis). So let's go ahead and invert the matrix $B$:

	$$
		[[ 1, 0, 0, 0 | 1, 0, 0, 0 ; 1, 1, 1, 0 | 0, 1, 0, 0 ; 0, 1, 0, 0 | 0, 0, 1, 0 ; 0, 0, -2, 1 | 0, 0, 0, 1 ]] &

		[[ 1, 0, 0, 0 | 1, 0, 0, 0 ; 0, 1, 1, 0 | -1, 1, 0, 0 ; 0, 1, 0, 0 | 0, 0, 1, 0 ; 0, 0, -2, 1 | 0, 0, 0, 1 ]] & \qquad \vec{r_2} \me \vec{r_1}

		[[ 1, 0, 0, 0 | 1, 0, 0, 0 ; 0, 1, 1, 0 | -1, 1, 0, 0 ; 0, 0, -1, 0 | 1, -1, 1, 0 ; 0, 0, -2, 1 | 0, 0, 0, 1 ]] & \qquad \vec{r_3} \me \vec{r_2}

		[[ 1, 0, 0, 0 | 1, 0, 0, 0 ; 0, 1, 1, 0 | -1, 1, 0, 0 ; 0, 0, -1, 0 | 1, -1, 1, 0 ; 0, 0, 0, 1 | -2, 2, -2, 1 ]] & \qquad \vec{r_4} \me 2\vec{r_3}

		[[ 1, 0, 0, 0 | 1, 0, 0, 0 ; 0, 1, 0, 0 | 0, 0, 1, 0 ; 0, 0, -1, 0 | 1, -1, 1, 0 ; 0, 0, 0, 1 | -2, 2, -2, 1 ]] & \qquad \vec{r_2} \pe \vec{r_3}

		[[ 1, 0, 0, 0 | 1, 0, 0, 0 ; 0, 1, 0, 0 | 0, 0, 1, 0 ; 0, 0, 1, 0 | -1, 1, -1, 0 ; 0, 0, 0, 1 | -2, 2, -2, 1 ]] & \qquad \vec{r_3} \te -1
	$$

	With $B^{-1}$ in hand, we can express our vector in the basis $\mathcal{B}$: we just multiply $B^{-1}$ with the coordinate vector for $1 + x + x^2 + x^3$ in the standard basis $\mathcal{E}$:

	$$
		B^{-1} [1 + x + x^2 + x^3]_\mathcal{E} &= [[ 1, 0, 0, 0 ; 0, 0, 1, 0 ; -1, 1, -1, 0 ; -2, 2, -2, 1 ]] [[ 1 ; 1 ; 1 ; 1 ]]

		&= [[ 1 ; 1 ; -1 ; -1 ]].
	$$

	In total, this expression tells us that

	$$
		1 + x + x^2 + x^3 = 1(1 + x) + 1(x + x^2) - 1(x - 2x^3) - 1(x^3),
	$$

	and so we've successfully expressed the polynomial in $\mathcal{B}$.

###

### exc a change-of-basis matrix

	Let $\mathcal{B}$ be the basis

	$$
		[[ 1, 2 ; 0, 2 ]], [[ 1, 1 ; 1, 0 ]], [[ 0, 1 ; 0, 1 ]], [[ 1, 0 ; 0, -1 ]].
	$$

	Express the matrix $[[ 1, 2 ; -1, 3 ]]$ in $\mathcal{B}$.

###



## The Fundamental Theorem

This last theorem earns its name: it provides a fundamental relation between linear maps and their domain, kernel, and image. Let's get right to the statement and proof.

### thm The Fundamental Theorem of Linear Algebra
	
	Let $V$ and $W$ be vector spaces, where $V$ is finite-dimensional, and let $T : V \to W$ be a linear transformation. Then
	
	$$
		\dim V = \dim \ker T + \dim \image T.
	$$
	
###

### pf
	
	Although we won't pin down every detail, this proof will be appropriately rigorous for the level of our class. To begin, $\ker T$ is a subspace of $V$ as we've previously mentioned, since it's closed under addition and scalar multiplication and contains the zero vector. It therefore has a basis $\{\vec{v_1}, ..., \vec{v_k}\}$ (the basis must have only finitely many elements since we required $V$ to be finite-dimensional). If we let $n = \dim V$, then $n \geq k$, and in fact we can extend our basis for $\ker T$ to a basis for $V$: just choose any basis for $V$ and select just the vectors linearly independent from $\{\vec{v_1}, ..., \vec{v_k}\}$. If we label those linearly independent basis vectors $\vec{v_{k+1}}, ..., \vec{v_n}$, then
	
	$$
		\{\vec{v_1}, ..., \vec{v_k}, \vec{v_{k+1}}, ..., \vec{v_n}\} = \{\vec{v_1}, ..., \vec{v_n}\}
	$$
	
	is a basis for $V$. Since we've denoted $n = \dim V$ and $k = \dim \ker T$, the result we're trying to show is that $\dim \image T = n - k$. The only time $n - k$ has appeared so far in our proof is as the number of vectors we added to our basis for $\ker T$, so let's work with them.
	
	The image of $T$ is the set of its outputs, and it's a subspace of $W$ in its own right. So if we're trying to find a basis of $\image T$ with $n - k$ elements, the set $\{\vec{v_{k+1}}, ..., \vec{v_n}\}$ definitely won't work, since all of those vectors are contained in $V$. In order to translate them into vectors in $W$, the only tool we have is $T$ itself. So let's try to show that
	
	$$
		\{T(\vec{v_{k+1}}), ..., T(\vec{v_n})\}
	$$
	
	is a basis for $\image T$. Let's start with the fact that it spans $\image T$, since that will be easier. Since $\{\vec{v_1}, ..., \vec{v_n}\}$ is a basis for $V$, we can definitely express the image of $T$ as
	
	$$
		\image T = \span\{T(\vec{v_1}), ..., T(\vec{v_n})\},
	$$
	
	since any evaluation $T(\vec{v})$ can write $\vec{v}$ as a linear combination of the $\vec{v_i}$ and then split $T$ across them. But since $\vec{v_1}, ..., \vec{v_k} \in \ker T$, the first $k$ entries in the span are zero and so don't contribute to the span at all. We can therefore rewrite the span equation as
	
	$$
		\image T = \span\{T(\vec{v_{k + 1}}), ..., T(\vec{v_n})\},
	$$
	
	which is halfway to proving that the elements on the right form a basis for $\image T$ --- now we just need to show they're linearly independent. So let's suppose some linear combination of them equals zero:
	
	$$
		c_{k + 1}T(\vec{v_{k+1}}) + \cdots + c_nT(\vec{v_n}) = \vec{0}.
	$$
	
	We can combine all of these into one vector, since $T$ is a linear transformation:
	
	$$
		T(c_{k + 1}\vec{v_{k + 1}} + \cdots + c_n\vec{v_n}) = \vec{0}.
	$$
	
	It would be great if we could conclude that the linear combination inside of $T$ equaled zero, but sadly, that's not the case --- $T$ sends other vectors than just the zero vector to zero. But there's good news: we know exactly what those other vectors are! What we can conclude here is that
	
	$$
		c_{k + 1}\vec{v_{k + 1}} + \cdots + c_n\vec{v_n} \in \ker T,
	$$
	
	which means we can express it as a linear combination of the vectors in our basis for $\ker T$:
	
	$$
		c_{k + 1}\vec{v_{k + 1}} + \cdots + c_n\vec{v_n} = c_1\vec{v_1} + \cdots + c_k\vec{v_k}
	$$
	
	for some $c_1, ..., c_k$. But moving everything to one side, we have
	
	$$
		-c_1\vec{v_1} - \cdots - c_k\vec{v_k} + c_{k + 1}\vec{v_{k + 1}} + \cdots + c_n\vec{v_n} = \vec{0},
	$$
	
	and since the $\vec{v_i}$ form a basis for $V$, they must be linearly independent. All of the $c_i$ therefore have to be zero, and so in particular, $c_{k + 1} = \cdots = c_n = 0$. That shows that $T(\vec{v_{k + 1}}), ..., T(\vec{v_n})$ are linearly independent, and so they form a basis for $\image T$. Its dimension is therefore $n - k$, and so we've shown that
	
	$$
		\dim V = \dim \ker T + \dim \image T,
	$$
	
	as required.
	
###

This result is also called the **Rank-Nullity Theorem** --- when we associate $T$ with a matrix $A$, $\dim \image T$ is called the **rank** of $A$, and $\dim \ker T$ is called the **nullity**. The theorem therefore says that the rank plus the nullity is equal to the number of columns of $A$.

Although we won't have substantial use for the fundamental theorem, it's worth stating and proving for the sake of completeness. More importantly, it gives a sense of what proof-based math classes are like: theorems like this one are proven in class, not just stated like most of ours have been, and proving (usually simpler) results is most of what the homework and exams consist of. It's a very different way of thinking about math, and the subject can eventually begin to resemble art or philosophy more than science. If any of that sounds interesting, it's definitely worth pursuing at least a minor in math.

For us, though, this is as far as we'll go with core linear algebra. The next course in the sequence focuses on eigenvectors and eigenvalues, topics we previewed briefly in the homework, but we'll use the remaining course time to explore some different directions, starting with two of the most widely-used applications of linear algebra.



### nav-buttons