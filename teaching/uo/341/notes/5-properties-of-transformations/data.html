<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 5: Properties of Transformations</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">With the foundation of matrices, inverses, linear transformations, and linear independence, we&#x2019;re ready to tie everything we&#x2019;ve learned together. We&#x2019;ll begin by defining two properties that a linear transformation can have, and we&#x2019;ll soon see how they relate to the ones we&#x2019;ve already seen.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: one-to-one and onto</div><p class="body-text">A function <span class="tex-holder inline-math" data-source-tex="f : \mathbb{R}^n \to \mathbb{R}^m">$f : \mathbb{R}^n \to \mathbb{R}^m$</span> is <strong>one-to-one</strong> (or <strong>injective</strong>) if <span class="tex-holder inline-math" data-source-tex="f(\vec{v}) \neq f(\vec{w})">$f(\vec{v}) \neq f(\vec{w})$</span> whenever <span class="tex-holder inline-math" data-source-tex="\vec{v} \neq \vec{w}">$\vec{v} \neq \vec{w}$.</span> It&#x2019;s <strong>onto</strong> (or <strong>surjective</strong>) if every vector <span class="tex-holder inline-math" data-source-tex="\vec{u} \in \mathbb{R}^m">$\vec{u} \in \mathbb{R}^m$</span> satisfies <span class="tex-holder inline-math" data-source-tex="\vec{u} = f(\vec{v})">$\vec{u} = f(\vec{v})$</span> for some vector <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$.</span></p></div><p class="body-text">In simple terms, one-to-one functions are those that pass the horizontal line test, and onto ones are those whose image (i.e. range) is equal to their codomain.</p><p class="body-text">This first definition of one-to-one is unfortunately a little too clunky to work with directly most of the time. Instead, we&#x2019;ll use an equivalent one that&#x2019;s quite a bit more streamlined.</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition: an equivalent one-to-one definition</div><p class="body-text">A linear transformation <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$</span> is one-to-one exactly when the only vector <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$</span> for which <span class="tex-holder inline-math" data-source-tex="T(\vec{v}) = \vec{0}">$T(\vec{v}) = \vec{0}$</span> is <span class="tex-holder inline-math" data-source-tex="\vec{v} = \vec{0}">$\vec{v} = \vec{0}$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">Let&#x2019;s very briefly sketch the reasoning behind this proposition. First of all, if <span class="tex-holder inline-math" data-source-tex="T">$T$</span> sends a vector <span class="tex-holder inline-math" data-source-tex="\vec{v} \neq \vec{0}">$\vec{v} \neq \vec{0}$</span> to <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$,</span> then <span class="tex-holder inline-math" data-source-tex="T(\vec{v}) = \vec{0} = T(\vec{0})">$T(\vec{v}) = \vec{0} = T(\vec{0})$,</span> so <span class="tex-holder inline-math" data-source-tex="T">$T$</span> isn&#x2019;t one-to-one. On the other hand, if the only vector <span class="tex-holder inline-math" data-source-tex="T">$T$</span> sends to <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$</span> is <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$,</span> then whenever we have <span class="tex-holder inline-math" data-source-tex="T(\vec{v}) = T(\vec{w})">$T(\vec{v}) = T(\vec{w})$,</span> we can rearrange it to get</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]T(\vec{w}) - T(\vec{w}) = T(\vec{v} - \vec{w}) = \vec{0},[NEWLINE]$$">$$\begin{align*}T(\vec{w}) - T(\vec{w}) = T(\vec{v} - \vec{w}) = \vec{0},\end{align*}$$</span></p><p class="body-text">And under the assumption, that means <span class="tex-holder inline-math" data-source-tex="\vec{v} - \vec{w} = \vec{0}">$\vec{v} - \vec{w} = \vec{0}$,</span> so <span class="tex-holder inline-math" data-source-tex="\vec{v} = \vec{w}">$\vec{v} = \vec{w}$,</span> meaning <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is one-to-one.</p></div><p class="body-text">It&#x2019;s important to note that this alternate definition works <em>only</em> with linear transformations, not functions in general. For example, <span class="tex-holder inline-math" data-source-tex="f(x) = x^2">$f(x) = x^2$</span> isn&#x2019;t one-to-one, since <span class="tex-holder inline-math" data-source-tex="f(1) = f(-1)">$f(1) = f(-1)$,</span> but the only <span class="tex-holder inline-math" data-source-tex="x">$x$</span> with <span class="tex-holder inline-math" data-source-tex="f(x) = 0">$f(x) = 0$</span> is <span class="tex-holder inline-math" data-source-tex="x = 0">$x = 0$.</span></p><p class="body-text">Similarly, we&#x2019;ll want to use a different definition of onto when we&#x2019;re actually working with transformations. Remember from the last section that applying a linear transformation to a vector results in taking a linear combination of the columns of the linear transformation&#x2019;s matrix, where the coefficients are given by the vector (this is just restating matrix-vector multiplication). For example, the transformation <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^2 \to \mathbb{R}^3">$T : \mathbb{R}^2 \to \mathbb{R}^3$</span> defined by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]T\left( \left[\begin{array}{c} x \\ y \end{array}\right] \right) = \left[\begin{array}{c} 2x + y \\ -y \\ 3x + 4y \end{array}\right][NEWLINE]$$">$$\begin{align*}T\left( \left[\begin{array}{c} x \\ y \end{array}\right] \right) = \left[\begin{array}{c} 2x + y \\ -y \\ 3x + 4y \end{array}\right]\end{align*}$$</span></p><p class="body-text">corresponds to the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} 2& 1 \\ 0& -1 \\ 3& 4\end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} 2& 1 \\ 0& -1 \\ 3& 4\end{array}\right]\end{align*}$$</span></p><p class="body-text">and applying it to a vector results in</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} 2& 1 \\ 0& -1 \\ 3& 4\end{array}\right] \left[\begin{array}{c} x \\ y \end{array}\right] = \left[\begin{array}{c} 2 \\ 0 \\ 3 \end{array}\right]x + \left[\begin{array}{c} 1 \\ -1 \\ 4 \end{array}\right]y.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} 2& 1 \\ 0& -1 \\ 3& 4\end{array}\right] \left[\begin{array}{c} x \\ y \end{array}\right] = \left[\begin{array}{c} 2 \\ 0 \\ 3 \end{array}\right]x + \left[\begin{array}{c} 1 \\ -1 \\ 4 \end{array}\right]y.\end{align*}$$</span></p><p class="body-text">Since we can plug in whatever we like for <span class="tex-holder inline-math" data-source-tex="x">$x$</span> and <span class="tex-holder inline-math" data-source-tex="y">$y$,</span> the image of <span class="tex-holder inline-math" data-source-tex="T">$T$</span> &mdash; the set of outputs that actually occur &mdash; is the <em>span of the columns of the matrix corresponding to <span class="tex-holder inline-math" data-source-tex="T">$T$</em>.</span> That&#x2019;s absolutely a mouthful, so let&#x2019;s give it a more concise name.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: column space</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="T">$T$</span> be a linear transformation. The <strong>column space</strong> of <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is the span of the columns of the matrix corresponding to <span class="tex-holder inline-math" data-source-tex="T">$T$.</span> The column space of <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is equal to <span class="tex-holder inline-math" data-source-tex="\operatorname{image} T">$\operatorname{image} T$,</span> so we don&#x2019;t need extra notation to distinguish it.</p></div><p class="body-text">Let&#x2019;s finally bring it all together. For a linear transformation <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$</span> to be onto &mdash; that is, for its column space to be equal to <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$</span> &mdash; we need the columns of the matrix for <span class="tex-holder inline-math" data-source-tex="T">$T$</span> to have at least <span class="tex-holder inline-math" data-source-tex="m">$m$</span> linearly independent vectors among them. On the other hand, for <span class="tex-holder inline-math" data-source-tex="T">$T$</span> to be one-to-one, we need <em>all</em> the columns to be linearly independent &mdash; if there&#x2019;s any linear dependence, then two different inputs will get sent to the same output. It&#x2019;s tempting to say that being one-to-one is a stronger condition than being onto, but this actually isn&#x2019;t always the case, as we&#x2019;ll shortly see. For now, though, we can state the result that will let us properly evaluate individual transformations.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: criteria for one-to-one and onto</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$</span> be a linear transformation and let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be the <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix corresponding to it. Then <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is one-to-one exactly when all the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are linearly independent, and <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is onto exactly when there are at least <span class="tex-holder inline-math" data-source-tex="m">$m$</span> linearly independent columns of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p></div><p class="body-text">To check how many vectors in a set (e.g. the columns of a matrix) are linearly independent, the simplest method is to place them all as <em>rows</em> in a matrix and reduce it. That&#x2019;s because row operations replace rows with linear combinations of other rows, with the goal of making them zero if possible. When we&#x2019;re done reducing, the number of nonzero rows left is the number of linearly independent vectors in the set we started with.</p><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: one-to-one and onto</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^3 \to \mathbb{R}^3">$T : \mathbb{R}^3 \to \mathbb{R}^3$</span> be defined by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]T\left( \left[\begin{array}{c} x \\ y \\ z \end{array}\right] \right) = \left[\begin{array}{c} 2x + 3y + 5z \\ 3x + 4y + 7z \\ x - 2y - z \end{array}\right].[NEWLINE]$$">$$\begin{align*}T\left( \left[\begin{array}{c} x \\ y \\ z \end{array}\right] \right) = \left[\begin{array}{c} 2x + 3y + 5z \\ 3x + 4y + 7z \\ x - 2y - z \end{array}\right].\end{align*}$$</span></p><p class="body-text">Is <span class="tex-holder inline-math" data-source-tex="T">$T$</span> one-to-one? Is it onto?</p><p class="body-text">By the previous theorem, we first want to express <span class="tex-holder inline-math" data-source-tex="T">$T$</span> as a matrix: it corresponds to</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} 2& 3& 5 \\ 3& 4& 7 \\ 1& -2& -1 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} 2& 3& 5 \\ 3& 4& 7 \\ 1& -2& -1 \end{array}\right],\end{align*}$$</span></p><p class="body-text">so we want to row-reduce the matrix whose rows are the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc} 2& 3& 1 \\ 3& 4& -2 \\ 5& 7& -1 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{ccc} 2& 3& 1 \\ 6& 8& -4 \\ 10& 14& -2 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ \times\!\!= 2 \\ \vec{r_3} \ \times\!\!= 2 \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc} 2& 3& 1 \\ 0& -1& -7 \\ 0& -1& -7 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ -\!\!= 3\vec{r_1} \\ \vec{r_3} \ -\!\!= 5 \vec{r_1} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc} 2& 3& 1 \\ 0& -1& -7 \\ 0& 0& 0 \end{array}\right] & \qquad \vec{r_3} \ -\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc} 2& 0& 20 \\ 0& -1& -7 \\ 0& 0& 0 \end{array}\right] & \qquad \vec{r_1} \ +\!\!= 3\vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc} 1& 0& 10 \\ 0& 1& 7 \\ 0& 0& 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_1} \ \times\!\!= \frac{1}{2} \\ \vec{r_2} \ \times\!\!= -1 \end{array}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc} 2& 3& 1 \\ 3& 4& -2 \\ 5& 7& -1 \end{array}\right] &\\[4px]\left[\begin{array}{ccc} 2& 3& 1 \\ 6& 8& -4 \\ 10& 14& -2 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ \times\!\!= 2 \\ \vec{r_3} \ \times\!\!= 2 \end{array}\\[4px]\left[\begin{array}{ccc} 2& 3& 1 \\ 0& -1& -7 \\ 0& -1& -7 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ -\!\!= 3\vec{r_1} \\ \vec{r_3} \ -\!\!= 5 \vec{r_1} \end{array}\\[4px]\left[\begin{array}{ccc} 2& 3& 1 \\ 0& -1& -7 \\ 0& 0& 0 \end{array}\right] & \qquad \vec{r_3} \ -\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc} 2& 0& 20 \\ 0& -1& -7 \\ 0& 0& 0 \end{array}\right] & \qquad \vec{r_1} \ +\!\!= 3\vec{r_2}\\[4px]\left[\begin{array}{ccc} 1& 0& 10 \\ 0& 1& 7 \\ 0& 0& 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_1} \ \times\!\!= \frac{1}{2} \\ \vec{r_2} \ \times\!\!= -1 \end{array}\end{align*}$$</span></p><p class="body-text">In the end, we have two linearly independent vectors, so <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has only two independent columns. Since not all the columns were independent, <span class="tex-holder inline-math" data-source-tex="T">$T$</span> isn&#x2019;t one-to-one, and since there weren&#x2019;t at least three linearly independent columns, <span class="tex-holder inline-math" data-source-tex="T">$T$</span> isn&#x2019;t onto. That sentence certainly seems a bit redundant when we write it out, and we&#x2019;ll have more to say on the subject momentarily.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: one-to-one and onto</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="S : \mathbb{R}^3 \to \mathbb{R}^2">$S : \mathbb{R}^3 \to \mathbb{R}^2$</span> be defined by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]S\left( \left[\begin{array}{c} x \\ y \\ z \end{array}\right] \right) = \left[\begin{array}{c} x + y - z \\ 2x + y - 3z \end{array}\right].[NEWLINE]$$">$$\begin{align*}S\left( \left[\begin{array}{c} x \\ y \\ z \end{array}\right] \right) = \left[\begin{array}{c} x + y - z \\ 2x + y - 3z \end{array}\right].\end{align*}$$</span></p><p class="body-text">Is <span class="tex-holder inline-math" data-source-tex="S">$S$</span> one-to-one? Is it onto?</p></div></section><h2 class="section-text"> Invertibility</h2><section><p class="body-text">Let&#x2019;s consider, for yet another time, a linear transformation <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$.</span> If <span class="tex-holder inline-math" data-source-tex="n > m">$n > m$,</span> then there are more columns than rows in the matrix corresponding to <span class="tex-holder inline-math" data-source-tex="T">$T$,</span> and so the columns can&#x2019;t all possibly be linearly independent. That&#x2019;s because when we place them as rows in an <span class="tex-holder inline-math" data-source-tex="n \times m">$n \times m$</span> matrix and row-reduce it, the best that the process could go is if none of the first <span class="tex-holder inline-math" data-source-tex="m">$m$</span> reduce to rows of all zeros &mdash; but then we could use those reduced rows to clear all of the others. For example, consider the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} 1& 3& 5 \\ -1& 4& 2 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} 1& 3& 5 \\ -1& 4& 2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Placing the rows of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> as columns in a new matrix and partially row-reducing, we get</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} 1& 1 \\ 3& 4 \\ 5& 2 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc} 1& 1 \\ 0& 1 \\ 5& 2 \end{array}\right] & \qquad \vec{r_2} \ -\!\!= 3\vec{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} 1& 1 \\ 3& 4 \\ 5& 2 \end{array}\right] & \\[4px]\left[\begin{array}{cc} 1& 1 \\ 0& 1 \\ 5& 2 \end{array}\right] & \qquad \vec{r_2} \ -\!\!= 3\vec{r_1}.\end{align*}$$</span></p><p class="body-text">Since neither of the first two rows is all zero, we&#x2019;re going to be able to reduce the third row to zero no matter what its entries are. Geometrically, if we already have two linearly independent vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> there&#x2019;s no third vector in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> that&#x2019;s linearly independent with them both.</p><p class="body-text">On the other hand, if <span class="tex-holder inline-math" data-source-tex="n < m">$n < m$,</span> then there are more rows than columns. This case is a lot simpler: for <span class="tex-holder inline-math" data-source-tex="T">$T$</span> to be onto, there need to be at least <span class="tex-holder inline-math" data-source-tex="m">$m$</span> linearly independent columns, but there are only <span class="tex-holder inline-math" data-source-tex="n">$n$</span> columns at all, linearly independent or not. We can sum up our findings in far fewer words:</p><div class="notes-prop notes-environment"><div class="notes-prop-title notes-title">Proposition</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$</span> be a linear transformation. If <span class="tex-holder inline-math" data-source-tex="n > m">$n > m$,</span> then <span class="tex-holder inline-math" data-source-tex="T">$T$</span> can&#x2019;t be one-to-one. If <span class="tex-holder inline-math" data-source-tex="n < m">$n < m$,</span> then <span class="tex-holder inline-math" data-source-tex="T">$T$</span> can&#x2019;t be onto.</p></div><p class="body-text">Geometrically, it&#x2019;s helpful to think of two cases here: first, when <span class="tex-holder inline-math" data-source-tex="n = 2">$n = 2$</span> and <span class="tex-holder inline-math" data-source-tex="m = 1">$m = 1$,</span> we&#x2019;re squishing all of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> down onto <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^1">$\mathbb{R}^1$,</span> which is effectively mapping the <span class="tex-holder inline-math" data-source-tex="xy">$xy$-plane</span> onto the <span class="tex-holder inline-math" data-source-tex="x">$x$-axis.</span> There&#x2019;s just no way to do that in a nice (i.e. linear) fashion without sending many inputs to one output. On the other hand, when <span class="tex-holder inline-math" data-source-tex="n = 1">$n = 1$</span> and <span class="tex-holder inline-math" data-source-tex="m = 2">$m = 2$,</span> we&#x2019;re doing the reverse: mapping the <span class="tex-holder inline-math" data-source-tex="x">$x$-axis</span> into the <span class="tex-holder inline-math" data-source-tex="xy">$xy$-plane.</span> There&#x2019;s no nice way for that map to fill up all of the plane, which is exactly what would need to happen for it to be onto.</p><p class="body-text">When we have a one-to-one and onto linear transformation <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$,</span> then we now know that <span class="tex-holder inline-math" data-source-tex="m = n">$m = n$</span> &mdash; otherwise, one of the two would be impossible. That means the matrix for <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is square, and the two conditions now say exactly the same thing: <span class="tex-holder inline-math" data-source-tex="T">$T$</span> being one-to-one means it has all <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of its columns linearly independent, and onto means it has at least <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of its columns linearly independent. And in fact, both being one-to-one and being onto are equivalent to a property we&#x2019;re already familiar with:</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: invertibility</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^n">$T : \mathbb{R}^n \to \mathbb{R}^n$</span> be a linear transformation whose domain and codomain are the same. Then all of the following statements are equivalent:</p><p class="body-text numbered-list-item">1. <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is one-to-one.</p><p class="body-text numbered-list-item">2. <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is onto.</p><p class="body-text numbered-list-item">3. <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is invertible.</p></div><p class="body-text">Our transformation in the previous example isn&#x2019;t invertible by this theorem, which means both that there is no inverse linear transformation and that the corresponding matrix isn&#x2019;t invertible. In fact, we didn&#x2019;t even need to ask whether it was onto after concluding it was one-to-one: since the matrix was square, the two properties are one and the same.</p><p class="body-text">At this point, we&#x2019;re finished with the material in the section, but I think it&#x2019;s worth taking a minute to summarize where we are in regard to linear transformations, matrices, and the interplay between them. There are a lot of terms and ideas to digest, and a quick list organizing many of them should help. For all the items in the list, we let <span class="tex-holder inline-math" data-source-tex="T : \mathbb{R}^n \to \mathbb{R}^m">$T : \mathbb{R}^n \to \mathbb{R}^m$</span> be a linear transformation and <span class="tex-holder inline-math" data-source-tex="A">$A$</span> its corresponding matrix.</p><p class="body-text"><span style="width: 32px"></span><strong>&#8226;</strong> Evaluation: we evaluate <span class="tex-holder inline-math" data-source-tex="T(\vec{v})">$T(\vec{v})$</span> by plugging the entries of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> into a formula for <span class="tex-holder inline-math" data-source-tex="T">$T$,</span> just like a one-variable function <span class="tex-holder inline-math" data-source-tex="f(x)">$f(x)$.</span> We evaluate a matrix on a vector by the matrix-vector multiplication <span class="tex-holder inline-math" data-source-tex="A\vec{v}">$A\vec{v}$.</span></p><p class="body-text"><span style="width: 32px"></span><strong>&#8226;</strong> Composition: we compose <span class="tex-holder inline-math" data-source-tex="T">$T$</span> with another linear map <span class="tex-holder inline-math" data-source-tex="S: \mathbb{R}^m \to \mathbb{R}^k">$S: \mathbb{R}^m \to \mathbb{R}^k$</span> to form the map <span class="tex-holder inline-math" data-source-tex="T \circ S : \mathbb{R}^n \to \mathbb{R}^k">$T \circ S : \mathbb{R}^n \to \mathbb{R}^k$</span> by plugging the formula for the output of <span class="tex-holder inline-math" data-source-tex="S">$S$</span> as the input to the formula for <span class="tex-holder inline-math" data-source-tex="T">$T$.</span> We compose <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with an <span class="tex-holder inline-math" data-source-tex="m \times k">$m \times k$</span> matrix <span class="tex-holder inline-math" data-source-tex="B">$B$</span> to form <span class="tex-holder inline-math" data-source-tex="AB">$AB$</span> by matrix multiplication.</p><p class="body-text"><span style="width: 32px"></span><strong>&#8226;</strong> One-to-one: <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is one-to-one if <span class="tex-holder inline-math" data-source-tex="T(\vec{v}) \neq T(\vec{w})">$T(\vec{v}) \neq T(\vec{w})$</span> for <span class="tex-holder inline-math" data-source-tex="\vec{v} \neq \vec{w}">$\vec{v} \neq \vec{w}$,</span> if <span class="tex-holder inline-math" data-source-tex="T(\vec{v}) \neq T(\vec{0})">$T(\vec{v}) \neq T(\vec{0})$</span> for <span class="tex-holder inline-math" data-source-tex="\vec{v} \neq \vec{0}">$\vec{v} \neq \vec{0}$,</span> or if all of the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are linearly independent.</p><p class="body-text"><span style="width: 32px"></span><strong>&#8226;</strong> Onto: <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is onto if for any vector <span class="tex-holder inline-math" data-source-tex="\vec{w} \in \mathbb{R}^m">$\vec{w} \in \mathbb{R}^m$,</span> there is a vector <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$</span> with <span class="tex-holder inline-math" data-source-tex="T(\vec{v} = \vec{w})">$T(\vec{v} = \vec{w})$,</span> or if there are at least <span class="tex-holder inline-math" data-source-tex="m">$m$</span> linearly independent columns of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p><p class="body-text"><span style="width: 32px"></span><strong>&#8226;</strong> Invertibility: <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is invertible if its domain and codomain are equal (so <span class="tex-holder inline-math" data-source-tex="m = n">$m = n$),</span> and it is one-to-one and onto (checking either one of those suffices to determine both). Alternatively, <span class="tex-holder inline-math" data-source-tex="T">$T$</span> is invertible if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is invertible, meaning it row-reduces to the identity matrix.</p><p class="body-text"><span style="width: 32px"></span><strong>&#8226;</strong> Inversion: we don&#x2019;t invert transformations directly with their formulas, only with their matrices. With a matrix <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> we invert it by augmenting with <span class="tex-holder inline-math" data-source-tex="I">$I$</span> and row-reducing.</p><p class="body-text">We&#x2019;re getting some hints that the bulk of the subject lies with square matrices: they&#x2019;re the only ones that can be invertible, and being one-to-one and onto are identical conditions. And as we&#x2019;ll see in the next section, the next big object we&#x2019;ll want to develop depends on the matrices it takes in being square.</p><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>