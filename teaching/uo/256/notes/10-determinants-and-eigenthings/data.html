<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 10: Determinants and Eigenthings</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">When a matrix fails to be invertible, it&#x2019;s because at some point in the row reduction process, a row became completely zero. The only thing row reduction can do is turn rows into sums of multiples of all the rows, so if some row becomes all zero, then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\mathbf{r_1} + c_2\mathbf{r_2} + \cdots + c_n\mathbf{r_n} = \mathbf{0}[NEWLINE]$$">$$\begin{align*}c_1\mathbf{r_1} + c_2\mathbf{r_2} + \cdots + c_n\mathbf{r_n} = \mathbf{0}\end{align*}$$</span></p><p class="body-text">for some constants <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span>, where <span class="tex-holder inline-math" data-source-tex="\mathbf{0}">$\mathbf{0}$</span> is the row vector of all zeros. Some of the <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span> may be zero and some negative, but the point is that this is a much more concise description of when a matrix is invertible. In fact, this is such an important concept in linear algebra that it deserves its own name.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: linear independence</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v_k}">$\mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v_k}$</span> be <span class="tex-holder inline-math" data-source-tex="n">$n$</span>-dimensional vectors. The <span class="tex-holder inline-math" data-source-tex="\mathbf{v_i}">$\mathbf{v_i}$</span> are <strong>linearly dependent</strong> if</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\mathbf{v_1} + c_2\mathbf{v_2} + \cdots + c_k\mathbf{v_k} = \mathbf{0}[NEWLINE]$$">$$\begin{align*}c_1\mathbf{v_1} + c_2\mathbf{v_2} + \cdots + c_k\mathbf{v_k} = \mathbf{0}\end{align*}$$</span></p><p class="body-text">for some choice of <span class="tex-holder inline-math" data-source-tex="c_1, c_2, ..., c_k">$c_1, c_2, ..., c_k$</span>, and <strong>linearly independent</strong> if no such <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span> exist. We say that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\mathbf{v_1} + c_2\mathbf{v_2} + \cdots + c_k\mathbf{v_k}[NEWLINE]$$">$$\begin{align*}c_1\mathbf{v_1} + c_2\mathbf{v_2} + \cdots + c_k\mathbf{v_k}\end{align*}$$</span></p><p class="body-text">is a <strong>linear combination</strong> of the <span class="tex-holder inline-math" data-source-tex="\mathbf{v_i}">$\mathbf{v_i}$</span>.</p></div><p class="body-text">For example, <span class="tex-holder inline-math" data-source-tex="\mathbf{v_1} = \left[\begin{array}{ccc}2& -5& 1\end{array}\right]">$\mathbf{v_1} = \left[\begin{array}{ccc}2& -5& 1\end{array}\right]$</span> and <span class="tex-holder inline-math" data-source-tex="\mathbf{v_2} = \left[\begin{array}{ccc}-4& 10& -2\end{array}\right]">$\mathbf{v_2} = \left[\begin{array}{ccc}-4& 10& -2\end{array}\right]$</span> are linearly dependent, since <span class="tex-holder inline-math" data-source-tex="-2\mathbf{v_1} + \mathbf{v_2} = \mathbf{0}">$-2\mathbf{v_1} + \mathbf{v_2} = \mathbf{0}$</span>. To determine if a more complicated set of vectors is linearly dependent, just throw them into a matrix as row vectors and try to row reduce the matrix. If a row becomes the zero vector, they&#x2019;re linearly dependent, and if not, they&#x2019;re linearly independent.</p><p class="body-text">If we&#x2019;re looking at <span class="tex-holder inline-math" data-source-tex="n">$n$</span>-dimensional vectors, any collection of more than <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of them is <em>guaranteed</em> to be linearly dependent. That&#x2019;s because putting the into a matrix results in one that is taller than it is wide, and so even if the first <span class="tex-holder inline-math" data-source-tex="n">$n$</span> rows are linearly independent, meaning they reduce to the identity matrix, the rows after it will reduce to zero: once we have the identity matrix, we can destroy any other row, meaning the matrix has the form <span class="tex-holder inline-math" data-source-tex="\left[\begin{array}{c} \mathbf{I} \\ \hline \mathbf{0}\end{array}\right]">$\left[\begin{array}{c} \mathbf{I} \\ \hline \mathbf{0}\end{array}\right]$</span>. Since linear dependence of the rows is the only problem that can arise when row reducing, we have a clean result relating it to matrix inversion.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: linear independence and matrix inversion</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. Then <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> is invertible if and only if the rows of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> are linearly independent.</p></div><p class="body-text">This definition lets us go back and clarify something from earlier sections: fundamental solutions to linear homogeneous DEs must be linearly independent, and the general solution is an arbitrary linear combination of the <span class="tex-holder inline-math" data-source-tex="y_i">$y_i$</span>. In fact, we can even express an <span class="tex-holder inline-math" data-source-tex="n">$n$</span>th order linear homogeneous DE as a matrix product:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]p_n(t)y^{(n)} + p_{n-1}(t)y^{(n - 1)} + \cdots + p_1(t)y' + p_0(t)y &= 0\\[NEWLINE][TAB]\left[\begin{array}{ccccc}p_n(t)& p_{n-1}(t)& \cdots& p_1(t)& p_0(t)\end{array}\right] \left[\begin{array}{c}y^{(n)} \\ y^{(n - 1)} \\ \vdots \\ y' \\ y\end{array}\right] &= 0.[NEWLINE]\end{align*}">$$\begin{align*}p_n(t)y^{(n)} + p_{n-1}(t)y^{(n - 1)} + \cdots + p_1(t)y' + p_0(t)y &= 0\\[4px]\left[\begin{array}{ccccc}p_n(t)& p_{n-1}(t)& \cdots& p_1(t)& p_0(t)\end{array}\right] \left[\begin{array}{c}y^{(n)} \\ y^{(n - 1)} \\ \vdots \\ y' \\ y\end{array}\right] &= 0.\end{align*}$$</span></p><p class="body-text">This gives us our first hint of how matrices will relate to systems of DEs, and we&#x2019;ll begin exploring that connection properly in the next section. For now, let&#x2019;s get back to matrices.</p></section><h2 class="section-text"> Determinants</h2><section><p class="body-text">It&#x2019;s time to properly talk about determinants. We&#x2019;ve seen plenty of them for <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> matrices with Wronskians and occasionally for <span class="tex-holder inline-math" data-source-tex="3 \times 3">$3 \times 3$</span> matrices. We&#x2019;ll soon need them for matrices of any size, so we&#x2019;ll need a general definition.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: determinant</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. The <strong>determinant</strong> of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span>, written <span class="tex-holder inline-math" data-source-tex="\det \mathbf{A}">$\det \mathbf{A}$</span>, is a single number (that measures the factor by which <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> scales volume). The determinant of a <span class="tex-holder inline-math" data-source-tex="1 \times 1">$1 \times 1$</span> matrix &mdash; i.e. a number &mdash; is its single entry, and the determinant of a larger matrix can be found expanding along any row or column: pick a row or column, and for every entry in it, cross off its row and column and find the determinant of the remaining <span class="tex-holder inline-math" data-source-tex="(n - 1) \times (n - 1)">$(n - 1) \times (n - 1)$</span> matrix. Multiply that by the entry in consideration and then add these up for all <span class="tex-holder inline-math" data-source-tex="n">$n$</span> entries, alternating sign according to the following checkerboard pattern:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccccc} +& -& +& -& \cdots \\ -& +& -& +& \cdots \\ +& -& +& -& \cdots \\ -& +& -& +& \cdots \\ \vdots& \vdots& \vdots& \vdots& \ddots \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccccc} +& -& +& -& \cdots \\ -& +& -& +& \cdots \\ +& -& +& -& \cdots \\ -& +& -& +& \cdots \\ \vdots& \vdots& \vdots& \vdots& \ddots \end{array}\right]\end{align*}$$</span></p></div><div class="notes-ex notes-environment"><p class="body-text"</p><span class="notes-ex-title">Example: determinant</span></p><p class="body-text">Evaluate <span class="tex-holder inline-math" data-source-tex="\det \left[\begin{array}{ccc} 1& 4& 7 \\ 3& 6& 9 \\ 2& 5& 8 \end{array}\right].">$\displaystyle \det \left[\begin{array}{ccc} 1& 4& 7 \\ 3& 6& 9 \\ 2& 5& 8 \end{array}\right].$</span></p><p class="body-text">Let&#x2019;s expand along row <span class="tex-holder inline-math" data-source-tex="2">$2$</span>. For the entry <span class="tex-holder inline-math" data-source-tex="3">$3$</span>, crossing off its row and column leaves the following matrix:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc} & 4& 7 \\ & & \\  & 5& 8 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc} & 4& 7 \\ & & \\  & 5& 8 \end{array}\right].\end{align*}$$</span></p><p class="body-text">The determinant of this is just <span class="tex-holder inline-math" data-source-tex="4(8) - 7(5) = -3">$4(8) - 7(5) = -3$</span>, and so our first term in the determinant is this times the <span class="tex-holder inline-math" data-source-tex="3">$3$</span>, making <span class="tex-holder inline-math" data-source-tex="-9">$-9$</span>. Repeating this for the other two terms in the row, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]6 \det \left[\begin{array}{ccc} 1& & 7 \\ & & \\ 2& & 8 \end{array}\right] &= 6(1(8) - 7(2))\\[NEWLINE][TAB]&= -36.\\[NEWLINE][TAB]9 \det \left[\begin{array}{ccc} 1& 4& \\ & & \\ 2& 5& \end{array}\right] &= 9(1(5) - 4(2))\\[NEWLINE][TAB]&= -27.[NEWLINE]\end{align*}">$$\begin{align*}6 \det \left[\begin{array}{ccc} 1& & 7 \\ & & \\ 2& & 8 \end{array}\right] &= 6(1(8) - 7(2))\\[4px]&= -36.\\[4px]9 \det \left[\begin{array}{ccc} 1& 4& \\ & & \\ 2& 5& \end{array}\right] &= 9(1(5) - 4(2))\\[4px]&= -27.\end{align*}$$</span></p><p class="body-text">Finally, we&#x2019;ll add these three up and modify their signs according to the checkerboard pattern. The second row begins with <span class="tex-holder inline-math" data-source-tex="-">$-$</span>, so in total, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\det \left[\begin{array}{ccc} 1& 4& 7 \\ 3& 6& 9 \\ 2& 5& 8 \end{array}\right] &= -(-9) + (-36) - (-27)\\[NEWLINE][TAB]&= 0.[NEWLINE]\end{align*}">$$\begin{align*}\det \left[\begin{array}{ccc} 1& 4& 7 \\ 3& 6& 9 \\ 2& 5& 8 \end{array}\right] &= -(-9) + (-36) - (-27)\\[4px]&= 0.\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: determinant</span></p><p class="body-text">Evaluate <span class="tex-holder inline-math" data-source-tex="\det \left[\begin{array}{ccc} 2& 1& 0 \\ 2& 7& 1 \\ -1& 2& -5 \end{array}\right].">$\displaystyle \det \left[\begin{array}{ccc} 2& 1& 0 \\ 2& 7& 1 \\ -1& 2& -5 \end{array}\right].$</span></p></div><p class="body-text">One of the most important results regarding determinants is their straightforward relationship to inverses:</p><div class="notes-thm notes-environment"><p class="body-text"</p><span class="notes-thm-title">Theorem: determinants and inverses</span></p><p class="body-text">A square matrix <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> is invertible if and only if <span class="tex-holder inline-math" data-source-tex="\det \mathbf{A} \neq 0">$\det \mathbf{A} \neq 0$</span>.</p></div><p class="body-text">The short explanation here is that writing down a general formula for the inverse of a matrix is possible, though extremely complicated, and it always requires dividing by the determinant.</p></section><h2 class="section-text"> Eigenvalues and Eigenvectors</h2><section><p class="body-text">When matrices and vectors are small, multiplying them together isn&#x2019;t a particularly long operation, but as the dimension increases, it gets progressively more intensive. For some very special vectors, though, multiplication isn&#x2019;t difficult at all. For example, let&#x2019;s look at the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\mathbf{A} = \left[\begin{array}{cc} 2& 2 \\ 3& 1 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\mathbf{A} = \left[\begin{array}{cc} 2& 2 \\ 3& 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">If we evaluate the following two products, they&#x2019;re equal to multiples of the vector we plug in:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\mathbf{A} \left[\begin{array}{c} 1 \\ 1 \end{array}\right] &= \left[\begin{array}{cc} 2& 2 \\ 3& 1 \end{array}\right] \left[\begin{array}{c} 1 \\ 1 \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{c}4 \\ 4\end{array}\right]\\[NEWLINE][TAB]&= 4\left[\begin{array}{c} 1 \\ 1 \end{array}\right]\\[NEWLINE][TAB]\mathbf{A} \left[\begin{array}{c} -2 \\ 3 \end{array}\right] &= [ 2, 2 ; 3, 1 ]] \left[\begin{array}{c} -2 \\ 3 \end{array}\right]\\[NEWLINE][TAB]&= \left[\begin{array}{c} 2 \\ -3 \end{array}\right]\\[NEWLINE][TAB]&= -\left[\begin{array}{c} -2 \\ 3 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\mathbf{A} \left[\begin{array}{c} 1 \\ 1 \end{array}\right] &= \left[\begin{array}{cc} 2& 2 \\ 3& 1 \end{array}\right] \left[\begin{array}{c} 1 \\ 1 \end{array}\right]\\[4px]&= \left[\begin{array}{c}4 \\ 4\end{array}\right]\\[4px]&= 4\left[\begin{array}{c} 1 \\ 1 \end{array}\right]\\[4px]\mathbf{A} \left[\begin{array}{c} -2 \\ 3 \end{array}\right] &= [ 2, 2 ; 3, 1 ]] \left[\begin{array}{c} -2 \\ 3 \end{array}\right]\\[4px]&= \left[\begin{array}{c} 2 \\ -3 \end{array}\right]\\[4px]&= -\left[\begin{array}{c} -2 \\ 3 \end{array}\right]\end{align*}$$</span></p><p class="body-text">Although it might not seem like a pivotal topic at first, vectors like these play an incredibly important role in the study of matrices, and it&#x2019;ll be helpful to give them a name.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: eigenvalues and eigenvectors</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> be a square matrix. An <strong>eigenvector of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span></strong> is a vector <span class="tex-holder inline-math" data-source-tex="\mathbf{v}">$\mathbf{v}$</span> with <span class="tex-holder inline-math" data-source-tex="\mathbf{Av} = \lambda\mathbf{v}">$\mathbf{Av} = \lambda\mathbf{v}$</span> for some nonzero value <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span>, called the <strong>eigenvalue</strong> corresponding to <span class="tex-holder inline-math" data-source-tex="\mathbf{v}">$\mathbf{v}$</span>.</p></div><p class="body-text">Any eigenvector <span class="tex-holder inline-math" data-source-tex="\mathbf{v}">$\mathbf{v}$</span> of a matrix <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> satisfies <span class="tex-holder inline-math" data-source-tex="\mathbf{Av} = \lambda\mathbf{Iv}">$\mathbf{Av} = \lambda\mathbf{Iv}$</span>, or in other words,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB](\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}.[NEWLINE]$$">$$\begin{align*}(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}.\end{align*}$$</span></p><p class="body-text">We&#x2019;re used to solving that for <span class="tex-holder inline-math" data-source-tex="\mathbf{v}">$\mathbf{v}$</span>, but since we don&#x2019;t know <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span>, that isn&#x2019;t possible directly. Instead, let&#x2019;s solve for <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> first. When we multiply by <span class="tex-holder inline-math" data-source-tex="\mathbf{v}">$\mathbf{v}$</span>, we&#x2019;re taking a linear combination of the columns of <span class="tex-holder inline-math" data-source-tex="\mathbf{A} - \lambda\mathbf{I}">$\mathbf{A} - \lambda\mathbf{I}$</span> and asking for it to be zero. That means the columns need to be linearly dependent, so the matrix <span class="tex-holder inline-math" data-source-tex="\mathbf{A} - \lambda\mathbf{I}">$\mathbf{A} - \lambda\mathbf{I}$</span> needs to be <em>not</em> invertible. That isn&#x2019;t particularly useful on its own, but now that we know the connection between invertibility and determinants, we can solve the equation</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\det (\mathbf{A} - \lambda\mathbf{I}) = 0.[NEWLINE]$$">$$\begin{align*}\det (\mathbf{A} - \lambda\mathbf{I}) = 0.\end{align*}$$</span></p><p class="body-text">In a deeply unfortunate naming collision, this equation is called the <strong>characteristic polynomial</strong> of the matrix <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span>. Once we&#x2019;ve solved it for the eigenvalues, we can then return to the setup of <span class="tex-holder inline-math" data-source-tex="\mathbf{Av} = \lambda \mathbf{v}">$\mathbf{Av} = \lambda \mathbf{v}$</span> to find the corresponding eigenvectors. In a surprising turn, though, many of the applications we&#x2019;ll use eigenvalues for don&#x2019;t require the eigenvectors at all, meaning we can stop early.</p><div class="notes-ex notes-environment"><p class="body-text"</p><span class="notes-ex-title">Example: finding eigenvectors and eigenvalues</span></p><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\mathbf{A} = \left[\begin{array}{cc} 5& -3 \\ 2& -2 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\mathbf{A} = \left[\begin{array}{cc} 5& -3 \\ 2& -2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">To get the characteristic polynomial, we subtract <span class="tex-holder inline-math" data-source-tex="\lambda \mathbf{I}">$\lambda \mathbf{I}$</span>, which just means subtracting <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> from every entry in the diagonal, and then take the determinant.</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\det (\mathbf{A} - \lambda \mathbf{I}) &= \det \left[\begin{array}{cc} 5 - \lambda& -3 \\ 2& -2 - \lambda \end{array}\right]\\[NEWLINE][TAB]&= (5 - \lambda)(-2 - \lambda) - (-3)(2)\\[NEWLINE][TAB]&= -10 - 3\lambda + \lambda^2 + 6\\[NEWLINE][TAB]&= \lambda^2 - 3\lambda - 4\\[NEWLINE][TAB]&= (\lambda - 4)(\lambda + 1).[NEWLINE]\end{align*}">$$\begin{align*}\det (\mathbf{A} - \lambda \mathbf{I}) &= \det \left[\begin{array}{cc} 5 - \lambda& -3 \\ 2& -2 - \lambda \end{array}\right]\\[4px]&= (5 - \lambda)(-2 - \lambda) - (-3)(2)\\[4px]&= -10 - 3\lambda + \lambda^2 + 6\\[4px]&= \lambda^2 - 3\lambda - 4\\[4px]&= (\lambda - 4)(\lambda + 1).\end{align*}$$</span></p><p class="body-text">Since the characteristic polynomial is supposed to equal zero, the eigenvalues are <span class="tex-holder inline-math" data-source-tex="4">$4$</span> and <span class="tex-holder inline-math" data-source-tex="-1">$-1$</span>. We&#x2019;ll handle them one at a time to find the eigenvectors.</p><p class="body-text">First, let&#x2019;s take <span class="tex-holder inline-math" data-source-tex="\lambda = 4">$\lambda = 4$</span>. That gives us the system <span class="tex-holder inline-math" data-source-tex="(\mathbf{A} - 4 \mathbf{I})\mathbf{v} = \mathbf{0}">$(\mathbf{A} - 4 \mathbf{I})\mathbf{v} = \mathbf{0}$</span>, which corresponds to the augmented matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 2& -6 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad \mathbf{r_2} \ +\!\!= -2\mathbf{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 1& -3 & 0 \\ 2& -6 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad \mathbf{r_2} \ +\!\!= -2\mathbf{r_1}.\end{align*}$$</span></p><p class="body-text">In the form of an equation, <span class="tex-holder inline-math" data-source-tex="v_1 - 3v_2 = 0">$v_1 - 3v_2 = 0$</span>, so <span class="tex-holder inline-math" data-source-tex="v_1 = 3v_2">$v_1 = 3v_2$</span>. We just need one eigenvector, so let&#x2019;s take <span class="tex-holder inline-math" data-source-tex="v_2 = 1">$v_2 = 1$</span> and <span class="tex-holder inline-math" data-source-tex="v_1 = 3">$v_1 = 3$</span> to get our first eigenvector of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\mathbf{v} = \left[\begin{array}{c} 3 \\ 1 \end{array}\right][NEWLINE]$$">$$\begin{align*}\mathbf{v} = \left[\begin{array}{c} 3 \\ 1 \end{array}\right]\end{align*}$$</span></p><p class="body-text">with <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 4">$\lambda_1 = 4$</span>. For the other eigenvalue, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 2& -1 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad \mathbf{r_2} \ +\!\!= -\frac{1}{3}\mathbf{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 6& -3 & 0 \\ 2& -1 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad \mathbf{r_2} \ +\!\!= -\frac{1}{3}\mathbf{r_1}.\end{align*}$$</span></p><p class="body-text">Now <span class="tex-holder inline-math" data-source-tex="6v_1 - 3v_2 = 0">$6v_1 - 3v_2 = 0$</span>, so <span class="tex-holder inline-math" data-source-tex="2v_1 = v_2">$2v_1 = v_2$</span>. We&#x2019;ll just take <span class="tex-holder inline-math" data-source-tex="v_1 = 1">$v_1 = 1$</span> and <span class="tex-holder inline-math" data-source-tex="v_2 = 2">$v_2 = 2$</span> to get our second eigenvector of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\mathbf{v} = \left[\begin{array}{c} 1 \\ 2 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\mathbf{v} = \left[\begin{array}{c} 1 \\ 2 \end{array}\right].\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: finding eigenvectors and eigenvalues</span></p><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\mathbf{A} = \left[\begin{array}{ccc} 1& 1& 0 \\ 0& 0& 0 \\ 0& 1& 1 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\mathbf{A} = \left[\begin{array}{ccc} 1& 1& 0 \\ 0& 0& 0 \\ 0& 1& 1 \end{array}\right].\end{align*}$$</span></p></div><p class="body-text">As one last result, though, let&#x2019;s properly connect the two titles of this section:</p><div class="notes-thm notes-environment"><p class="body-text"</p><span class="notes-thm-title">Theorem: determinants and eigenvalues</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> with eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span>. Then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\det \mathbf{A} = \lambda_1 \cdots \lambda_n.[NEWLINE]$$">$$\begin{align*}\det \mathbf{A} = \lambda_1 \cdots \lambda_n.\end{align*}$$</span></p><p class="body-text">In other words, the determinant of a matrix is the product of its eigenvalues.</p></div><p class="body-text">In higher-level math classes, this <em>is</em> the definition of the determinant, and we use it to show all of the other properties. With everything connected, we can now write down a list of properties about matrices that are all equivalent: if any one is true, they all are, and similarly if any one is false.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: properties of square matrices</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. The following are equivalent:</p><p class="body-text numbered-list-item">1. <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> is invertible.</p><p class="body-text numbered-list-item">2. <span class="tex-holder inline-math" data-source-tex="\det \mathbf{A} \neq 0">$\det \mathbf{A} \neq 0$</span>.</p><p class="body-text numbered-list-item">3. All the eigenvalues of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> are nonzero.</p><p class="body-text numbered-list-item">4. The rows of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> are linearly independent.</p><p class="body-text numbered-list-item">5. The columns of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> are linearly independent.</p></div><p class="body-text">In a field as mature as linear algebra, it&#x2019;s not too surprising that there are many, <em>many</em> more items we could list here, but these are by far the most important.</p><p class="body-text">Let&#x2019;s say a quick word about the meaning of eigenvalues. When the eigenvectors of a matrix <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> are linearly independent, we can think of them as the sides of a parallelogram (or an analogous shape in higher dimensions). Then each eigenvalue is the scale factor by which <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> scales the length of the corresponding eigenvector, so the product of them all is the factor by which <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> scales the total area &mdash; or volume, more generally.</p><div class="desmos-border"><div id="eigenvectors" class="desmos-container"></div></div><p class="body-text">That&#x2019;s the geometric interpretation of the determinant: it&#x2019;s a volume scaling factor. Now it&#x2019;s a little easier to see why it being zero is a problem for invertibility: if the volume goes to zero, <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> must have squished one or more of the eigenvectors to zero, which means a whole lot of inputs are being mapped to the same output. That&#x2019;s exactly what <em>can&#x2019;t</em> happen for the process to be invertible.</p><p class="body-text">We&#x2019;ll begin to reap the benefits of eigenvalues, eigenvectors, and determinants in the next section, when we return at long last to the world of differential equations.</p><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>