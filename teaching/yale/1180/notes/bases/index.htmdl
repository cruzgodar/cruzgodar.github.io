### nav-buttons



Although we've already talked quite a bit about vectors, we're only just getting started --- vectors have an incredible amount of algebraic and geometric properties that we'll be able to leverage before long. First, though, we're in desperate need of notation and terminology. At its worst, math can feel like pointless symbol-pushing, and so it's important to only add new notation when we truly need it to manage complexity. The best notation bottles up a moderately tricky idea and makes it usable without having to interface with it all the time.

Let's begin with something we see everywhere: the set of all real numbers. A **real** number is one on the number line, whether rational or irrational --- for example, $0$, $1$, $\pi$, $e$, and $\sqrt{2}$ are all real. We write the whole set of them as $#R#$, a rather fancy-looking R, and to actually use that symbol most of the time, we need one more: $\in$. This one means "element of", and it lets us write the rather verbose "let $x$ be a real number" as simply "let $x \in #R#$", read "let $x$ be in $#R#$".

A length-$2$ vector just consists of two independent real numbers --- it's effectively an ordered pair. If each element of an ordered pair has three possibilities (say $x$ and $y$ can each independently be $1$, $2$, or $3$ in the pair $(x, y)$), then there are $3^2 = 9$ possibilities for the whole pair. Inspired by the power of two, we write $#R#^2$ (read "R two") for the set of all ordered pairs --- or equivalently, the set of all length-2 vectors. Similarly, $#R#^n$ is the set of all length-$n$ vectors.

That's a fair amount of symbols, but it's thankfully all we'll have for a while. With the notation in place, we can start talking about the math.

First of all, vectors are just matrices with a single column, so all of the nice properties of matrix arithmetic carry over to them. Two vectors of the same length are added together by just adding each corresponding entry, and two vectors of different lengths cannot be added together at all. We can multiply a vector by a scalar $c$ (i.e. just a number) by multiplying each entry by $c$. None of this is different from matrices, but what's new to vectors is the ability to visualize them in $#R#^n$. When $n = 2$, we have $#R#^2$, which is just the $xy$-plane --- every length-$2$ vector corresponds to a point in the plane. We typically think of these as *directions* rather than locations, and we draw them as arrows from the origin.

### desmos planeVectors

When we add vectors, we add corresponding components together, so the result is a vector from the origin with the total amount of movement of both vectors in both the $x$ and $y$ directions. Geometrically, we can place the vectors one after the other so that the second begins where the first ends --- then the sum is the vector that travels from the beginning of the first to the end of the second, since it has the total $x$ and $y$ movement of both vectors.

### desmos vectorAddition

Scalar multiplication works similarly: multiplying a vector $\vec{v}$ by a positive constant $c$ results in a vector that points in the same direction as $\vec{v}$, but $c$ times as long.



## Linear Combinations and Linear Independence

As we've seen, our primary use of vectors is to represent systems of equations with many equations and unknowns, and a critical feature of those systems is that they don't always have just one solution. For example, let's look at the system

$$
	x_1 - x_2 &= 3

	-2x_1 + 2x_2 &= -6.
$$

To solve this system, let's put it into a matrix and row reduce it.

$$
	[[ 1, -1 | 3 ; -2, 2 | -6 ]] & 

	[[ 1, -1 | 3 ; 0, 0 | 0 ]] & \qquad \vec{r_2} \pe 2\vec{r_1}.
$$

The system is now in reduced row echelon form: that row of all zeros prevents us from clearing the $-1$ in the first row. The best we can do is convert this back to a system of equations, which gives us $x_1 - x_2 = 3$. The second equation is just $0 = 0$, which is useless to us since it's always true and doesn't contain any variables.

When we have more unknowns than equations like this, we get a solution involving **free parameters**. If $x_2 = t$ for *any* value of $t$, then $x_1 = 3 + t$ is a solution. We can write a vector solution as

$$
	\vec{x} = [[ 3 + t ; t ]].
$$

Generally speaking, encountering rows of all zeros while row-reducing means that we'll have one or more free parameters in our solution. But it sure seems like we could identify the problem upstream: those first two equations weren't very different from one another. Since the second equation was $-2$ times the first, it didn't contribute any new information to the system. Put another way, $-2$ times the first row plus $1$ times the second row equals the zero vector, and it's this situation --- and generalizations of it when there are more than two vectors --- that's exactly what we'll want to understand.

### def "linear combination and span"

	Let $\vec{v_1}, ..., \vec{v_k} \in #R#^n$. A **linear combination** of the $\vec{v_i}$ is a vector of the form

	$$
		c_1\vec{v_1} + \cdots + c_k\vec{v_k}
	$$

	for any scalars $c_i$. The **span** of all the $v_i$ is the set of *all* linear combinations of them, denoted

	$$
		\span\{v_1, ..., v_k\}.
	$$

###

For example, the linear combination in question from the previous question was

$$
	-2[[ 1 ; -1 ; 3 ]] + [[ -2 ; 2 ; -6 ]] = [[ 0 ; 0 ; 0 ]].
$$

Here, we've written the vectors as columns to be consistent and make it easier to parse, but the equation works just as well with row vectors too:

$$
	-2[[ 1, -1, 3 ]] + [[ -2, 2, -6 ]] = [[ 0, 0, 0 ]].
$$

We'll typically write the zero vector as $\vec{0}$ --- its shape should hopefully be clear from the equation it's in.

The cases in which a linear combination results in the zero vector are of the most interest to us: they're the ones that affect whether a system of equations has a solution involving free parameters. We'll be talking about that situation quite a bit, so let's give it a name.

### def "linear independence"

	A collection of vectors $v_1, ..., v_k \in #R#^n$ is **linearly dependent** if there is a linear combination of the $v_i$ that is equal to the zero vector (other than the linear combination with every coefficient being $0$). If a collection of vectors is not linearly dependent, we say it's **linearly independent**.

###

### ex "linear independence"

	Are the vectors $$\vec{v_1} = [[ 1 ; 2 ; -1 ]]$$, $$\vec{v_2} = [[ 2 ; 0 ; 4 ]]$$, and $$\vec{v_3} = [[ 0 ; 2 ; -3 ]]$$ linearly independent? Why or why not?

	### solution

	For the vectors to be linearly *dependent*, we would need scalars $c_1$, $c_2$, and $c_3$ so that

	$$
		c_1\vec{v_1} + c_2\vec{v_2} + c_3\vec{v_3} = \vec{0}
	$$

	and not all the $c_i$ are zero. A great lesson in math is to use existing theory whenever possible, and this is a great example: this is just a system of linear equations in the $c_i$! Specifically, it corresponds to the augmented matrix

	$$
		[[ 1, 2, 0 | 0 ; 2, 0, 2 | 0 ; -1, 4, -3 | 0 ]].
	$$

	Take a moment to really dig into that and convince yourself of why this is the case. If it doesn't become clear, it's absolutely worth starting with the vectors, multiplying them each by their respective constant, and then adding them together.

	Now that we have the system as a matrix, we can row-reduce it to solve for the $c_i$:

	$$
		[[ 1, 2, 0 | 0 ; 2, 0, 2 | 0 ; -1, 4, -3 | 0 ]] & 

		[[ 1, 2, 0 | 0 ; 0, -4, 2 | 0 ; 0, 6, -3 | 0 ]] & \qquad \begin{array}{l} \vec{r_2} \me 2\vec{r_1} \\ \vec{r_3} \pe \vec{r_1} \end{array}

		[[ 1, 2, 0 | 0 ; 0, 2, -1 | 0 ; 0, 2, -1 | 0 ]] & \qquad \begin{array}{l} \vec{r_2} \te -\frac{1}{2} \\ \vec{r_3} \te \frac{1}{3} \end{array}

		[[ 1, 0, 1 | 0 ; 0, 2, -1 | 0 ; 0, 0, 0 | 0 ]] & \qquad \begin{array}{l} \vec{r_3} \me \vec{r_2} \\ \vec{r_1} \me \vec{r_2} \end{array}
	$$

	Letting $c_3 = t$, we have $c_1 = -t$, $c_2 = \frac{1}{2}t$, and $c_3 = t$. Taking $t = 1$ for example, we have a linear combination that produces $\vec{0}$, so these vectors are linearly dependent. As a gut check, we expect there to be at least one free parameter whenever vectors are linearly dependent, since if we take any linear combination of $\vec{v_1}$, $\vec{v_2}$, $\vec{v_3}$, we can multiply every coefficient by any number $t$ to produce another linear combination equaling $\vec{0}$. If we had instead been able to row-reduce all the way to the identity matrix, then we would have found that $c_1 = c_2 = c_3 = 0$, meaning that was the only way to make a linear combination equal to $\vec{0}$ --- and so the vectors would be linearly independent.

###

### exc "linear independence"

	Are the vectors $$\vec{w_1} = [[ 2 ; 0 ; 1 ]]$$, $$\vec{w_2} = [[ -1 ; -1 ; 1 ]]$$, and $$\vec{w_3} = [[ 0 ; 1 ; 0 ]]$$ linearly independent? Why or why not?

###

When a collection of vectors $\vec{v_1}, ..., \vec{v_n} \in #R#^k$ has at least $k$ different linearly independent ones, then $\span\{\vec{v_1}, ..., \vec{v_n}\} = #R#^k$: that's because we can take the $k$ linearly independent vectors and row-reduce them as rows in a matrix to get to the identity matrix, so all the vectors $\vec{e_i}$ that are all zero except for a one in the $i$th position are expressible as linear combinations of the $\vec{v_j}$, and every vector in $#R#^k$ is expressible as a linear combination of the $\vec{e_i}$.

Similarly, when a collection of vectors $\vec{v_1}, ..., \vec{v_n} \in #R#^k$ has $n > k$, so there are more vectors than entries per vector, then the vectors have to be linearly dependent. To see this, place them all as rows in a matrix to produce one that's wider than it is tall, and then row-reduce it. At least one of the rows has to be all zero in the end, so the vectors must be linearly dependent. We'll see more on this result and the previous one in the homework for this section.

That's all for linear combinations and independence! These concepts will lay the groundwork for much of the rest of the course, though, and we'll return to them often.



### nav-buttons