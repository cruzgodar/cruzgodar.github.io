<header><div id="logo"><a href="/home" tabindex="-1"><img src="/graphics/general-icons/logo.webp" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 12: Bases</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">Although we&#x2019;ve already talked quite a bit about vectors, we&#x2019;re only just getting started &mdash; vectors have an incredible amount of algebraic and geometric properties that we&#x2019;ll be able to leverage before long. First, though, we&#x2019;re in desperate need of notation and terminology. At its worst, math can feel like pointless symbol-pushing, and so it&#x2019;s important to only add new notation when we truly need it to manage complexity. The best notation bottles up a moderately tricky idea and makes it usable without having to interface with it all the time.</p><p class="body-text">Let&#x2019;s begin with something we see everywhere: the set of all real numbers. A <strong>real</strong> number is one on the number line, whether rational or irrational &mdash; for example, <span class="tex-holder inline-math" data-source-tex="0">$0$,</span> <span class="tex-holder inline-math" data-source-tex="1">$1$,</span> <span class="tex-holder inline-math" data-source-tex="\pi">$\pi$,</span> <span class="tex-holder inline-math" data-source-tex="e">$e$,</span> and <span class="tex-holder inline-math" data-source-tex="\sqrt{2}">$\sqrt{2}$</span> are all real. We write the whole set of them as <span class="tex-holder inline-math" data-source-tex="\mathbb{R}">$\mathbb{R}$,</span> a rather fancy-looking R, and to actually use that symbol most of the time, we need one more: <span class="tex-holder inline-math" data-source-tex="\in">$\in$.</span> This one means &#x201C;element of&#x201D;, and it lets us write the rather verbose &#x201C;let <span class="tex-holder inline-math" data-source-tex="x">$x$</span> be a real number&#x201D; as simply &#x201C;let <span class="tex-holder inline-math" data-source-tex="x \in \mathbb{R}">$x \in \mathbb{R}$&#x201D;,</span> read &#x201C;let <span class="tex-holder inline-math" data-source-tex="x">$x$</span> be in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}">$\mathbb{R}$&#x201D;.</span></p><p class="body-text">A length-<span class="tex-holder inline-math" data-source-tex="2">$2$</span> vector just consists of two independent real numbers &mdash; it&#x2019;s effectively an ordered pair. If each element of an ordered pair has three possibilities (say <span class="tex-holder inline-math" data-source-tex="x">$x$</span> and <span class="tex-holder inline-math" data-source-tex="y">$y$</span> can each independently be <span class="tex-holder inline-math" data-source-tex="1">$1$,</span> <span class="tex-holder inline-math" data-source-tex="2">$2$,</span> or <span class="tex-holder inline-math" data-source-tex="3">$3$</span> in the pair <span class="tex-holder inline-math" data-source-tex="(x, y)">$(x, y)$),</span> then there are <span class="tex-holder inline-math" data-source-tex="3^2 = 9">$3^2 = 9$</span> possibilities for the whole pair. Inspired by the power of two, we write <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> (read &#x201C;R two&#x201D;) for the set of all ordered pairs &mdash; or equivalently, the set of all length-2 vectors. Similarly, <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> is the set of all length-<span class="tex-holder inline-math" data-source-tex="n">$n$</span> vectors.</p><p class="body-text">That&#x2019;s a fair amount of symbols, but it&#x2019;s thankfully all we&#x2019;ll have for a while. With the notation in place, we can start talking about the math.</p><p class="body-text">First of all, vectors are just matrices with a single column, so all of the nice properties of matrix arithmetic carry over to them. Two vectors of the same length are added together by just adding each corresponding entry, and two vectors of different lengths cannot be added together at all. We can multiply a vector by a scalar <span class="tex-holder inline-math" data-source-tex="c">$c$</span> (i.e. just a number) by multiplying each entry by <span class="tex-holder inline-math" data-source-tex="c">$c$.</span> None of this is different from matrices, but what&#x2019;s new to vectors is the ability to visualize them in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> When <span class="tex-holder inline-math" data-source-tex="n = 2">$n = 2$,</span> we have <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> which is just the <span class="tex-holder inline-math" data-source-tex="xy">$xy$-plane</span> &mdash; every length-<span class="tex-holder inline-math" data-source-tex="2">$2$</span> vector corresponds to a point in the plane. We typically think of these as <em>directions</em> rather than locations, and we draw them as arrows from the origin.</p><div class="desmos-border"><div id="planeVectors" class="desmos-container"></div></div><p class="body-text">When we add vectors, we add corresponding components together, so the result is a vector from the origin with the total amount of movement of both vectors in both the <span class="tex-holder inline-math" data-source-tex="x">$x$</span> and <span class="tex-holder inline-math" data-source-tex="y">$y$</span> directions. Geometrically, we can place the vectors one after the other so that the second begins where the first ends &mdash; then the sum is the vector that travels from the beginning of the first to the end of the second, since it has the total <span class="tex-holder inline-math" data-source-tex="x">$x$</span> and <span class="tex-holder inline-math" data-source-tex="y">$y$</span> movement of both vectors.</p><div class="desmos-border"><div id="vectorAddition" class="desmos-container"></div></div><p class="body-text">Scalar multiplication works similarly: multiplying a vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> by a positive constant <span class="tex-holder inline-math" data-source-tex="c">$c$</span> results in a vector that points in the same direction as <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$,</span> but <span class="tex-holder inline-math" data-source-tex="c">$c$</span> times as long.</p></section><h2 class="section-text">Linear Combinations and Linear Independence</h2><section><p class="body-text">As we&#x2019;ve seen, our primary use of vectors is to represent systems of equations with many equations and unknowns, and a critical feature of those systems is that they don&#x2019;t always have just one solution. For example, let&#x2019;s look at the system</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]x_1 - x_2 &= 3\\[NEWLINE][TAB]-2x_1 + 2x_2 &= -6.[NEWLINE]\end{align*}">$$\begin{align*}x_1 - x_2 &= 3\\[4px]-2x_1 + 2x_2 &= -6.\end{align*}$$</span></p><p class="body-text">To solve this system, let&#x2019;s put it into a matrix and row reduce it.</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -1 & 3 \\ -2& 2 & -6 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -1 & 3 \\ 0& 0 & 0 \end{array}\right] & \qquad \vec{r_2} \ +\!\!= 2\vec{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 1& -1 & 3 \\ -2& 2 & -6 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 1& -1 & 3 \\ 0& 0 & 0 \end{array}\right] & \qquad \vec{r_2} \ +\!\!= 2\vec{r_1}.\end{align*}$$</span></p><p class="body-text">The system is now in reduced row echelon form: that row of all zeros prevents us from clearing the <span class="tex-holder inline-math" data-source-tex="-1">$-1$</span> in the first row. The best we can do is convert this back to a system of equations, which gives us <span class="tex-holder inline-math" data-source-tex="x_1 - x_2 = 3">$x_1 - x_2 = 3$.</span> The second equation is just <span class="tex-holder inline-math" data-source-tex="0 = 0">$0 = 0$,</span> which is useless to us since it&#x2019;s always true and doesn&#x2019;t contain any variables.</p><p class="body-text">When we have more unknowns than equations like this, we get a solution involving <strong>free parameters</strong>. If <span class="tex-holder inline-math" data-source-tex="x_2 = t">$x_2 = t$</span> for <em>any</em> value of <span class="tex-holder inline-math" data-source-tex="t">$t$,</span> then <span class="tex-holder inline-math" data-source-tex="x_1 = 3 + t">$x_1 = 3 + t$</span> is a solution. We can write a vector solution as</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{x} = \left[\begin{array}{c} 3 + t \\ t \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{x} = \left[\begin{array}{c} 3 + t \\ t \end{array}\right].\end{align*}$$</span></p><p class="body-text">Generally speaking, encountering rows of all zeros while row-reducing means that we&#x2019;ll have one or more free parameters in our solution. But it sure seems like we could identify the problem upstream: those first two equations weren&#x2019;t very different from one another. Since the second equation was <span class="tex-holder inline-math" data-source-tex="-2">$-2$</span> times the first, it didn&#x2019;t contribute any new information to the system. Put another way, <span class="tex-holder inline-math" data-source-tex="-2">$-2$</span> times the first row plus <span class="tex-holder inline-math" data-source-tex="1">$1$</span> times the second row equals the zero vector, and it&#x2019;s this situation &mdash; and generalizations of it when there are more than two vectors &mdash; that&#x2019;s exactly what we&#x2019;ll want to understand.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: linear combination and span</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_k} \in \mathbb{R}^n">$\vec{v_1}, ..., \vec{v_k} \in \mathbb{R}^n$.</span> A <strong>linear combination</strong> of the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> is a vector of the form</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\vec{v_1} + \cdots + c_k\vec{v_k}[NEWLINE]$$">$$\begin{align*}c_1\vec{v_1} + \cdots + c_k\vec{v_k}\end{align*}$$</span></p><p class="body-text">for any scalars <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$.</span> The <strong>span</strong> of all the <span class="tex-holder inline-math" data-source-tex="v_i">$v_i$</span> is the set of <em>all</em> linear combinations of them, denoted</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\operatorname{span}\{v_1, ..., v_k\}.[NEWLINE]$$">$$\begin{align*}\operatorname{span}\{v_1, ..., v_k\}.\end{align*}$$</span></p></div><p class="body-text">For example, the linear combination in question from the previous question was</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]-2\left[\begin{array}{c} 1 \\ -1 \\ 3 \end{array}\right] + \left[\begin{array}{c} -2 \\ 2 \\ -6 \end{array}\right] = \left[\begin{array}{c} 0 \\ 0 \\ 0 \end{array}\right].[NEWLINE]$$">$$\begin{align*}-2\left[\begin{array}{c} 1 \\ -1 \\ 3 \end{array}\right] + \left[\begin{array}{c} -2 \\ 2 \\ -6 \end{array}\right] = \left[\begin{array}{c} 0 \\ 0 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Here, we&#x2019;ve written the vectors as columns to be consistent and make it easier to parse, but the equation works just as well with row vectors too:</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]-2\left[\begin{array}{ccc} 1& -1& 3 \end{array}\right] + \left[\begin{array}{ccc} -2& 2& -6 \end{array}\right] = \left[\begin{array}{ccc} 0& 0& 0 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}-2\left[\begin{array}{ccc} 1& -1& 3 \end{array}\right] + \left[\begin{array}{ccc} -2& 2& -6 \end{array}\right] = \left[\begin{array}{ccc} 0& 0& 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">We&#x2019;ll typically write the zero vector as <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$</span> &mdash; its shape should hopefully be clear from the equation it&#x2019;s in.</p><p class="body-text">The cases in which a linear combination results in the zero vector are of the most interest to us: they&#x2019;re the ones that affect whether a system of equations has a solution involving free parameters. We&#x2019;ll be talking about that situation quite a bit, so let&#x2019;s give it a name.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: linear independence</div><p class="body-text">A collection of vectors <span class="tex-holder inline-math" data-source-tex="v_1, ..., v_k \in \mathbb{R}^n">$v_1, ..., v_k \in \mathbb{R}^n$</span> is <strong>linearly dependent</strong> if there is a linear combination of the <span class="tex-holder inline-math" data-source-tex="v_i">$v_i$</span> that is equal to the zero vector (other than the linear combination with every coefficient being <span class="tex-holder inline-math" data-source-tex="0">$0$).</span> If a collection of vectors is not linearly dependent, we say it&#x2019;s <strong>linearly independent</strong>.</p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: linear independence</div><p class="body-text">Are the vectors <span class="tex-holder inline-math" data-source-tex="\displaystyle \vec{v_1} = \left[\begin{array}{c} 1 \\ 2 \\ -1 \end{array}\right]">$\displaystyle \vec{v_1} = \left[\begin{array}{c} 1 \\ 2 \\ -1 \end{array}\right]$,</span> <span class="tex-holder inline-math" data-source-tex="\displaystyle \vec{v_2} = \left[\begin{array}{c} 2 \\ 0 \\ 4 \end{array}\right]">$\displaystyle \vec{v_2} = \left[\begin{array}{c} 2 \\ 0 \\ 4 \end{array}\right]$,</span> and <span class="tex-holder inline-math" data-source-tex="\displaystyle \vec{v_3} = \left[\begin{array}{c} 0 \\ 2 \\ -3 \end{array}\right]">$\displaystyle \vec{v_3} = \left[\begin{array}{c} 0 \\ 2 \\ -3 \end{array}\right]$</span> linearly independent? Why or why not?</p><div class="solution"></div><p class="body-text">For the vectors to be linearly <em>dependent</em>, we would need scalars <span class="tex-holder inline-math" data-source-tex="c_1">$c_1$,</span> <span class="tex-holder inline-math" data-source-tex="c_2">$c_2$,</span> and <span class="tex-holder inline-math" data-source-tex="c_3">$c_3$</span> so that</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\vec{v_1} + c_2\vec{v_2} + c_3\vec{v_3} = \vec{0}[NEWLINE]$$">$$\begin{align*}c_1\vec{v_1} + c_2\vec{v_2} + c_3\vec{v_3} = \vec{0}\end{align*}$$</span></p><p class="body-text">and not all the <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span> are zero. A great lesson in math is to use existing theory whenever possible, and this is a great example: this is just a system of linear equations in the <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$!</span> Specifically, it corresponds to the augmented matrix</p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 2& 0& 2 & 0 \\ -1& 4& -3 & 0 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 2& 0& 2 & 0 \\ -1& 4& -3 & 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Take a moment to really dig into that and convince yourself of why this is the case. If it doesn&#x2019;t become clear, it&#x2019;s absolutely worth starting with the vectors, multiplying them each by their respective constant, and then adding them together.</p><p class="body-text">Now that we have the system as a matrix, we can row-reduce it to solve for the <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$:</span></p><p class="body-text" style="text-align: center; line-height: 0"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 2& 0& 2 & 0 \\ -1& 4& -3 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 0& -4& 2 & 0 \\ 0& 6& -3 & 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ -\!\!= 2\vec{r_1} \\ \vec{r_3} \ +\!\!= \vec{r_1} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 0& 2& -1 & 0 \\ 0& 2& -1 & 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ \times\!\!= -\frac{1}{2} \\ \vec{r_3} \ \times\!\!= \frac{1}{3} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 1 & 0 \\ 0& 2& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_3} \ -\!\!= \vec{r_2} \\ \vec{r_1} \ -\!\!= \vec{r_2} \end{array}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 2& 0& 2 & 0 \\ -1& 4& -3 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 0& -4& 2 & 0 \\ 0& 6& -3 & 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ -\!\!= 2\vec{r_1} \\ \vec{r_3} \ +\!\!= \vec{r_1} \end{array}\\[4px]\left[\begin{array}{ccc|c} 1& 2& 0 & 0 \\ 0& 2& -1 & 0 \\ 0& 2& -1 & 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_2} \ \times\!\!= -\frac{1}{2} \\ \vec{r_3} \ \times\!\!= \frac{1}{3} \end{array}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 1 & 0 \\ 0& 2& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \qquad \begin{array}{l} \vec{r_3} \ -\!\!= \vec{r_2} \\ \vec{r_1} \ -\!\!= \vec{r_2} \end{array}\end{align*}$$</span></p><p class="body-text">Letting <span class="tex-holder inline-math" data-source-tex="c_3 = t">$c_3 = t$,</span> we have <span class="tex-holder inline-math" data-source-tex="c_1 = -t">$c_1 = -t$,</span> <span class="tex-holder inline-math" data-source-tex="c_2 = \frac{1}{2}t">$c_2 = \frac{1}{2}t$,</span> and <span class="tex-holder inline-math" data-source-tex="c_3 = t">$c_3 = t$.</span> Taking <span class="tex-holder inline-math" data-source-tex="t = 1">$t = 1$</span> for example, we have a linear combination that produces <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$,</span> so these vectors are linearly dependent. As a gut check, we expect there to be at least one free parameter whenever vectors are linearly dependent, since if we take any linear combination of <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{v_2}">$\vec{v_2}$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{v_3}">$\vec{v_3}$,</span> we can multiply every coefficient by any number <span class="tex-holder inline-math" data-source-tex="t">$t$</span> to produce another linear combination equaling <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$.</span> If we had instead been able to row-reduce all the way to the identity matrix, then we would have found that <span class="tex-holder inline-math" data-source-tex="c_1 = c_2 = c_3 = 0">$c_1 = c_2 = c_3 = 0$,</span> meaning that was the only way to make a linear combination equal to <span class="tex-holder inline-math" data-source-tex="\vec{0}">$\vec{0}$</span> &mdash; and so the vectors would be linearly independent.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: linear independence</div><p class="body-text">Are the vectors <span class="tex-holder inline-math" data-source-tex="\displaystyle \vec{w_1} = \left[\begin{array}{c} 2 \\ 0 \\ 1 \end{array}\right]">$\displaystyle \vec{w_1} = \left[\begin{array}{c} 2 \\ 0 \\ 1 \end{array}\right]$,</span> <span class="tex-holder inline-math" data-source-tex="\displaystyle \vec{w_2} = \left[\begin{array}{c} -1 \\ -1 \\ 1 \end{array}\right]">$\displaystyle \vec{w_2} = \left[\begin{array}{c} -1 \\ -1 \\ 1 \end{array}\right]$,</span> and <span class="tex-holder inline-math" data-source-tex="\displaystyle \vec{w_3} = \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right]">$\displaystyle \vec{w_3} = \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right]$</span> linearly independent? Why or why not?</p></div><p class="body-text">When a collection of vectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n} \in \mathbb{R}^k">$\vec{v_1}, ..., \vec{v_n} \in \mathbb{R}^k$</span> has at least <span class="tex-holder inline-math" data-source-tex="k">$k$</span> different linearly independent ones, then <span class="tex-holder inline-math" data-source-tex="\operatorname{span}\{\vec{v_1}, ..., \vec{v_n}\} = \mathbb{R}^k">$\operatorname{span}\{\vec{v_1}, ..., \vec{v_n}\} = \mathbb{R}^k$:</span> that&#x2019;s because we can take the <span class="tex-holder inline-math" data-source-tex="k">$k$</span> linearly independent vectors and row-reduce them as rows in a matrix to get to the identity matrix, so all the vectors <span class="tex-holder inline-math" data-source-tex="\vec{e_i}">$\vec{e_i}$</span> that are all zero except for a one in the <span class="tex-holder inline-math" data-source-tex="i">$i$th</span> position are expressible as linear combinations of the <span class="tex-holder inline-math" data-source-tex="\vec{v_j}">$\vec{v_j}$,</span> and every vector in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^k">$\mathbb{R}^k$</span> is expressible as a linear combination of the <span class="tex-holder inline-math" data-source-tex="\vec{e_i}">$\vec{e_i}$.</span></p><p class="body-text">Similarly, when a collection of vectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n} \in \mathbb{R}^k">$\vec{v_1}, ..., \vec{v_n} \in \mathbb{R}^k$</span> has <span class="tex-holder inline-math" data-source-tex="n > k">$n > k$,</span> so there are more vectors than entries per vector, then the vectors have to be linearly dependent. To see this, place them all as rows in a matrix to produce one that&#x2019;s wider than it is tall, and then row-reduce it. At least one of the rows has to be all zero in the end, so the vectors must be linearly dependent. We&#x2019;ll see more on this result and the previous one in the homework for this section.</p><p class="body-text">That&#x2019;s all for linear combinations and independence! These concepts will lay the groundwork for much of the rest of the course, though, and we&#x2019;ll return to them often.</p><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>