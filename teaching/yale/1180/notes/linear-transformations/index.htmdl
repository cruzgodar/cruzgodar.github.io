### nav-buttons



A common trend in math is to define a new type of object (e.g. a vector), develop some properties about it, and then begin to study functions (often called **maps**) that operate on those objects. Depending on the structure of the objects in question, the characteristics of the maps we want to study are slightly different. Here, those objects are vectors in $#R#^n$, and the maps will be quite similar to matrices.

As we've discussed, an $m \times n$ matrix $A$ is a function that takes in length-$n$ vectors and gives back length-$m$ ones, so we can write $A : #R#^n \to #R#^m$. We've also seen that for any vectors $\vec{v}, \vec{w} \in #R#^n$,

$$
	A(\vec{v} + \vec{w}) = A\vec{v} + A\vec{w},
$$

and $A(c\vec{v}) = cA\vec{v}$ for any constant $c$. Inspired by these properties, let's look at functions that are *defined* this way: ones that factor through addition and scalar multiplication of vectors.

### def "linear transformation"
	
	A **linear transformation**, or **linear map**, from $#R#^n$ to $#R#^m$ is a function $T: #R#^n \to #R#^m$, such that
	
	1. For any two vectors $\vec{v}, \vec{w} \in #R#^n$, $T(\vec{v} + \vec{w}) = T(\vec{v}) + T(\vec{w})$.
	
	2. For any vector $\vec{n} \in #R#^n$ and any constant $c$, $T(c\vec{v}) = cT(\vec{v})$.
	
###

First of all, one immediate property of any linear transformation $T$ is that it must send the zero vector to itself:

$$
	T(\vec{0}) = T(0 \cdot \vec{0}) = 0 \cdot T(\vec{0}) = \vec{0}.
$$

Linear transformations from $#R#$ to $#R#$ aren't very exciting: if $T$ is such a linear transformation, then

$$
	T(x) = T(x \cdot 1) = x T(1),
$$

so as soon as we know what $T(1)$ is, every other output of $T$ is fixed. If $a = T(1)$, then $T(x) = ax$, so linear transformations $#R# \to #R#$ are just lines through the origin. It might seem like needless attention to detail, but let's take a moment to graph these.

### desmos 1dLinearTransformation

Here, the input vector (i.e. the number $1$) is colored purple, and the output vector colored blue. The value of $a$ is given by the red point. While we normally plot functions with inputs on one axis and outputs on another, we're using the $x$-axis here for both input and output. That's for two reasons: first, it's foreshadowing for the two-dimensional case we're about to consider, and more importantly, it reflects the fact that we don't need to see other input-output pairs to completely understand the effects of a linear map. Any other input, like $-2$, is sent to $-2$ times whatever $1$ is sent to --- that's what it means to be a linear map.

When we're not confined to a single input and output variable, linear transformations become much more interesting. Let's take a linear map $T : #R#^2 \to #R#^2$. By using the properties of linear maps, we can reduce the problem of evaluating it on a generic vector to just two very specific ones:

$$
	T\left( [[ a  ; b ]] \right) &= T\left( [[ a ; 0 ]] + [[ 0 ; b ]] \right)

	&= T\left( [[ a ; 0 ]] \right) + T \left( [[ 0 ; b ]] \right)

	&= T\left( a [[ 1 ; 0 ]] \right) + T \left( b [[ 0 ; 1 ]] \right)

	&= a T\left( [[ 1 ; 0 ]] \right) + b T \left( [[ 0 ; 1 ]] \right).
$$

So as soon as we know where $T$ sends $[[ 1 ; 0 ]]$ and $[[ 0 ; 1 ]]$, finding where it sends any other vector is as simple as taking a linear combination of those two outputs.

### desmos 2dLinearTransformation

There's a lot going on in this graph, so let's go through it carefully. The red and blue vectors that can't be dragged are $[[ 1 ; 0 ]]$ and $[[ 0 ; 1 ]]$, and the ones that can be dragged are the corresponding outputs --- i.e. $T \left( [[ 1 ; 0 ]] \right)$ and $T \left( [[ 0 ; 1 ]] \right)$. To give them names, let's say those output vectors are

$$
	T \left( [[ 1 ; 0 ]] \right) &= [[ a_{11} ; a_{21} ]]

	T \left( [[ 0 ; 1 ]] \right) &= [[ a_{12} ; a_{22} ]].
$$

As we just figured out, the entire behavior of $T$ is determined once those four values are chosen. The draggable purple vector $[[ x_1 ; x_2 ]]$ is fed into $T$, and the output is the non-draggable purple vector. To make this all a lot more concrete, let's work through an example.

### ex "linear transformation"

	Suppose $T : #R#^2 \to #R#^2$ is a linear transformation satisfying

	$$
		T \left( [[ 1 ; 0 ]] \right) &= [[ 1 ; 3 ]]

		T \left( [[ 0 ; 1 ]] \right) &= [[ 0 ; -2 ]].
	$$

	Find $T \left( [[ 3 ; 4 ]] \right)$.

	### solution

	Since $T$ is linear, we can expand that last expression quite a bit:

	$$
		T \left( [[ 3 ; 4 ]] \right) &= 3 T \left( [[ 1 ; 0 ]] \right) + 4 T \left( [[ 0 ; 1 ]] \right)

		&= 3 [[ 1 ; 3 ]] + 4 [[ 0 ; -2 ]]

		&= [[ 3 ; 1 ]].
	$$

	In the previous graph, setting $a_{11} = 1$, $a_{21} = 3$, $a_{12} = 0$, and $a_{22} = -2$ results in a linear map $T$ that sends $[[ 3 ; 4 ]]$ to $[[ 3 ; 1 ]]$. Drag the purple point to $(3, 4)$ to check!

###

### exc "linear transformation"

	Suppose $S : #R#^3 \to #R#^2$ is a linear transformation satisfying

	$$
		S \left( [[ 1 ; 0 ; 1 ]] \right) = [[ 1 ; 1 ]]

		S \left( [[ 0 ; 1 ; 1 ]] \right) = [[ 3 ; 1 ]]

		S \left( [[ 0 ; 0 ; 2 ]] \right) = [[ 6 ; 2 ]].
	$$

	Find $S \left( [[ 1 ; 2 ; 3 ]] \right)$.

###

In so many symbols, sending a vector $[[ x_1 ; x_2 ]]$ through $T$ results in a linear combination of $T \left( [[ 1 ; 0 ]] \right)$ and $T \left( [[ 0 ; 1 ]] \right)$. In fact, that's what it means for $T$ to be a linear map in the first place.

Let's fully work the $2 \times 2$ case through in general. We've already given names to all three relevant vectors: the results of $T$ applied to the two unit coordinate vectors, and the generic purple input vector. Applying $T$ to it, we have

$$
	T \left( [[ x_1 ; x_2 ]] \right) &= x_1 T \left( [[ 1 ; 0 ]] \right) + x_2 T \left( [[ 0 ; 1 ]] \right)

	&= x_1 [[ a_{11} ; a_{21} ]] + x_2 [[ a_{12} ; a_{22} ]].
$$

Now that looks a whole lot like matrix multiplication! Specifically, it's a matrix times a vector:

$$
	T \left( [[ x_1 ; x_2 ]] \right) = [[ a_{11}, a_{12} ; a_{21}, a_{22} ]] [[ x_1 ; x_2 ]].
$$

And so we get to the fundamental result of the section: **linear transformations from $#R#^n$ to $#R#^m$ are just $m \times n$ matrices**. We knew the reverse --- that matrices were linear maps --- but this says that the other direction is also true, and so we can interchangeably refer to linear transformations and matrices. To represent a linear transformation as a matrix, evaluate it on all of the $\vec{e_i}$ (the vectors with a $1$ in position $i$ and zeros everywhere else), and place the results as the columns of a matrix.

### ex "linear transformations as matrices"

	Let $T : #R#^3 \to #R#^3$ be the linear transformation defined by

	$$
		T\left( [[ a ; b ; c ]] \right) &= [[ 2a + 3b ; b + 2a ; 2c - a ]],
	$$

	Find the matrix for $T$.

	### solution

	By evaluating $T$ on $\vec{e_1}$, $\vec{e_2}$, $\vec{e_3}$, we have

	$$
		T(\vec{e_1}) &= [[ 2 ; 2 ; -1 ]]

		T(\vec{e_2}) &= [[ 3 ; 1 ; 0 ]]

		T(\vec{e_3}) &= [[ 0 ; 0 ; 2 ]],
	$$

	and so the matrix for $T$ is

	$$
		A = [[ 2, 3, 0 ; 2, 1, 0 ; -1, 0, 2 ]].
	$$

	In other words, $T(\vec{v}) = A\vec{v}$ for any vector $\vec{v} \in #R#^3$. This is the core of the idea that matrices are functions that act by multiplication: on the left is the function, and on the right is the matrix multiplication.

###

### exc "linear transformations as matrices"

	Let $S : #R#^3 \to #R#^2$ be a linear transformation defined by

	$$
		S\left( [[ x ; y ; z ]] \right) &= [[ x + z ; 3x - 2y ]].
	$$

	Find the matrix $B$ for $S$, and then verify that the matrix for the composition $S \circ T$ is $BA$.

###



## Properties of Transformations

With the foundation of matrices, inverses, linear transformations, and linear independence, we're ready to tie everything we've learned together. We'll begin by defining two properties that a linear transformation can have, and we'll soon see how they relate to the ones we've already seen.

### def "one-to-one and onto"

	A function $f : #R#^n \to #R#^m$ is **one-to-one** (or **injective**) if $f(\vec{v}) \neq f(\vec{w})$ whenever $\vec{v} \neq \vec{w}$. It's **onto** (or **surjective**) if every vector $\vec{u} \in #R#^m$ satisfies $\vec{u} = f(\vec{v})$ for some vector $\vec{v} \in #R#^n$.

###

In simple terms, one-to-one functions are those that pass the horizontal line test, and onto ones are those whose image (i.e. range) is equal to their codomain.

This first definition of one-to-one is unfortunately a little too clunky to work with directly most of the time. Instead, we'll use an equivalent one that's quite a bit more streamlined.

### prop "an equivalent one-to-one definition"

	A linear transformation $T : #R#^n \to #R#^m$ is one-to-one exactly when the only vector $\vec{v} \in #R#^n$ for which $T(\vec{v}) = \vec{0}$ is $\vec{v} = \vec{0}$.

###

### pf

	Let's very briefly sketch the reasoning behind this proposition. First of all, if $T$ sends a vector $\vec{v} \neq \vec{0}$ to $\vec{0}$, then $T(\vec{v}) = \vec{0} = T(\vec{0})$, so $T$ isn't one-to-one. On the other hand, if the only vector $T$ sends to $\vec{0}$ is $\vec{0}$, then whenever we have $T(\vec{v}) = T(\vec{w})$, we can rearrange it to get

	$$
		T(\vec{w}) - T(\vec{w}) = T(\vec{v} - \vec{w}) = \vec{0},
	$$

	And under the assumption, that means $\vec{v} - \vec{w} = \vec{0}$, so $\vec{v} = \vec{w}$, meaning $T$ is one-to-one.

###

It's important to note that this alternate definition works *only* with linear transformations, not functions in general. For example, $f(x) = x^2$ isn't one-to-one, since $f(1) = f(-1)$, but the only $x$ with $f(x) = 0$ is $x = 0$.

Similarly, we'll want to use a different definition of onto when we're actually working with transformations. Recall that applying a linear transformation to a vector results in taking a linear combination of the columns of the linear transformation's matrix, where the coefficients are given by the vector (this is just restating matrix-vector multiplication). For example, the transformation $T : #R#^2 \to #R#^3$ defined by

$$
	T\left( [[ x ; y ]] \right) = [[ 2x + y ; -y ; 3x + 4y ]]
$$

corresponds to the matrix

$$
	[[ 2, 1 ; 0, -1 ; 3, 4]]
$$,

and applying it to a vector results in

$$

	[[ 2, 1 ; 0, -1 ; 3, 4]] [[ x ; y ]] = [[ 2 ; 0 ; 3 ]]x + [[ 1 ; -1 ; 4 ]]y.
$$

Since we can plug in whatever we like for $x$ and $y$, the image of $T$ --- the set of outputs that actually occur --- is the *span of the columns of the matrix corresponding to $T$*. That's absolutely a mouthful, so let's give it a more concise name.

### def "column space"

	Let $T$ be a linear transformation. The **column space** of $T$ is the span of the columns of the matrix corresponding to $T$. The column space of $T$ is equal to $\image T$, so we don't need extra notation to distinguish it.

###

Let's finally bring it all together. For a linear transformation $T : #R#^n \to #R#^m$ to be onto --- that is, for its column space to be equal to $#R#^m$ --- we need the columns of the matrix for $T$ to have at least $m$ linearly independent vectors among them. On the other hand, for $T$ to be one-to-one, we need *all* the columns to be linearly independent --- if there's any linear dependence, then two different inputs will get sent to the same output. It's tempting to say that being one-to-one is a stronger condition than being onto, but this actually isn't always the case, as we'll shortly see. For now, though, we can state the result that will let us properly evaluate individual transformations.

### thm "criteria for one-to-one and onto"

	Let $T : #R#^n \to #R#^m$ be a linear transformation and let $A$ be the $m \times n$ matrix corresponding to it. Then $T$ is one-to-one exactly when all the columns of $A$ are linearly independent, and $T$ is onto exactly when there are at least $m$ linearly independent columns of $A$.

###

To check how many vectors in a set (e.g. the columns of a matrix) are linearly independent, the simplest method is to place them all as *rows* in a matrix and reduce it. That's because row operations replace rows with linear combinations of other rows, with the goal of making them zero if possible. When we're done reducing, the number of nonzero rows left is the number of linearly independent vectors in the set we started with.

### ex "one-to-one and onto"
	
	Let $T : #R#^3 \to #R#^3$ be defined by

	$$
		T\left( [[ x ; y ; z ]] \right) = [[ 2x + 3y + 5z ; 3x + 4y + 7z ; x - 2y - z ]].
	$$

	Is $T$ one-to-one? Is it onto?

	### solution

	By the previous theorem, we first want to express $T$ as a matrix: it corresponds to

	$$
		A = [[ 2, 3, 5 ; 3, 4, 7 ; 1, -2, -1 ]],
	$$

	so we want to row-reduce the matrix whose rows are the columns of $A$.

	$$
		[[ 2, 3, 1 ; 3, 4, -2 ; 5, 7, -1 ]] &

		[[ 2, 3, 1 ; 6, 8, -4 ; 10, 14, -2 ]] & \qquad \begin{array}{l} \vec{r_2} \te 2 \\ \vec{r_3} \te 2 \end{array}

		[[ 2, 3, 1 ; 0, -1, -7 ; 0, -1, -7 ]] & \qquad \begin{array}{l} \vec{r_2} \me 3\vec{r_1} \\ \vec{r_3} \me 5 \vec{r_1} \end{array}

		[[ 2, 3, 1 ; 0, -1, -7 ; 0, 0, 0 ]] & \qquad \vec{r_3} \me \vec{r_2}

		[[ 2, 0, 20 ; 0, -1, -7 ; 0, 0, 0 ]] & \qquad \vec{r_1} \pe 3\vec{r_2}

		[[ 1, 0, 10 ; 0, 1, 7 ; 0, 0, 0 ]] & \qquad \begin{array}{l} \vec{r_1} \te \frac{1}{2} \\ \vec{r_2} \te -1 \end{array}
	$$

	In the end, we have two linearly independent vectors, so $A$ has only two independent columns. Since not all the columns were independent, $T$ isn't one-to-one, and since there weren't at least three linearly independent columns, $T$ isn't onto. That sentence certainly seems a bit redundant when we write it out, and we'll have more to say on the subject momentarily.

###

### exc "one-to-one and onto"

	Let $S : #R#^3 \to #R#^2$ be defined by

	$$
		S\left( [[ x ; y ; z ]] \right) = [[ x + y - z ; 2x + y - 3z ]].
	$$

	Is $S$ one-to-one? Is it onto?

###



## Invertibility

Let's consider, for yet another time, a linear transformation $T : #R#^n \to #R#^m$. If $n > m$, then there are more columns than rows in the matrix corresponding to $T$, and so the columns can't all possibly be linearly independent. That's because when we place them as rows in an $n \times m$ matrix and row-reduce it, the best that the process could go is if none of the first $m$ reduce to rows of all zeros --- but then we could use those reduced rows to clear all of the others. For example, consider the matrix

$$
	A = [[ 1, 3, 5 ; -1, 4, 2 ]].
$$

Placing the rows of $A$ as columns in a new matrix and partially row-reducing, we get

$$
	[[ 1, 1 ; 3, 4 ; 5, 2 ]] & 

	[[ 1, 1 ; 0, 1 ; 5, 2 ]] & \qquad \vec{r_2} \me 3\vec{r_1}.
$$

Since neither of the first two rows is all zero, we're going to be able to reduce the third row to zero no matter what its entries are. Geometrically, if we already have two linearly independent vectors in $#R#^2$, there's no third vector in $#R#^2$ that's linearly independent with them both.

On the other hand, if $n < m$, then there are more rows than columns. This case is a lot simpler: for $T$ to be onto, there need to be at least $m$ linearly independent columns, but there are only $n$ columns at all, linearly independent or not. We can sum up our findings in far fewer words:

### prop "one-to-one and/or onto rectangular matrices"

	Let $T : #R#^n \to #R#^m$ be a linear transformation. If $n > m$, then $T$ can't be one-to-one. If $n < m$, then $T$ can't be onto.

###

Geometrically, it's helpful to think of two cases here: first, when $n = 2$ and $m = 1$, we're squishing all of $#R#^2$ down onto $#R#^1$, which is effectively mapping the $xy$-plane onto the $x$-axis. There's just no way to do that in a nice (i.e. linear) fashion without sending many inputs to one output. On the other hand, when $n = 1$ and $m = 2$, we're doing the reverse: mapping the $x$-axis into the $xy$-plane. There's no nice way for that map to fill up all of the plane, which is exactly what would need to happen for it to be onto.

When we have a one-to-one and onto linear transformation $T : #R#^n \to #R#^m$, then we now know that $m = n$ --- otherwise, one of the two would be impossible. That means the matrix for $T$ is square, and the two conditions now say exactly the same thing: $T$ being one-to-one means it has all $n$ of its columns linearly independent, and onto means it has at least $n$ of its columns linearly independent. And in fact, both being one-to-one and being onto are equivalent to a property we're already familiar with:

### thm "invertibility"

	Let $T : #R#^n \to #R#^n$ be a linear transformation whose domain and codomain are the same. Then all of the following statements are equivalent:

	1. $T$ is one-to-one.

	2. $T$ is onto.

	3. $T$ is invertible.
	
###

Our transformation in the previous example isn't invertible by this theorem, which means both that there is no inverse linear transformation and that the corresponding matrix isn't invertible. In fact, we didn't even need to ask whether it was onto after concluding it was one-to-one: since the matrix was square, the two properties are one and the same.

At this point, we're finished with the material in the section, but I think it's worth taking a minute to summarize where we are in regard to linear transformations, matrices, and the interplay between them. There are a lot of terms and ideas to digest, and a quick list organizing many of them should help. For all the items in the list, we let $T : #R#^n \to #R#^m$ be a linear transformation and $A$ its corresponding matrix.

> - Evaluation: we evaluate $T(\vec{v})$ by plugging the entries of $\vec{v}$ into a formula for $T$, just like a one-variable function $f(x)$. We evaluate a matrix on a vector by the matrix-vector multiplication $A\vec{v}$.

> - Composition: we compose $T$ with another linear map $S: #R#^m \to #R#^k$ to form the map $T \circ S : #R#^n \to #R#^k$ by plugging the formula for the output of $S$ as the input to the formula for $T$. We compose $A$ with an $m \times k$ matrix $B$ to form $AB$ by matrix multiplication.

> - One-to-one: $T$ is one-to-one if $T(\vec{v}) \neq T(\vec{w})$ for $\vec{v} \neq \vec{w}$, if $T(\vec{v}) \neq T(\vec{0})$ for $\vec{v} \neq \vec{0}$, or if all of the columns of $A$ are linearly independent.

> - Onto: $T$ is onto if for any vector $\vec{w} \in #R#^m$, there is a vector $\vec{v} \in #R#^n$ with $T(\vec{v} = \vec{w})$, or if there are at least $m$ linearly independent columns of $A$.

> - Invertibility: $T$ is invertible if its domain and codomain are equal (so $m = n$), and it is one-to-one and onto (checking either one of those suffices to determine both). Alternatively, $T$ is invertible if $A$ is invertible, meaning it row-reduces to the identity matrix.

> - Inversion: we don't invert transformations directly with their formulas, only with their matrices. With a matrix $A$, we invert it by augmenting with $I$ and row-reducing.

We're getting some hints that the bulk of the subject lies with square matrices: they're the only ones that can be invertible, and being one-to-one and onto are identical conditions. And as we'll see in the next section, the next big object we'll want to develop depends on the matrices it takes in being square.



### nav-buttons