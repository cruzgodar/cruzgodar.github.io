### nav-buttons



Part of the confusion surrounding matrices comes from the heavily-blurred line between matrix multiplication and function composition --- but at the same time, many of the most powerful uses of matrices come from playing those two perspectives off of one another. One of the goals of this section will be to take a matrix $A$ and produce an **inverse matrix** $A^{-1}$ that undoes $A$. By multiplying both sides of an equation $A\vec{x} = \vec{b}$ by $A^{-1}$, we'll be able to solve for $\vec{x}$ all at once.

First of all, let's talk about the identity matrix. We mentioned it briefly in the last section, but it's worth revisiting now that we understand more about matrix multiplication. The $n \times n$ identity matrix $I$ is given by

$$
	I = [[ 1, 0, \cdots, 0, 0 ; 0, 1, \cdots, 0, 0 ; \vdots, \vdots, \ddots, \vdots, \vdots ; 0, 0, \cdots, 1, 0 ; 0, 0, \cdots, 0, 1 ]],
$$

and its purpose is to serve as the identity *function*, meaning it leaves matrices and vectors unchanged when multiplied by them. (You'll verify in a homework problem that $AI = IA = A$ for any matrix $A$). In the language of multiplication, $I$ is the equivalent of the number $1$, and in the language of functions, it's the equivalent of the identity function $f(x) = x$.



## Row Operations

Matrices represent systems of equations, which are things that change as we solve them, so it'd be nice to have some sort of equivalence relation, where two matrices are equivalent if they represent the same system of equations. When we solve systems by hand with methods like substitution, variables are moving all over the place, including to the right side of the equals signs, but matrices don't have any support for that --- the $i$th column is where $x_i$ lives, and it can't leave. To comply with the rather strict conditions of matrices, there are actually only three things we can do to a matrix to solve it. Since any sort of operation like this will affect the right side of the equations too (i.e. it will change $\vec{b}$ in $A\vec{x} = \vec{b}$), we'll want to put $A$ and $\vec{b}$ together. The simplest way is by **augmenting** the matrix $A$. This isn't a fancy solution, but it gets the job done: we'll just shove $\vec{b}$ in on the right side of $A$. For example, the system of equations

$$
	2x_1 - x_2 &= 3
	
	x_1 + 5x_2 &= -1.
$$

has the matrix form

$$
	[[ 2, -1 ; 1, 5 ]] [[ x_1 ; x_2 ]] = [[ 3 ; -1 ]].
$$

To represent this system in a single augmented matrix, we write it as

$$
	[[ A | \vec{b} ]] = [[ 2, -1 | 3 ; 1, 5 | -1 ]].
$$

So: what operations can we do that preserve the equation structure? Since each row represents its own equation, we can definitely swap any two rows. We can't add or subtract a single number or variable on both sides, because there can't be any constants on the left or variables on the right --- but we can multiply an entire row by a nonzero constant (multiplying both sides by zero is the fastest way to destroy information when solving an equation, so we definitely want to avoid that). Finally, we can add any multiple of one row to another, analogous to the method of addition for solving systems. Together, these form the three **elementary row operations** on matrices, and they let us reduce matrices to simpler forms through a step-by-step process. Let's define that simpler form and then describe how to get there.

### def "reduced row echelon form"
	
	A matrix $A$ is in **reduced row echelon form** if every row is either all zero or contains a $1$ as its first nonzero entry, and every column with one of those leading $1$s has $0$s for all its other entries.
	
###

In practice, matrices in reduced row echelon form look like the identity matrix, but some rows may be zero and there may be some extra columns inserted. For example,

$$
	[[ 1, 0, 5, 0, 0 ; 0, 1, -2, 0, 0 ; 0, 0, 0, 0, 1 ; 0, 0, 0, 0, 0 ]]
$$

is in reduced row echelon form. The idea here is that matrices in this form correspond to a solved system of equations: all of the rows with a leading $1$ have a variable isolated, and the extra columns are free parameters.

### exc "reduced row echelon form"

	Define a matrix $A$ by

	$$
		A = [[ 0, 1, 0, 1, 0 ; 0, 0, 1, 2, 2 ; 0, 0, 0, 0, 1 ]].
	$$

	Is $A$ in reduced row echelon form? Why or why not?

###

A matrix in reduced row echelon form is effectively as good as we can get --- once it's in that form, none of the elementary row operations can clear the columns that don't contain a leading $1$. Instead, we can cash out by converting from a matrix back to equations, and they're very simple to solve. Every one of those leading $1$s is an isolated variable, and the columns we couldn't clear are free parameters. With the goal in mind, let's look at how we get there.

### thm -m "Method: Row Reduction"
	
	To perform elementary row operations on a matrix $A$ to convert it to reduced row echelon form, first find a row with a nonzero first entry, swap it to the top of the matrix, and then add multiples of that row to all the ones below it to make their first terms zero.

	Now find a row with a nonzero second entry, swap it to the second row down from the top, and add multiples of it to all the rows below it to make their second entries zero. Repeat this with every row --- at the end, you'll have a matrix with a triangular region of zeros in the bottom-left and a strip of $1$s diagonally from the top-left to bottom-right. Now take the bottom row and add multiples of it to every row above it to zero out their entries, and repeat with all of the rows. Finally, divide through by the nonzero entries in each row to make them $1$.
	
###

This is a method best explained with examples, so let's dive right in.

### ex "row-reduction"
	
	Solve the system of equations
	
	$$
		4x - y + 3z &= -1
		
		x + 2y &= 20
		
		-6x + z &= -12.
	$$

	### solution
	
	To begin, we'll place this into an augmented matrix:
	
	$$
		[[ 4, -1, 3 | -1 ; 1, 2, 0 | 20 ; -6, 0, 1 | -12 ]].
	$$
	
	And now we'll get reducing! For ease of language, we'll call the rows $\vec{r_1}$, $\vec{r_2}$, and $\vec{r_3}$ --- as we move the rows around, what they refer to will change. Let's start with $\vec{r_2}$, since it already has a $1$ in the first position, and move it to the top by swapping it with $\vec{r_1}$:
	
	$$
		[[ 1, 2, 0 | 20 ; 4, -1, 3 | -1 ; -6, 0, 1 | -12 ]].
	$$
	
	Notice that the part of the row to the right of the bar comes along for the ride. Now we can add $-4\vec{r_1}$ to $\vec{r_2}$ and $6\vec{r_1}$ to $\vec{r_3}$ to remove their first entries.
	
	$$
		[[ 1, 2, 0 | 20 ; 0, -9, 3 | -81 ; 0, 12, 1 | 108 ]].
	$$
	
	We can clean up $\vec{r_2}$ by dividing it through by $-3$ to get
	
	$$
		[[ 1, 2, 0 | 20 ; 0, 3, -1 | 27 ; 0, 12, 1 | 108 ]].
	$$
	
	Now if we add $-4\vec{r_2}$ to $\vec{r_3}$, we can clear its second entry.
	
	$$
		[[ 1, 2, 0 | 20 ; 0, 3, -1 | 27 ; 0, 0, 5 | 0 ]].
	$$
	
	That's the first half of row-reduction done! For the second half, we start with $\vec{r_3}$. Let's first divide by $5$:
	
	$$
		[[ 1, 2, 0 | 20 ; 0, 3, -1 | 27 ; 0, 0, 1 | 0 ]].
	$$
	
	Now we can add $\vec{r_3}$ to $\vec{r_2}$ to clear its last entry. There's no need to do anything to $\vec{r_1}$, since its last entry is already zero.
	
	$$
		[[ 1, 2, 0 | 20 ; 0, 3, 0 | 27 ; 0, 0, 1 | 0 ]].
	$$
	
	Dividing $\vec{r_2}$ by $3$ gives
	
	$$
		[[ 1, 2, 0 | 20 ; 0, 1, 0 | 9 ; 0, 0, 1 | 0 ]],
	$$
	
	and then finally, we can add $-2\vec{r_2}$ to $\vec{r_1}$ to get
	
	$$
		[[ 1, 0, 0 | 2 ; 0, 1, 0 | 9 ; 0, 0, 1 | 0 ]],
	$$
	
	If we rewrite this as a matrix equation, we have
	
	$$
		[[ 1, 0, 0 ; 0, 1, 0 ; 0, 0, 1 ]][[ x ; y ; z ]] &= [[ 2 ; 9 ; 0 ]]
		
		[[ x ; y ; z ]] &= [[ 2 ; 9 ; 0 ]],
	$$
	
	so $x = 2$, $y = 9$, and $z = 0$.
	
###

To streamline the process while still explaining what we're doing at each step, it's helpful to have a notation for elementary row operations. These vary quite a bit, but I'm personally fond of the following:

$$
	\swap \vec{r_i}, \vec{r_j}
	
	\vec{r_i} \te c
	
	\vec{r_i} \pe c\vec{r_j}
$$

These are shorthand for swapping rows $i$ and $j$, multiplying row $i$ by a constant $c$, and adding $c$ times row $j$ to row $i$, respectively. Let's work through another example with this notation.

### ex "row-reduction"
	
	Solve the system of equations
	
	$$
		x_1 + 4x_2 - 14x_4 &= -11
		
		2x_1 + 5x_2 + x_3 + 14x_4 &= -15
		
		x_2 - 5x_3 + 28x_4 &= 7.
	$$

	### solution
	
	Once this is in a matrix, we're off to the races:
	
	$$
		[[ 1, 4, 0, -14 | -11 ; 2, 5, 1, 14 | -15 ; 0, 1, -5, 28 | 7 ]] & 
		
		[[ 1, 4, 0, -14 | -11 ; 0, -3, 1, 42 | 7 ; 0, 1, -5, 28 | 7 ]] & \qquad \vec{r_2} \me 2\vec{r_1}
		
		[[ 1, 4, 0, -14 | -11 ; 0, 1, -5, 28 | 7 ; 0, -3, 1, 42 | 7 ]] & \qquad \swap \vec{r_2}, \vec{r_3}
		
		[[ 1, 4, 0, -14 | -11 ; 0, 1, -5, 28 | 7 ; 0, 0, -14, 126 | 28 ]] & \qquad \vec{r_3} \pe 3\vec{r_2}
		
		[[ 1, 4, 0, -14 | -11 ; 0, 1, -5, 28 | 7 ; 0, 0, 1, -9 | -2 ]] & \qquad \vec{r_3} \te -\frac{1}{14}
		
		[[ 1, 4, 0, -14 | -11 ; 0, 1, 0, -17 | -3 ; 0, 0, 1, -9 | -2 ]] & \qquad \vec{r_2} \pe 5\vec{r_3}
		
		[[ 1, 0, 0, 54 | 1 ; 0, 1, 0, -17 | -3 ; 0, 0, 1, -9 | -2 ]] & \qquad \vec{r_1} \me 4\vec{r_2}
	$$
	
	The matrix is now fully reduced, even though there are still a bunch of ugly-looking numbers in it. What's going on with that? Every row of a matrix represents an equation and every column represents a different variable, so if a matrix is square, the number of equations and the number of unknowns are equal, meaning that we'd expect to be able to solve for all of them. If the matrix has more rows than columns --- it's a tall rectangle --- then the balance is tipped to having more equations than unknowns, meaning we'd expect there to be *no* solution unless things align just right. We'll see examples of this later, but it typically looks like one of the rows having all zeros on the left and a nonzero value on the right, which is impossible.
	
	On the other hand, a matrix like this with more columns than rows means there are extra unknowns, and in general, we can only solve for as many unknowns as there are equations. A great side effect of matrix reduction is that it's made the extra step of writing the general solution in terms of a parameter a whole lot easier. If we set $t = x_4$, then our equations are
	
	$$
		x_1 + 54t &= 1
		
		x_2 - 17t &= -3
		
		x_3 - 9t &= -2,
	$$
	
	so the general solution is
	
	$$
		x_1 &= 1 - 54t
		
		x_2 &= -3 + 17t
		
		x_3 &= -2 + 9t
	$$
	
	for any value of $t$.
	
###

### exc "row-reduction"
	
	Solve the system
	
	$$
		x_1 + x_2 - 4x_3 &= 6
		
		x_1 + 5x_2 &= 2
		
		-2x_1 + 2x_2 + 12x_3 & = -16.
	$$
	
###



## Inverse Matrices

With the tool of row-reduction, we're ready to talk about inverse matrices. First of all, if there's going to be a matrix $A^{-1}$ so that the products $AA^{-1} = I$ and $A^{-1}A = I$ both make sense, then we need $A$ to have the same number of rows and columns. So let's start with a square matrix $A$ and try to invert it. The magic here comes from augmenting $A$ not with a single column vector, but with an entire identity matrix.

### thm "matrix inversion"
	
	Let $A$ be an $n \times n$ matrix and augment it with the identity matrix: $[[ A | I ]]$. Then row-reduce $A$. If it can be reduced to the identity matrix, then the result will be $[[ I | A^{-1} ]]$ --- in other words, applying the same elementary row operations to $I$ will transform it into $A^{-1}$. If $A$ cannot be row-reduced to $I$ (because at least one row becomes entirely zero), then $A$ cannot be inverted.
	
###

### pf
	
	It's worth saying a bit about why this process works. All three types of elementary row operations can actually be expressed as matrix multiplication. If we define $S_{i, j}$ as the $n \times n$ matrix that is $I$ but with rows $i$ and $j$ swapped, then it's not too hard to verify that for any $n \times n$ matrix $A$, $S_{i, j}A$ is equal to $A$, but with rows $i$ and $j$ swapped. Similarly, there are matrices for multiplying a row by a constant and adding a multiple of one row to another. So if $A$ is row-reducible to $I$, then there's a long sequence of matrices we can multiply $A$ by in order to get to $I$ --- collecting them all up, that's exactly what it means to be $A^{-1}$. On the other side of the augmented matrix, we're multiplying by all the same matrices, but now we *start* with $I$, so what results is just $A^{-1}$.
	
###

### ex "matrix inversion"
	
	Consider the system
	
	$$
		x_1 + 2x_2 - 3x_3 &= -6
		
		-2x_1 + 2x_3 &= -4
		
		x_1 + x_2 + 4x_3 &= 4.
	$$
	
	Express it as a matrix equation, then solve it by inverting the matrix.

	### solution
	
	The matrix equation here is
	
	$$
		[[ 1, 2, -3 ; -2, 0, 2 ; 1, 1, 4 ]] \vec{x} = [[ -6 ; -4 ; 4 ]].
	$$
	
	Calling the matrix $A$ and the right-hand vector $\vec{b}$, we have $A\vec{x} = \vec{b}$. While we could solve this by row-reducing $[[ A | \vec{b} ]]$, we'll instead invert $A$ and then multiply both sides by it.
	
	$$
		[[ 1, 2, -3 | 1, 0, 0 ; -2, 0, 2 | 0, 1, 0 ; 1, 1, 4 | 0, 0, 1 ]] &
		
		[[ 1, 2, -3 | 1, 0, 0 ; 0, 4, -4 | 2, 1, 0 ; 1, 1, 4 | 0, 0, 1 ]] & \qquad \vec{r_2} \pe 2\vec{r_1}
		
		[[ 1, 2, -3 | 1, 0, 0 ; 0, 4, -4 | 2, 1, 0 ; 0, -1, 7 | -1, 0, 1 ]] & \qquad \vec{r_3} \me \vec{r_1}
		
		[[ 1, 2, -3 | 1, 0, 0 ; 0, -1, 7 | -1, 0, 1 ; 0, 4, -4 | 2, 1, 0 ]] & \qquad \swap \vec{r_2}, \vec{r_3}
		
		[[ 1, 2, -3 | 1, 0, 0 ; 0, 1, -7 | 1, 0, -1 ; 0, 4, -4 | 2, 1, 0 ]] & \qquad \vec{r_2} \te -1
		
		[[ 1, 2, -3 | 1, 0, 0 ; 0, 1, -7 | 1, 0, -1 ; 0, 0, 24 | -2, 1, 4 ]] & \qquad \vec{r_3} \me 4\vec{r_2}
	$$
	
	At this point, we could divide $\vec{r_3}$ through by $24$, but then we're going to be swamped in fractions. We have to get a $1$ where that $24$ is --- but only eventually. For now, we can make the process a whole lot easier by multiplying every *other* row by $24$:
	
	$$
		[[ 24, 48, -72 | 24, 0, 0 ; 0, 24, -168 | 24, 0, -24 ; 0, 0, 24 | -2, 1, 4 ]] & \qquad \begin{array}{l} \vec{r_1} \te 24 \\ \vec{r_2} \te 24 \end{array}
		
		[[ 24, 48, -72 | 24, 0, 0 ; 0, 24, 0 | 10, 7, 4 ; 0, 0, 24 | -2, 1, 4 ]] & \qquad \vec{r_2} \pe 7\vec{r_3}
		
		[[ 24, 48, 0 | 18, 3, 12 ; 0, 24, 0 | 10, 7, 4 ; 0, 0, 24 | -2, 1, 4 ]] & \qquad \vec{r_1} \pe 3\vec{r_3}
		
		[[ 24, 0, 0 | -2, -11, 4 ; 0, 24, 0 | 10, 7, 4 ; 0, 0, 24 | -2, 1, 4 ]] & \qquad \vec{r_1} \me 2\vec{r_2}
	$$
	
	We're almost done! If we divide through every row by $24$, then the right side will contain $A^{-1}$, so we can write it as the right matrix times $\frac{1}{24}$.
	
	$$
		A^{-1} = \frac{1}{24} [[ -2, -11, 4 ; 10, 7, 4 ; -2, 1, 4 ]]
	$$
	
	so we can use it to solve the original equation --- and notably, it can solve *any* variant of the equation where we change $\vec{b}$. Specifically, we have
	
	$$
		A\vec{x} &= \vec{b}
		
		A^{-1} A\vec{x} &= A^{-1} \vec{b}
		
		I\vec{x} &= \frac{1}{24} [[ -2, -11, 4 ; 10, 7, 4 ; -2, 1, 4 ]] [[ -6 ; -4 ; 4 ]]
		
		\vec{x} &= \frac{1}{24} [[ 72 ; -72 ; 24 ]]
		
		\vec{x} &= [[ 3 ; -3 ; 1 ]].
	$$
	
	Therefore, $x_1 = 3$, $x_2 = -3$, and $x_3 = 1$.
	
###

### exc "matrix inversion"
	
	Given the system
	
	$$
		x + 2y &= a
		
		-2x + 3y &= b,
	$$
	
	represent it as a matrix equation and then find the inverse matrix in order to express $x$ and $y$ in terms of $a$ and $b$.
	
###



### nav-buttons