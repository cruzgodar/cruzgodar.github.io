### nav-buttons

It's hard to overstate just how pervasive linear algebra is in science. It underpins nearly every math course that comes after it in one way or another, and it's so important in other STEM fields that when they talk about math, they often just mean linear algebra. With such an impressive resume, it might be surprising to hear that the subject is no more than a formalized, organized way to solve systems of linear equations --- at least, that's how it gets started. As we'll quickly see, formalizing that process will result in an unreasonably useful set of tools and definitions with applications far beyond what it was designed for.

This course is a clean break not only from multivariable calculus, but also from prerequisite courses: there isn't much to review that will be particularly useful. Instead, we'll be building our theory of linear algebra mostly from scratch --- to that end, let's take a look at our central object of study: systems of linear equations. *Linear* means that variables aren't multiplied or raised to powers, just added together, so when there are only two variables, the graph is a line.

### ex "a system of linear equations"
	
	Solve the system
	
	$$
		x_1 + 3x_2 &= -5
		
		2x_1 - 3x_2 &= 8.
	$$

	### solution
	
	We could use substitution by solving the first equation for $x_1$ and substituting it into the second, but let's think critically about that. Substitution doesn't scale well --- if we had $n$ equations and $n$ unknowns, solving by substitution means isolating and plugging in $n - 1$ equations until we finally reach the end, and then we have to do a large amount of back-substitution. That's not to mention the complexity that arises when not every equation contains every variable, and we have to carefully pick and choose our order. We'd like more of a general solving method here, and so we owe it to ourselves to build a better foundation.
	
	It may have been a while since you learned about solving systems of linear equations, but you probably learned another solving method, different from substitution. It's usually called the method of **addition**, and critically for us, it's much more scalable. We begin by looking at the top-left term in the system (i.e. $x_1$) and adding copies of the entire first equation down to the others to clear their $x_1$ terms. Here, that means adding $-2$ times the first equation down to the second, resulting in
	
	$$
		x_1 + 3x_2 &= -5
		
		- 9x_2 &= 18.
	$$
	
	At this point, we can divide the second equation through by $-9$ to find that $x_2 = -2$:
	
	$$
		x_1 + 3x_2 &= -5
		
		x_2 &= -2.
	$$

	Instead of substituting that into the first equation, we'll add $-3$ times the entire second equation up to the first.
	
	$$
		x_1 &= 1
		
		x_2 &= -2.
	$$
	
	So --- why does this supposedly scale better? The key is in the organization. The variables stayed on the left and the constants on the right, and we only had to look at two coefficients at a time to figure out how many of one equation to add to another. As the number of variables and equations increases, the number of operations required to do both substitution and addition increases too, but the *complexity* of those operations doesn't increase with addition.
	
###

### exc "a system of linear equations"
	
	Solve the system
	
	$$
		2x_1 - x_2 &= -6
		
		-4x_1 - 3x_2 &= 12.
	$$

	### solution

	Adding $2$ times the first equation to the second,

	$$
		2x_1 - x_2 &= -6
		
		- x_2 &= 0.
	$$

	It's a little silly, but adding $-1$ times the second equation to the first,

	$$
		2x_1 &= -6
		
		- x_2 &= 0.
	$$

	Therefore, $x_1 = -3$ and $x_2 = 0$.
	
###

Now that we've worked through an example, we're prepared to define linear systems in general.

### def "linear system"

	A general linear equation in the variables $x_1, x_2, ..., x_n$ has the form
	
	$$
		a_1x_1 + a_2x_2 + \cdots + a_nx_n = b
	$$
	
	for some constants $a_i$ and $b$. A **linear system** is a collection of linear equations. The coefficients usually pick up an extra subscript --- below, $a_{ij}$ is the coefficient in row $i$ and column $j$.
	
	$$
		a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1
		
		a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2
		
		& \ \ \vdots
		
		a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m.
	$$
	
###

Much of the intuition we have for small linear systems helps us understand the behavior of large ones. Depending on the particular equations, a system can have a single solution, no solutions, or infinitely many. Geometrically, linear equations form lines in $#R#^2$, planes in $#R#^3$, and so on, and the points where they all intersect are the solutions. In the example below, setting $a = 1$ results in the two lines being parallel, so there are no intersection points and therefore no solutions. When $a = 0$, the lines are the same (we sometimes say they **coincide**), and so there are infinitely many solutions. For all other values of $a$, the lines intersect at exactly one point, meaning there is a unique solution.

### desmos linearSystem

We'll develop vocabulary to describe all of these situations shortly, but the most pressing issue is that it's unwieldy to deal with $m$ different equations at once. If we could consolidate them into a single equation that still contained all the data, it would make the discussion of linear systems quite a bit simpler.

Let's start with the right sides --- the $b_i$. There are quite a few ways we could try to combine them all into one single object, but let's just follow our intuition and try the simplest thing we can. When we write the equations in a row like we did above, the $b_i$ form a column. To that end, we'd like an object that contains all of the $b_i$ at once. That's just a vector in $#R#^m$, but in keeping with the notion of the $b_i$ being stacked vertically, we're going to modify our vector notation a little bit to support that configuration.

### def "column vector"

	A **column vector** $\vec{b}$ is a vertical rectangle of numbers of the form

	$$
		[[ b_1 ; b_2 ; \vdots ; b_m ]].
	$$

	We use an arrow over the variable name to indicate that $\vec{b}$ is a vector, and we also say that $\vec{b}$ has length $m$, or that its dimensions are $m \times 1$ (since it has $m$ rows and just one column).

###

For all intents and purposes, column vectors are identical to ordinary vectors, just written in a different way. We can leverage them here by assembling all of those linear equations into vectors. Going back to our original system,

$$
	x_1 + 3x_2 &= -5
	
	2x_1 - 3x_2 &= 8
$$

becomes

$$
	[[ x_1 + 3x_2 ; 2x_1 - 3x_2 ]] = [[ -5 ; 8 ]].
$$

This is a good step, but it's not particularly useful yet: all we've done is draw a few extra brackets and not as many equals signs. To really cut through the noise and find a good generalization, we need to go one step further. In a linear system with two unknowns, there will always be a single copy of $x_1$ and $x_2$ in each equation. What changes is the coefficients --- the only part of the left side that matters are the four numbers $1$, $3$, $2$, and $-3$. As before, let's try the simplest possible organization. When we write out the equations, they form a square, so let's use vector notation and put them into a $2 \times 2$ rectangle:

$$
	[[ 1, 3 ; 2, -3 ]].
$$

This object is called a $2 \times 2$ **matrix**. All we need is a way to use this matrix *as a coefficient* on both $x_1$ and $x_2$ at the same time --- that combination sounds like a vector, which means we want

$$
	[[ 1, 3 ; 2, -3 ]] [[ x_1 ; x_2 ]] = [[ x_1 + 3x_2 ; 2x_1 - 3x_2 ]].
$$

With this example guiding us, let's properly define matrices and how they multiply with vectors.

### def "matrix"

	An $m \times n$ **matrix** $A$ is a rectangle of numbers with $m$ rows and $n$ columns:

	$$
		A = [[ a_{11}, a_{12}, \cdots, a_{1n} ; a_{21}, a_{22}, \cdots, a_{2n} ; \vdots, \vdots, \ddots, \vdots ; a_{m1}, a_{m2}, \cdots, a_{mn} ]].
	$$

	We typically use capital letters for matrices and the corresponding lowercase letter for the entries. We'll also occasionally write $A = [a_{ij}]$ to save space.

	Given an $m \times n$ matrix $A$ and an $n \times 1$ vector $\vec{x}$, we can define the product $A\vec{x}$: it's an $m \times 1$ vector, given by

	$$
		[[ a_{11}, a_{12}, \cdots, a_{1n} ; a_{21}, a_{22}, \cdots, a_{2n} ; \vdots, \vdots, \ddots, \vdots ; a_{m1}, a_{m2}, \cdots, a_{mn} ]] [[ x_1 ; x_2 ; \vdots ; x_n ]] = [[ a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n ; a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n ; \vdots ; a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n ]]
	$$

	The easiest way to think about this is that we take the $i$th row of $A$ and all of $\vec{x}$, and multiply each pair of entries together. Since the number of columns of $A$ and the number of entries of $\vec{x}$ are the same, there are no entries left over. Then we take all the products, add them up, and write that in the $i$th entry of our new vector. When we've done that with every row of $A$, that new vector is $A\vec{x}$.

###

Although this is an intense definition, it's thankfully among the most complicated we'll have to wrestle with in this course. Let's work through some examples to get our bearing.

### ex "matrix-vector multiplication"
	
	Evaluate the following products:
	
	1. $$[[ 1, -1 ; 2, 3 ]] [[ 3 ; -2 ]].$$
	
	2. $$[[ 5, 0, 2 ; 2, 3, 0 ]] [[ 1 ; 0 ; -2 ]].$$
	
	3. $$[[ 1, 0, 0 ; 0, 1, 0 ; 0, 0, 1 ]] [[ 5 ; -4 ; 1 ]].$$
	
	### solution
	
	1. Multiplying each row of the matrix with the vector, we have
	
	$$
		[[ 1(3) + -1(-2) ; 2(3) + 3(-2) ]] = [[ 5 ; 0 ]].
	$$
	
	2. Although the matrix isn't square, the definition still works fine:
	
	$$
		[[ 5(1) + 0(0) + 2(-2) ; 2(1) + 3(0) + 0(-2) ]] = [[ 1 ; 2 ]].
	$$
	
	3. This one works exactly the same way! The resulting vector is
	
	$$
		[[ 1(5) + 0(-4) + 0(1) ; 0(5) + 1(-4) + 0(1) ; 0(5) + 0(-4) + 1(1) ]] = [[ 5 ; -4 ; 1 ]].
	$$
	
###

### exc "matrix-vector multiplication"
	
	Evaluate the following products:
	
	1. $$[[ 1, 0 ; 0, 1 ]] [[ 5 ; 7 ]].$$
	
	2. $$[[ 1, 1 ; 2, 1 ; -1, 1 ]] [[ -2 ; 5 ]].$$
	
	3. $$[[ 1, 1 ; 2, 1 ; -1, 1 ]] [[ -3 ; 6 ; 1 ]].$$

	### solution

	1. We have

	$$
		[[ 1, 0 ; 0, 1 ]] [[ 5 ; 7 ]] &= [[ 1(5) + 0(7) ; 0(5) + 1(7) ]]

		&= [[ 5 ; 7 ]].
	$$

	More generally, this matrix leaves any vector it multiplies alone.

	2.

	$$
		[[ 1, 1 ; 2, 1 ; -1, 1 ]] [[ -2 ; 5 ]] &= [[ 1(-2) + 1(5) ; 2(-2) + 1(5) ; -1(-2) + 1(5) ]]

		&= [[ 3 ; 1 ; 7 ]].
	$$

	3. This one's undefined! We're trying to multiply a $3 \times 2$ matrix with a $3 \times 1$ vector, and we would need the number of columns of the matrix (i.e. $2$) to equal the number of rows of the vector (i.e. $3$).
	
###

From these examples, we can see some general properties of matrix-vector multiplication. If $A$ is an $m \times n$ matrix, meaning it has $m$ rows and $n$ columns, then $A\vec{x}$ is defined only if $\vec{x}$ has length $n$ --- in other words, if $\vec{x}$ is $n \times 1$ --- and if so, then the length of $A\vec{x}$ is $m$.

A square matrix with all $0$s except for a strip of $1$s down the diagonal from top-left to bottom-right, like those in part 3 of the example or part 1 of the exercise, seems to multiply vectors by not changing them, and it's not too hard to see that the same will happen with every vector: the $i$th row pulls out exactly the $i$th component. This matrix is useful enough that it deserves its own name.

### def "identity matrix"
	
	The $n \times n$ **identity matrix** is a square matrix consisting of all $0$s, except for $1$s down the top-left to bottom-right diagonal (called the **main diagonal**). We write it as $I_n$, or just $I$ if the dimension is clear from context.
	
###



## Matrices as Functions

Now that we're used to the basic idea of matrices, let's look at some of their basic properties. The guiding principle to keep in mind is that **we can think of matrices as functions** that operate by multiplying. Every matrix $A$ corresponds to a function $T$ so that $A\vec{x} = T(\vec{x})$. For example, the matrix

$$
	[[ 1, 3 ; 2, -3 ]]
$$

operates by taking in the vector $$[[ x_1, x_2 ]]$$ and sending it to

$$
	[[ x_1 + 3x_2 ; 2x_1 - 3x_2 ]].
$$

To this end, a good way to define matrix addition is so that it acts just like function addition --- matrices $A$ and $B$ should have their sum defined so that

$$
	(A + B)\vec{x} = A\vec{x} + B\vec{x},
$$

just like functions. Therefore, we need $A$ and $B$ to have the same number of columns, so they can both multiply $\vec{x}$, and the same number of rows, so that they can add together after multiplying. For vector multiplication to distribute like this, we just need the $i$th row of $A + B$ to be the $i$th row of $A$ plus the $i$th row of $B$. All of this is to say that matrix addition works exactly like we'd hope: as long as they're exactly the same shape, we can just add each component together. For example,

$$
	[[ 5, 0, 2 ; 2, 3, 0 ]] + [[ -1, 2, 0 ; -5, 0, 2 ]] &= [[ 5 - 1, 0 + 2, 2 + 0 ; 2 - 5, 3 + 0, 0 + 2 ]]
	
	&= [[ 5, 2, 2 ; -3, 3, 2 ]].
$$

For exactly the same reason, scalar multiplication works the same way: for any number $c$, the matrix $cA$ is created by multiplying every entry by $c$. For example,

$$
	-4[[ 5, 0, 2 ; 2, 3, 0 ]] = [[ -20, 0, -8 ; -8, -12, 0 ]].
$$

Now, let's talk about matrix multiplication. This one topic has a hefty amount of explanation behind it --- far, far too often, matrix multiplication is introduced as a quirky and complicated operation and then never justified or dug into. It'll take us a minute to go through the details, but we're going to make sure we understand why matrices multiply the way they do.

An $n \times k$ matrix $B$ takes in a $k$-dimensional vector $\vec{x}$ and gives back an $n$-dimensional one by doing matrix-vector multiplication, which we write as $B\vec{x}$. If we have another matrix $A$ and we want the product $AB$ to make any sense at all, then when we multiply by $\vec{x}$, the following equality should hold:

$$
	(AB)\vec{x} = A(B\vec{x}).
$$

On the left is a matrix called $AB$ that we don't know how to calculate yet, but on the right, there's just matrix-vector multiplication happening twice. Since we're thinking of matrices as being functions that act by multiplication, the right side is function composition, so, crucially, the left side should be too. In other words, **matrix multiplication should be defined to be composition of functions**. This point is glossed over or ignored in many introductory classes, and that's really a shame! Understanding why matrices work the way they do is half the battle in using them properly.

So --- how do we do that? First of all, not just any two matrices can be multiplied together. Since an $n \times k$ matrix $B$ takes in a $k$-dimensional vector $\vec{x}$ and gives back an $n$-dimensional one $B\vec{x}$, a matrix $A$ that makes the product $AB$ make sense needs to take in $n$-dimensional vectors, so $A$ needs to be an $m \times n$ matrix for some $m$. Finally, since $ABx = A(B\vec{x})$, the matrix $AB$ should be $m \times k$.

To figure out the exact formula for matrix multiplication, let's look at a few vectors in particular. Let $\vec{e_j}$ be a $k$-dimensional column vector defined by

$$
	\vec{e_j} = [[ 0 ; \vdots ; 0 ; 1 ; 0 ; \vdots ; 0 ]],
$$

where the $1$ is in position $j$. When we evaluate $B\vec{e_j}$, only column $j$ of $B$ is picked out --- in other words, if we write

$$
	B = [[ \mid, \mid,, \mid, \mid ; \vec{b_1}, \vec{b_2}, \cdots, \vec{b_{k - 1}}, \vec{b_k} ; \mid, \mid,, \mid, \mid ]],
$$

so that $\vec{b_j}$ is the $j$th column of $B$, then $B\vec{e_j} = \vec{b_j}$. By the same token, $(AB)\vec{e_j} = A\vec{b_j}$ is the $j$th column of $AB$. So the matrix product $AB$ is defined by

$$
	AB = [[ \mid, \mid,, \mid, \mid ; A\vec{b_1}, A\vec{b_2}, \cdots, A\vec{b_{k - 1}}, A\vec{b_k} ; \mid, \mid,, \mid, \mid ]].
$$

Thankfully, we can streamline this formula to make it easier to remember. To get the $i$th entry of $A\vec{b_j}$, we turn $\vec{b_j}$ sideways and look at its product with the $i$th row of $A$, and so we can get a formula for $AB$ one entry at a time.

### thm "matrix multiplication"
	
	Let $A$ be an $m \times n$ matrix and let $B$ be an $n \times k$ one. Then $AB$ is an $m \times k$ matrix, and the entry in row $i$ and column $j$ is defined by
	
	$$
		AB_{ij} = \sum_{l = 1}^n a_{il}b_{lj}.
	$$
	
	In other words, we take row $i$ from $A$ and column $j$ from $B$, multiply the corresponding entries, and add all $n$ together. When thinking of a matrix as a function operating on vectors, matrix multiplication *is* function composition. In other words,
	
	$$
		(AB)\vec{x} = A(B\vec{x}).
	$$
	
###

To summarize the result about shapes and products, if we see a product $AB$, we can write the dimensions $m \times n$ and $k \times l$ of the two matrices side-by side. Then the product is defined only if $n = k$ (i.e. the two middle numbers are equal), and if so, then the shape of the product $AB$ is $m \times l$ (the two outer numbers).

That was a *lot* of work! Let's turn to some examples to see this in action.

### ex "matrix multiplication"
	
	Let $$A = [[ 5, 0 ; -2, 1 ]]$$ and $$B = [[ 3, 1, -1 ; -1, 0, 2 ]]$$. Evaluate $AB$.

	### solution
	
	This is a valid product to take, since the number of columns $A$ and the number of rows of $B$ are the same (i.e. $2$). And the dimensions of $AB$ will be $2 \times 3$, since $A$ has $2$ rows and $B$ has $3$ columns. To get the element $AB_{11}$ --- the top-left entry --- we take the first row of $A$, which is $(5, 0)$, and the first column of $B$, which is $(3, -1)$, and multiply them together to get $5(3) + 0(-1) = 15$. Repeating this for the other five entries, we get
	
	$$
		AB = [[ 15, 5, -5 ; -7, -2, 4 ]].
	$$
	
###

### exc "matrix multiplication"
	
	1. Let $\vec{x} = [[ 1 ; 2 ; -1 ]]$. With the same matrices from the previous example, evaluate $(AB)\vec{x}$ and $A(B\vec{x})$ and show they're the same.

	2. Evaluate $$[[ 1, 2, 3 ]][[ -1, 0, 4 ; 2, 5, -1 ; 0, 0, 1 ]]$$.

	3. Find the product $$[[ -1, 0, 4 ; 2, 5, -1 ; 0, 0, 1 ]][[ 1, 2, 3 ]]$$.

	4. Let $$A = [[ 1, 2 ; 3, 4 ]]$$ and let $$B = [[ 0, 2 ; -1, 0 ]]$$. Are $AB$ and $BA$ the same? Explain why or why not in the context of function composition.

	### solution

	1. We already know $AB$, so we can compute

	$$
		(AB)\vec{x} &= [[ 15, 5, -5 ; -7, -2, 4 ]][[ 1 ; 2 ; -1 ]]

		&= [[ 30 ; -15 ]].
	$$

	On the other hand,

	$$
		B\vec{x} &= [[ 3, 1, -1 ; -1, 0, 2 ]][[ 1 ; 2 ; -1 ]]

		&= [[ 6 ; -3 ]]

		A(B\vec{x}) &= [[ 5, 0 ; -2, 1 ]] [[ 6 ; -3 ]]

		&= [[ 30 ; -15 ]].
	$$

	2. We take rows of the first factor and columns of the second, so the result is a $1 \times 3$ matrix (also called a **row vector**):

	$$
		[[ 1, 2, 3 ]][[ -1, 0, 4 ; 2, 5, -1 ; 0, 0, 1 ]] = [[ 3, 10, 5 ]].
	$$

	3. This way around is undefined! The dimensions are $3 \times 3$ and $1 \times 3$, and the middle dimensions don't line up.

	4. We have

	$$
		AB &= [[ -2, 2 ; -4, 6 ]]

		BA &= [[ 6, 8 ; -1, -2 ]],
	$$

	so they're very much different. This is what we'd expect from functions, too: order matters in composition!
	
###



There are two valuable lessons to take from this section that are much wider-reaching than linear systems: first, consolidating and abstracting a concept often leads to a much more powerful theory in the end, even if it's often more difficult to work with at first. Second, it's almost always worth trying the simplest idea first in math --- often, everything will fall in place as if it was meant to be, and it will feel as though you're discovering the theory instead of inventing it. It may not feel quite like that so far, but in the very next section, we'll begin reaping the benefits of matrices to efficiently tackle linear systems.



### nav-buttons