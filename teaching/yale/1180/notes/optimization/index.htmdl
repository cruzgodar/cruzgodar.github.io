### show-solutions

### nav-buttons



At long last, it's time to talk about optimization. This was our main application of derivatives in Calculus I, and we can boil it down to a few sentences: to find the maxima and minima of a function $f(x)$ on an interval, we take its derivative $f'(x)$, find every point $x = c$ where $f'(x) = 0$ or $f'(x)$ is undefined (called the **critical points**), which are places where there *might* be an extremum. To classify them, we can use one of two methods.

> - The **first derivative test** has us evaluate $f'(x)$ just to the left and right of $x = c$ --- if it's negative to the left and positive to the right, then there is a local minimum at $x = c$, and if the opposite is true, there is a local maximum. If the derivative has the same sign to both the left and right, then this isn't an extremum at all --- it's a saddle point, like with $y = x^3$ at $x = 0$.

> - The **second derivative test** works by evaluating $f''(x)$ *at* $x = c$, which measures the concavity of $f$. If $f''(c) > 0$, that means the function is concave up at $x = c$, and so $x = c$ is a local minimum, and similarly, if $f''(c) < 0$, then $x = c$ is a local maximum. If $f''(c) = 0$, then the test is inconclusive --- it might be a saddle point, like $f(x) = x^3$ at $x = 0$, or it might be an extremum, like $f(x) = x^4$ at $x = 0$.

With this brief refresher out of the way, let's dig into how this all extends to functions of multiple variables! While our discussion of critical points will work for functions of any number of variables, our later work will focus specifically on functions of two variables --- the theory extends out beyond that, but unfortunately also beyond our class's scope at the same time. I'm happy to talk more about that outside of class at some point, particularly after we've discussed some linear algebra in the second half of the course!



## Critical Points

We'll begin with critical points, which generalize nicely to more variables --- rather than a flat tangent line, we'll effectively be looking for points with a flat tangent plane.

### def "critical point"

	Let $f : #R#^n \to #R#$ be a function of $n$ variables. A **critical point** of $f$ is a point $p \in #R#^n$ with $\G f(p) = \vec{0}$ or where $\G f(p)$ is undefined.

###

This is a notationally dense but efficient way to say that critical points are where *all* of the partial derivatives are zero at once. We do this so that the tangent plane (for a function of two variables) is flat at $(x, y) = p$, which is the generalization of critical point that we need.

### exc "critical points"

	Find the critical points of the following functions.

	1. $f(x, y) = x^3 + y^2 - 2x^2 + xy$.

	2. $g(x, y) = \sqrt{x^2 + y^2}$.

	3. $h(x, y, z) = xyz$.

	### solution

	1. Let's start by finding the gradient. We have

	$$
		\G f = \left< 3x^2 - 4x + y, 2y + x \right>,
	$$

	which is zero when both

	$$
		3x^2 - 4x + y &= 0

		2y + x &= 0.
	$$

	We can solve for $ in the first equation and plug it into the second to get

	$$
		-6x^2 + 8x + x &= 0

		x(9 - 6x) &= 0

		x &= 0, \quad x = \frac{3}{2}.
	$$

	Plugging those back in,

	$$
		y &= 0, \quad y = -\frac{3}{4},
	$$

	and so our two critical points are $(0, 0)$ and $\left( \frac{3}{2}, -\frac{3}{4} \right)$.

	2. This time, the gradient is

	$$
		\G g = \left< \frac{1}{2} \left( x^2 + y^2 \right)^{-1/2}(2x), \frac{1}{2} \left( x^2 + y^2 \right)^{-1/2}(2y) \right>,
	$$

	which could only possibly be zero when $(x, y) = (0, 0)$, but it's undefined there anyways. That's our single critical point!

	3. This time, the gradient is a 3-dimensional vector:

	$$
		\G h = \left< yz, xz, xy \right>.
	$$

	This is a little more complicated to think through: we need all of those to be zero, but each component only requires one of its factors to be zero. Therefore, we can get away with having $x = y = 0$ but not $z$, or any other combination of two variables. Every point on the axes is therefore a critical point!

###



With critical points extended to $#R#^n$, the next missing piece is extrema. These work identically to functions of a single variable! For clarity, we'll state the definition in terms of a function of two variables, but it works just as well for any function $f : #R#^n \to #R#$.

### def "extrema"

	Let $f(x, y)$ be a function of two variables that is defined and continuous on an open disk near $(a, b)$ (i.e. nearby the point $(a, b)$). We say $f$ has a **local maximum** at $(a, b)$ if $f(x, y) \leq f(a, b)$ for all $(x, y)$ near $(a, b)$. Similarly, $f$ has a **local minimum** at $(a, b)$ if $f(x, y) \geq f(a, b)$ for all $(x, y)$ near $(a, b)$. We collectively call these **local extrema**.

	We say that $f$ has a **global** or **absolute maximum** at $(a, b)$ if $f(x, y) \leq f(a, b)$ for all $(x, y)$ in the domain of $f$, and similarly, $f$ has a **global** or **absolute minimum** at $(a, b)$ if $f(x, y) \geq f(a, b)$ for all $(x, y)$ in the domain of $f$. We collectively call these **global** or **absolute extrema**.

###

Perhaps unsurprisingly, critical points are potential locations for extrema, just like in single-variable calculus!

### thm "Fermat's Theorem"

	Let $f : #R#^n \to #R#$ be a function of $n$ variables. Then all extrema of $f$ occur at critical points.

###

The converse of this theorem (i.e. reversing the hypothesis and conclusion) isn't true! Just like with functions of a single variable, not all extrema occur at critical points: for example, $f(x, y) = x^2 - y^2$ has a critical point at $(0, 0)$ that is neither a local maximum nor a minimum.

### desmos saddlePoint

Critical points like this one are interesting to focus in on. Since the point isn't a local max or min, we know there must be points nearby that are both above and below it, just like $y = x^3$. We use the same term --- **saddle point** --- to describe critical points like this, and the name is even more apt with how much the shape of graphs like these look like saddles.

If we know that $(a, b)$ is a critical point of $f$, how do we determine if it's a local maximum or minimum? Of the two derivatives we mentioned for functions of a single variable, the first derivative test is unfortunately completely broken for functions of multiple variables. It relied on there being only two directions to move from a point at $x = a$: either to the right or left. For a two-variable function, there's now an entire circle of directions we could move in from a critical point $(a, b)$, and so we'd need to verify that the gradient $\G f$ points in a direction more toward $(a, b)$ than away from it for every point in a circle very close to $(a, b)$. It's certainly *possible*, but from that description, it's pretty clear the juice just isn't worth the squeeze. So with that test out, let's turn to the remaining one.



## The Second Derivative Test

Immediately upon considering how the second derivative test might generalize to a function $f(x, y)$, we're met with other problems: we have *four* second-order partial derivatives of $f$ and no complete second-derivative replacement like we did with the gradient. As it turns out, though, we can solve both problems at once. As a disclaimer, this portion of the section is *exclusively* for functions of two variables, as will probably be clear before long. Like we mentioned, everything we discuss is generalizable to more variables, but we have neither the tools nor time in this class. I'm more than happy to discuss the broad strokes outside of class, though!

Let's now focus in on a function $f(x, y)$ with a critical point at $(a, b)$ and see what we can do. Most discussions of this topic are either completely lacking motivation or extremely technical, but I hope to provide a much more intuitive and mathematically whole explanation. The credit for the broad strokes goes to <a href="https://m.youtube.com/watch?v=Q5Q9oswM2wo">Linda Green</a> --- I'm merely presenting that story here in a more bottom-up manner.


The saddle point of $f(x, y) = x^2 - y^2$ was effectively due to the graph having different concavity in the $x$- and $y$-directions. It'd be nice to hope we could just check *both* the signs of $f_{xx}(a, b)$ and $f_{yy}(a, b)$ to produce an analogue to the second derivative test, but the reality is more complicated: for example, $f(x, y) = x^2 + y^2 - 3xy$ has $f_{xx}(0, 0) = f_{yy}(0, 0) = 2$, making $(0, 0)$ look like a local min, but the concave down part of the graph is just in a different direction.

### desmos secretSaddle

This counterexample gives us a hint, though: we could verify that $(0, 0)$ was a local minimum not if the concavity in just the $x$- and $y$-directions were positive, but if the concavity in *all* directions were positive. Specifically, let's take what we might call a *second directional derivative* and see what we can determine.

Let $\vec{u} = \left< c, d \right>$ be a unit vector and consider $D_{\vec{u}}D_{\vec{u}} f$. The gradient formula for the directional derivative tells us that

$$
	D_{\vec{u}}D_{\vec{u}} f &= D_{\vec{u}} \left( \G f \bullet \left< c, d \right> \right)

	&= D_{\vec{u}} \left( cf_x + df_y \right)

	&= \G \left( cf_x + df_y \right) \bullet \left< c, d \right>

	&= \left< cf_{xx} + df_{yx}, cf_{xy} + df_{yy} \right> \bullet \left< c, d \right>

	&= c^2f_{xx} + cdf_{yx} + cdf_{xy} + d^2f_{yy}.
$$

While the gradient $\G \left( cf_x + df_y \right)$ might be daunting, $cf_x + df_y$ is just a function itself, and so we can take $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}$ of it without issue, as we did above --- they both split across addition and factor through the scalar multiplication by $c$ and $d$. Now we can use Clairaut's theorem to compactify the resulting expression a bit.

$$
	D_{\vec{u}}D_{\vec{u}} f &= c^2f_{xx} + 2cdf_{xy} + d^2f_{yy}.
$$

We're interested in plugging in the critical point $(a, b)$ and figuring out the sign of the resulting expression: if it's always positive, then we know this is a local minimum. If it's always negative, then we know it's a local maximum. If it's sometimes one and sometimes the other, then it must be a saddle point, and otherwise, the original second derivative test is inconclusive. With that in mind, let's plug in $(a, b)$ and see how to reason about the resulting formula.

We have

$$
	D_{\vec{u}}D_{\vec{u}} f(a, b) &= c^2f_{xx}(a, b) + 2cdf_{xy}(a, b) + d^2f_{yy}(a, b),
$$

and to keep it simple, let's write this as

$$
	Ac^2 + 2Bcd + Cd^2
$$

for

$$
	A &= f_{xx}(a, b)

	B &= f_{xy}(a, b)

	C &= f_{yy}(a, b).
$$

To summarize where we are, we're looking to analyze the sign of this polynomial over all possible values of $c$ and $d$. Thankfully for us, it's not that bad! Let's set $g(c) = Ac^2 + 2Bcd + Cd^2$, which is a quadratic in $c$ (and $d$, but let's just think of it in terms of one variable for now), and so its graph is a parabola, as long as $A \neq 0$. Let's assume that for now and put off the $A = 0$ case until later. Since $g$ a polynomial, it's continuous, and so to determine its sign, we can just see when it's zero. The quadratic formula gives us

$$
	c &= \frac{-2Bd \pm \sqrt{4B^2d^2 - 4ACd^2}}{2A}

	&= \frac{-2Bd \pm 2d\sqrt{B^2 - AC}}{2A}

	&= \frac{-Bd \pm d\sqrt{B^2 - AC}}{A}.
$$

Now this only gives real-valued solutions for $c$ (i.e. solutions that are useful to us) when the expression under the root is nonnegative. If it's negative, then $g(c)$ is never zero, and so it *cannot change sign*. That means every cross-section of the graph of $f(x, y)$ though the point $(a, b)$ always has the same concavity --- either up or down --- no matter which direction of cross-section we pick.

Similarly, if the expression $B^2 - AC$ under the root is positive, then the graph of $g(c)$ crosses the $c$-axis in two different places. That means it *must* be sometimes positive and sometimes negative, so $(a, b)$ has to be a saddle point. If $B^2 - AC = 0$, then the graph of $g(c)$ touches the $c$-axis but doesn't cross it, and so the test is unfortunately inconclusive: all of the cross-sectional graphs through $(a, b)$ have the same concavity, except for one, whose second derivative at $(a, b)$ is zero, and our knowledge of single-variable calculus tells us that we can't say anything conclusive about that one.

Lastly, let's think through what happens if $A = 0$. As long as $C \neq 0$, we can repeat the same logic but treat $g$ as a function of $d$ to produce an identical result. If $C = 0$ too, then the equation is just
$$
	D_{\vec{u}}D_{\vec{u}} f(a, b) &= 2cdf_{xy}(a, b),
$$

and as long as $B = f_{xy}(a, b)$ is nonzero, there will *always* be values of $c$ and $d$ that make this sometimes positive and sometimes negative (whether $c$ and $d$ have the same or opposite signs). That agrees with our previous result, since $B^2 - AC = B^2$ will always be positive in this case. Finally, if $B = 0$ too, meaning all mixed partials are zero, then $D_{\vec{u}}D_{\vec{u}} f(a, b) = 0$ for any unit vector $\vec{u}$, and so we can't determine anything about concavity --- again, that agrees with the previous result, since $B^2 - AC = 0$.

That's a lot of information! One thing that's immediately clear is that the expression $B^2 - AC$ needs a name, and in fact it tells us almost everything we want to know. Luckily for us, that expression for a quadratic equation already has a name in math! It's called a **discriminant**, and the only difference is that it's reversed in sign (i.e. $AC - B^2$) compared to what we have. We also need to distinguish between a local maximum and local minimum once we know that a critical point is one of the two. We can do that by recognizing that in that case, all cross-sectional graphs have the same concavity (either positive or negative), and so we can just check one, say $f_{xx}(a, b)$. With all that said, let's state the second derivative test!

### thm -m "The second derivative test for a function of two variables"

	Let $f(x, y)$ be a function of two variables that is continuous and differentiable, and whose first-order partial derivatives $f_x$ and $f_y$ are continuous near a point $(a, b)$ where $\G f = \vec{0}$ (we need the second condition so that we can apply Clairaut's theorem). The **discriminant** of $f$ is

	$$
		D(x, y) = f_{xx}(x, y)f_{yy}(x, y) - f_{xy}(x, y)^2.
	$$

	If $D(a, b) > 0$ and $f_{xx}(a, b) > 0$ or $f_{yy}(a, b) > 0$, then $f$ has a local minimum at $(a, b)$.

	If $D(a, b) > 0$ and $f_{xx}(a, b) < 0$ or $f_{yy}(a, b) < 0$, then $f$ has a local maximum at $(a, b)$.

	If $D(a, b) < 0$, then $f$ has a saddle point at $(a, b)$.

	Otherwise, the test is inconclusive.

###

### exc "the second derivative test"

	Find and classify the critical points of $f(x, y) = x^3 - y^2 + 2xy$.

	### solution

	We'll start by finding the critical points, so we need the gradient.

	$$
		\G f = \left< 3x^2 + 2y, -2y + 2x \right>.
	$$

	This is equal to $\vec{0}$ when $x = y$ and $3x^2 + 2y = 0$, so

	$$
		3x^2 + 2x &= 0

		x\left( 3x + 2 \right) &= 0

		x &= 0, \quad x = -\frac{2}{3}.
	$$

	The critical points are then at $(0, 0)$ and $\left( -\frac{2}{3}, -\frac{2}{3} \right)$, and to classify them, we need the discriminant. Let's compute the second partials we need:

	$$
		f_{xx}(x, y) &= 6x

		f_{xy}(x, y) &= 2

		f_{yy}(x, y) &= -2

		~

		D(x, y) &= -12x - 4.
	$$

	So $D(0, 0) = -4 < 0$ and $D\left( -\frac{2}{3}, -\frac{2}{3} \right) = 4 > 0$, which tells us that $(0, 0)$ is a saddle point and $\left( -\frac{2}{3}, -\frac{2}{3} \right)$ is a local max, since $f_{yy}\left( -\frac{2}{3}, -\frac{2}{3} \right) = -2$ is negative. Not bad!

	### desmos secondDerivativeTest1

###

### ex "the second derivative test"

	Find and classify the critical points of $g(x, y) = xye^{-x^2-y^2}$.

	This one is fairly in-depth. The gradient is

	$$
		\G g = \left< ye^{-x^2 - y^2} + xye^{-x^2-y^2}(-2x), xe^{-x^2 - y^2} + xye^{-x^2-y^2}(-2y) \right>,
	$$

	and if $\G g = \vec{0}$, then we can thankfully divide by the exponential factor, since it's never zero, leaving us with

	$$
		y - 2x^2 y &= 0

		x - 2x y^2 &= 0.
	$$

	This requires a little care. The first equation simplifies to $y\left( 1 - 2x^2 \right) = 0$, so either $y = 0$ *or* $x^2 = \frac{1}{2}$, meaning $x = \pm \frac{1}{\sqrt{2}}$. These are all independent conditions, so rather than pair them up in points $\left( \pm \frac{1}{\sqrt{2}}, \pm \frac{1}{\sqrt{2}} \right)$, which probably wouldn't satisfy the second equation, we take all three possibilities and plug them *each* into the second equation. We can first simplify it to $x\left( 1 - 2y^2 \right) = 0$, and then plug in our values, resulting in

	$$
		y = 0: \quad x\left( 1 - 2(0)^2 \right) &= 0

		x &= 0

		~

		x = \frac{1}{\sqrt{2}}: \quad \frac{1}{\sqrt{2}}\left( 1 - 2y^2 \right) &= 0

		1 - 2 y^2 &= 0

		y &= \pm \frac{1}{\sqrt{2}}

		~

		x = -\frac{1}{\sqrt{2}}: \quad -\frac{1}{\sqrt{2}}\left( 1 - 2y^2 \right) &= 0

		1 - 2 y^2 &= 0

		y &= \pm \frac{1}{\sqrt{2}}
	$$

	The nearly overwhelming result is that there are *five* critical points in total! We have $(0, 0)$, and then $\left( \pm \frac{1}{\sqrt{2}}, \pm \frac{1}{\sqrt{2}} \right)$ for any combination of signs. To classify them, let's use the discriminant.

	$$
		f_{xx}(x, y) &= \frac{\partial}{\partial x} \left[ \left( y - 2x^2 y \right)e^{-x^2 - y^2} \right]

		&= -4xye^{-x^2 - y^2} + \left( y - 2x^2 y \right) e^{-x^2 - y^2} (-2x)

		&= e^{-x^2 - y^2}\left( -6xy + 4x^3 y \right).

		~

		f_{xy}(x, y) &= \frac{\partial}{\partial y} \left[ \left( y - 2x^2 y \right)e^{-x^2 - y^2} \right]

		&= \left( 1 - 2x^2 \right)e^{-x^2 - y^2} + \left( y - 2x^2 y \right) e^{-x^2 - y^2} (-2y)

		&= e^{-x^2 - y^2}\left( 1 - 2x^2 - 2y^2 + 4x^2y^2 \right).

		~

		f_{yy}(x, y) &= \frac{\partial}{\partial y} \left[ \left( x - 2x y^2 \right)e^{-x^2 - y^2} \right]

		&= -4xye^{-x^2 - y^2} + \left( x - 2x y^2 \right) e^{-x^2 - y^2} (-2y)

		&= e^{-x^2 - y^2}\left( -6xy + 4x y^3 \right).

		~

		D(x, y) &= \left( e^{-x^2 - y^2} \right)^2 \left( \left( -6xy + 4x^3 y \right) \left( -6xy + 4x y^3 \right) - \left( 1 - 2x^2 - 2y^2 + 4x^2y^2 \right)^2 \right).
	$$

	In the interest of time, let's just cut straight to the simplified form:

	$$
		D(x, y) &= e^{-2x^2 - 2y^2} \left( -1 + 4x^2 + 4y^2 - 4x^4 + 20x^2y^2 - 4y^4 - 8x^4y^2 - 8x^2y^4 \right).

	$$

	Now $D(0, 0) = -1 < 0$, and conveniently for us, all of the powers on $x$ and $y$ are even, and so whether we plug in $\frac{1}{\sqrt{2}}$ or $-\frac{1}{\sqrt{2}}$ for either, the result is the same!

	$$
		D\left( \pm \frac{1}{\sqrt{2}}, \pm \frac{1}{\sqrt{2}} \right) &= e^{-2} \left( -1 + 2 + 2 - 1 + 5 - 1 - 1 - 1 \right)

		&= 4e^{-2} > 0.
	$$

	Almost done! We can now check $f_{xx}$ at each of these four points to determine its sign.

	$$
		f_{xx}\left( \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right) = f_{xx}\left( -\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} \right) &= e^{-1}\left( -6xy + 4x^3 y \right)

		&= e^{-1}\left( -3 + 1 \right) < 0

		~

		f_{xx}\left( -\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right) = f_{xx}\left( \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} \right) &= e^{-1}\left( -6xy + 4x^3 y \right)

		&= e^{-1}\left( 3 - 1 \right) > 0.
	$$

	In all, $(0, 0)$ is a saddle point, $\left( \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right)$ and $\left( -\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} \right)$ are local maxima, and $\left( \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}} \right)$ and $\left( -\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right)$ are local minima.

	### desmos secondDerivativeTest2

	As an aside, the second derivative test is doing fantastic work for us at $(0, 0)$ in particular: both the $x$ and $y$ cross-sections of the graph are constantly zero there, meaning neither has any concavity to speak of.

###

We've gone a long way toward bringing the second derivative test to $#R#^3$! In the next section, we'll finish the job by generalizing the notion of optimizing a function on a particular interval, and in the one after that, we'll conclude our discussion of multivariable calculus by considering more complicated interactions a function can have with the edge of its domain.

### nav-buttons