<header><div id="logo"><a href="/home" tabindex="-1"><img src="/graphics/general-icons/logo.webp" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 16: Least-Squares Approximations</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">After the behemoth that was the last section, let&#x2019;s take the opportunity to apply linear algebra to some more concrete problems. When we solve a matrix equation <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> currently, there are three different possible outcomes: either there&#x2019;s exactly one solution for <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$,</span> there are infinitely many (i.e. the solution contains one or more free parameters), or there are no solutions. Up to this point, we haven&#x2019;t been able to handle that last case beyond saying that we&#x2019;re unable to solve the system, but now that we have the language of the previous section, we&#x2019;re much better prepared to discuss it in more depth. In particular, what&#x2019;s the closest we can get? For any value of <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$,</span> <span class="tex-holder inline-math" data-source-tex="A\vec{x}">$A\vec{x}$</span> is a linear combination of the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with weights given by the entries of <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$,</span> so <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> is solvable exactly when <span class="tex-holder inline-math" data-source-tex="\vec{b} \in \operatorname{image} A">$\vec{b} \in \operatorname{image} A$,</span> which is just the span of the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> If that&#x2019;s not the case, then the closest vector that <em>is</em> in the image is given by a projection. Specifically, the closest output is <span class="tex-holder inline-math" data-source-tex="\vec{b}'">$\vec{b}'$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{b}' = \operatorname{proj}_{\operatorname{image} A} \vec{b}.[NEWLINE]$$">$$\begin{align*}\vec{b}' = \operatorname{proj}_{\operatorname{image} A} \vec{b}.\end{align*}$$</span></p><p class="body-text">So how can we solve <span class="tex-holder inline-math" data-source-tex="A\vec{x}' = \vec{b}'">$A\vec{x}' = \vec{b}'$</span> for a solution <span class="tex-holder inline-math" data-source-tex="\vec{x}'">$\vec{x}'$?</span> We can&#x2019;t use the convenient projection formula from the last section without an orthonormal basis for the image of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> but we do know that the vectors <span class="tex-holder inline-math" data-source-tex="\vec{b}'">$\vec{b}'$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{b} - \vec{b}'">$\vec{b} - \vec{b}'$</span> are orthogonal because the latter necessarily lives in the orthogonal complement of <span class="tex-holder inline-math" data-source-tex="\operatorname{image} A">$\operatorname{image} A$.</span> That means that the dot product of <span class="tex-holder inline-math" data-source-tex="\vec{b} - \vec{b}'">$\vec{b} - \vec{b}'$</span> with any column of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is zero, and so</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left( \vec{b} - \vec{b}' \right)^T A = \vec{0}^T,[NEWLINE]$$">$$\begin{align*}\left( \vec{b} - \vec{b}' \right)^T A = \vec{0}^T,\end{align*}$$</span></p><p class="body-text">since every component of that vector is one of those dot products. That&#x2019;s a very awkward way to write the product, though, and we&#x2019;re better off transposing both sides, remembering that transposing a matrix product reverses the order of the factors:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^T \left( \vec{b} - \vec{b}' \right) &= \vec{0}\\[NEWLINE][TAB]A^T \vec{b}' &= A^T \vec{b}\\[NEWLINE][TAB]A^T\!A \vec{x}' &= A^T \vec{b}.[NEWLINE]\end{align*}">$$\begin{align*}A^T \left( \vec{b} - \vec{b}' \right) &= \vec{0}\\[4px]A^T \vec{b}' &= A^T \vec{b}\\[4px]A^T\!A \vec{x}' &= A^T \vec{b}.\end{align*}$$</span></p><p class="body-text">And now this is a standard matrix equation we can solve for <span class="tex-holder inline-math" data-source-tex="\vec{x}'">$\vec{x}'$!</span> The moral here is that if <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> is unsolvable, multiplying both sides by <span class="tex-holder inline-math" data-source-tex="A^T">$A^T$</span> produces a solvable equation whose solution is the best approximation to <span class="tex-holder inline-math" data-source-tex="\vec{b}">$\vec{b}$.</span> Since &#x201C;best&#x201D; means minimizing <span class="tex-holder inline-math" data-source-tex="\left| \left| \vec{b}' - \vec{b} \right| \right|">$\left| \left| \vec{b}' - \vec{b} \right| \right|$,</span> or equivalently minimizing</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left| \left| \vec{b}' - \vec{b} \right| \right|^2 = \left( \vec{b}' - \vec{b} \right)_1^2 + \cdots + \left( \vec{b}' - \vec{b} \right)_n^2,[NEWLINE]$$">$$\begin{align*}\left| \left| \vec{b}' - \vec{b} \right| \right|^2 = \left( \vec{b}' - \vec{b} \right)_1^2 + \cdots + \left( \vec{b}' - \vec{b} \right)_n^2,\end{align*}$$</span></p><p class="body-text">these approximation problems are called <strong>least-squares problems</strong>.</p><p class="body-text">To wrap up the discussion, let&#x2019;s see when a least-squares solution is unique. Since we&#x2019;re solving <span class="tex-holder inline-math" data-source-tex="A^T\!A \vec{x}' = A^T \vec{b}">$A^T\!A \vec{x}' = A^T \vec{b}$,</span> that&#x2019;s exactly when <span class="tex-holder inline-math" data-source-tex="A^T\!A">$A^T\!A$</span> is invertible, regardless of <span class="tex-holder inline-math" data-source-tex="\vec{b}">$\vec{b}$.</span> That&#x2019;s not a particularly convenient condition, so let&#x2019;s explore it a little further. Even though <span class="tex-holder inline-math" data-source-tex="A^T\!A">$A^T\!A$</span> is square, <span class="tex-holder inline-math" data-source-tex="A">$A$</span> itself might not be. However, since <span class="tex-holder inline-math" data-source-tex="A^T\!A">$A^T\!A$</span> is invertible, <span class="tex-holder inline-math" data-source-tex="\ker A^T\!A = \left\{ \vec{0} \right\}">$\ker A^T\!A = \left\{ \vec{0} \right\}$,</span> and so <span class="tex-holder inline-math" data-source-tex="\ker A = \left\{ \vec{0} \right\}">$\ker A = \left\{ \vec{0} \right\}$</span> too &mdash; since any vector in the kernel of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is also in the kernel of <span class="tex-holder inline-math" data-source-tex="A^T\!A">$A^T\!A$.</span> That means <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has linearly independent columns, and the reverse is true too: if <span class="tex-holder inline-math" data-source-tex="\ker A = \left\{ \vec{0} \right\}">$\ker A = \left\{ \vec{0} \right\}$,</span> then any vector <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$</span> satisfying <span class="tex-holder inline-math" data-source-tex="A^T\!A\vec{x} = \vec{0}">$A^T\!A\vec{x} = \vec{0}$</span> also satisfies</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{x} \bullet \left( A^T\!A \vec{x} \right) &= 0\\[NEWLINE][TAB]\vec{x}^T A^T\!A \vec{x} &= 0\\[NEWLINE][TAB]\left( A \vec{x} \right)^T \left( A \vec{x} \right) &= 0\\[NEWLINE][TAB]\left( A \vec{x} \right) \bullet \left( A \vec{x} \right) &= 0\\[NEWLINE][TAB]A\vec{x} &= \vec{0}\\[NEWLINE][TAB]\vec{x} &= \vec{0}.[NEWLINE]\end{align*}">$$\begin{align*}\vec{x} \bullet \left( A^T\!A \vec{x} \right) &= 0\\[4px]\vec{x}^T A^T\!A \vec{x} &= 0\\[4px]\left( A \vec{x} \right)^T \left( A \vec{x} \right) &= 0\\[4px]\left( A \vec{x} \right) \bullet \left( A \vec{x} \right) &= 0\\[4px]A\vec{x} &= \vec{0}\\[4px]\vec{x} &= \vec{0}.\end{align*}$$</span></p><p class="body-text">In so many symbols, <span class="tex-holder inline-math" data-source-tex="\ker \left( A^T\!A \right) = \left\{ \vec{0} \right\}">$\ker \left( A^T\!A \right) = \left\{ \vec{0} \right\}$,</span> so <span class="tex-holder inline-math" data-source-tex="A^T\!A">$A^T\!A$</span> is invertible. It&#x2019;s been a bit of symbol-pushing, but we&#x2019;ve solved and characterized least-squares problems in general.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: least-squares approximations</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix and let <span class="tex-holder inline-math" data-source-tex="\vec{b} \in \mathbb{R}^m">$\vec{b} \in \mathbb{R}^m$.</span> A vector <span class="tex-holder inline-math" data-source-tex="\vec{x}' \in \mathbb{R}^n">$\vec{x}' \in \mathbb{R}^n$</span> that minimizes <span class="tex-holder inline-math" data-source-tex="\left| \left| A\vec{x}' - \vec{b} \right| \right|">$\left| \left| A\vec{x}' - \vec{b} \right| \right|$</span> is the projection of <span class="tex-holder inline-math" data-source-tex="\vec{b}">$\vec{b}$</span> onto the image of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> or equivalently, a solution to <span class="tex-holder inline-math" data-source-tex="A^T\!A \vec{x}' = A^T \vec{b}">$A^T\!A \vec{x}' = A^T \vec{b}$.</span> Moreover, this least-squares solution <span class="tex-holder inline-math" data-source-tex="\vec{x}'">$\vec{x}'$</span> is unique exactly when the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are linearly independent.</p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: least-squares approximations</div><p class="body-text">Find the least-squares solution to</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc} 1& 0 \\ 1& 1 \\ 2& 1 \end{array}\right]\vec{x} = \left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc} 1& 0 \\ 1& 1 \\ 2& 1 \end{array}\right]\vec{x} = \left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">One advantage of the least-squares approach is that if the matrix equation does actually have a solution, we&#x2019;ll find it anyway, so we don&#x2019;t need to check that before we start the process. Multiplying both sides by the transpose of the coefficient matrix, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc} 1& 1& 2 \\ 0& 1& 1 \end{array}\right]\left[\begin{array}{cc} 1& 0 \\ 1& 1 \\ 2& 1 \end{array}\right]\vec{x} &= \left[\begin{array}{ccc} 1& 1& 2 \\ 0& 1& 1 \end{array}\right]\left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{cc} 6& 3 \\ 3& 2 \end{array}\right] \vec{x} &= \left[\begin{array}{c} 3 \\ 1 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& 3 & 3 \\ 3& 2 & 1 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 2& 1 & 1 \\ 3& 2 & 1 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= \frac{1}{3}\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 2& 1 & 1 \\ 1& 1 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& 0 & 1 \\ 1& 1 & 0 \end{array}\right] & \quad \vec{r_1} \ -\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& 0 & 1 \\ 0& 1 & -1 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc} 1& 1& 2 \\ 0& 1& 1 \end{array}\right]\left[\begin{array}{cc} 1& 0 \\ 1& 1 \\ 2& 1 \end{array}\right]\vec{x} &= \left[\begin{array}{ccc} 1& 1& 2 \\ 0& 1& 1 \end{array}\right]\left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right]\\[4px]\left[\begin{array}{cc} 6& 3 \\ 3& 2 \end{array}\right] \vec{x} &= \left[\begin{array}{c} 3 \\ 1 \end{array}\right]\\[4px]\left[\begin{array}{cc|c} 6& 3 & 3 \\ 3& 2 & 1 \end{array}\right] &\\[4px]\left[\begin{array}{cc|c} 2& 1 & 1 \\ 3& 2 & 1 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= \frac{1}{3}\\[4px]\left[\begin{array}{cc|c} 2& 1 & 1 \\ 1& 1 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{cc|c} 1& 0 & 1 \\ 1& 1 & 0 \end{array}\right] & \quad \vec{r_1} \ -\!\!= \vec{r_2}\\[4px]\left[\begin{array}{cc|c} 1& 0 & 1 \\ 0& 1 & -1 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}.\end{align*}$$</span></p><p class="body-text">The least-squares solution is therefore</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{x}' = \left[\begin{array}{c} 1 \\ -1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{x}' = \left[\begin{array}{c} 1 \\ -1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Since the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are linearly independent, this solution is unique, and to see how far off we are from the actual value of <span class="tex-holder inline-math" data-source-tex="\vec{b}">$\vec{b}$,</span> we can compute</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left| \left| \left[\begin{array}{cc} 1& 0 \\ 1& 1 \\ 2& 1 \end{array}\right]\left[\begin{array}{c} 1 \\ -1 \end{array}\right] - \left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right] \right| \right| &= \left| \left| \left[\begin{array}{c} 1 \\ 0 \\ 1 \end{array}\right] - \left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right] \right| \right|\\[NEWLINE][TAB]&= \left| \left| \left[\begin{array}{c} -1 \\ -1 \\ 1 \end{array}\right] \right| \right|\\[NEWLINE][TAB]&= \sqrt{3}.[NEWLINE]\end{align*}">$$\begin{align*}\left| \left| \left[\begin{array}{cc} 1& 0 \\ 1& 1 \\ 2& 1 \end{array}\right]\left[\begin{array}{c} 1 \\ -1 \end{array}\right] - \left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right] \right| \right| &= \left| \left| \left[\begin{array}{c} 1 \\ 0 \\ 1 \end{array}\right] - \left[\begin{array}{c} 2 \\ 1 \\ 0 \end{array}\right] \right| \right|\\[4px]&= \left| \left| \left[\begin{array}{c} -1 \\ -1 \\ 1 \end{array}\right] \right| \right|\\[4px]&= \sqrt{3}.\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: least-squares approximations</div><p class="body-text">Find the least-squares solution to</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc} 1& 2& -1 \\ 2& 1& 1 \\ -1& 0& -1 \end{array}\right]\vec{x} = \left[\begin{array}{c} 7 \\ 7 \\ 7 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc} 1& 2& -1 \\ 2& 1& 1 \\ -1& 0& -1 \end{array}\right]\vec{x} = \left[\begin{array}{c} 7 \\ 7 \\ 7 \end{array}\right].\end{align*}$$</span></p><p class="body-text">How far away is the closest output?</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button nav-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>